[{"page_content": "LECTURE NOTES ON PROBABILITY ,\nSTATISTICS AND LINEAR ALGEBRA\nC. H. Taubes\nDepartment of Mathematics\nHarvard University\nCambridge, MA 02138\nSpring, 2010", "metadata": {"page": 0}}, {"page_content": "CONTENTS\n1 Data Exploration 2\n1.1 Snowfall data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Data mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2 Basic notions from probability theory 7", "metadata": {"page": 1}}, {"page_content": "2 Basic notions from probability theory 7\n2.1 Talking the talk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 Axiomatic de\ufb01nition of probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.3 Computing probabilities for subsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Some consequences of the de\ufb01nition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12", "metadata": {"page": 1}}, {"page_content": "2.5 That\u2019s all there is to probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3 Conditional probability 16\n3.1 The de\ufb01nition of conditional probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.2 Independent events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17", "metadata": {"page": 1}}, {"page_content": "3.3 Bayes theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.4 Decomposing a subset to compute probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.5 More linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.6 An iterated form of Bayes\u2019 theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22", "metadata": {"page": 1}}, {"page_content": "3.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4 Linear transformations 25\n4.1 Protein molecules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n4.2 Protein folding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n5 How matrix products arise 27", "metadata": {"page": 1}}, {"page_content": "5 How matrix products arise 27\n5.1 Genomics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n5.2 How bacteria \ufb01nd food . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n5.3 Growth of nerves in a developing embryo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n5.4 Enzyme dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29", "metadata": {"page": 1}}, {"page_content": "5.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n6 Random variables 31\n6.1 The de\ufb01nition of a random variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n6.2 Probability for a random variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n6.3 A probability function on the possible values off . . . . . . . . . . . . . . . . . . . . . . . . . . . 33", "metadata": {"page": 1}}, {"page_content": "6.4 Mean and standard distribution for a random variable . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n6.5 Random variables as proxies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n6.6 A biology example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\ni", "metadata": {"page": 1}}, {"page_content": "6.7 Independent random variables and correlation matrices . . . . . . . . . . . . . . . . . . . . . . . . . 37\n6.8 Correlations and proteomics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n6.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n7 The statistical inverse problem 41\n7.1 A general setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44", "metadata": {"page": 2}}, {"page_content": "7.2 The Bayesian guess . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n7.3 An example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n7.4 Gregor Mendel\u2019s peas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n7.5 Another candidate for P(\u03b8): A maximum likelihood candidate. . . . . . . . . . . . . . . . . . . . . 46", "metadata": {"page": 2}}, {"page_content": "7.6 What to remember from this chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n7.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n8 Kernel and image in biology 50\n9 Dimensions and coordinates in a scienti\ufb01c context 52\n9.1 Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52", "metadata": {"page": 2}}, {"page_content": "9.2 A systematic approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n9.3 Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n9.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n10 More about Bayesian statistics 55\n10.1 A problem for Bayesians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55", "metadata": {"page": 2}}, {"page_content": "10.2 A second problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n10.3 Meet the typical Bayesian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n10.4 A \ufb01rst example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n10.5 A second example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57", "metadata": {"page": 2}}, {"page_content": "10.6 Something traumatic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n10.7 Rolling dice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n10.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n11 Common probability functions 59\n11.1 What does \u2018random\u2019 really mean? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59", "metadata": {"page": 2}}, {"page_content": "11.2 A mathematical translation of the term \u2018random\u2019 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n11.3 Some standard counting solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n11.4 Some standard probability functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n11.5 Means and standard deviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64", "metadata": {"page": 2}}, {"page_content": "11.6 The Chebychev theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n11.7 Characteristic functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n11.8 Loose ends about counting elements in various sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n11.9 A Nobel Prize for the clever use of statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68", "metadata": {"page": 2}}, {"page_content": "11.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n12 P-values 72\n12.1 Point statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n12.2P-value and bad choices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n12.3 A binomial example using DNA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74", "metadata": {"page": 2}}, {"page_content": "12.4 An example using the Poisson function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n12.5 Another Poisson example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n12.6 A silly example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n12.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n13 Continuous probability functions 80", "metadata": {"page": 2}}, {"page_content": "13 Continuous probability functions 80\n13.1 An example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n13.2 Continuous probability functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\nii", "metadata": {"page": 2}}, {"page_content": "13.3 The mean and standard deviation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n13.4 The Chebychev theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n13.5 Examples of probability functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n13.6 The Central Limit Theorem: Version 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83", "metadata": {"page": 3}}, {"page_content": "13.7 The Central Limit Theorem: Version 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n13.8 The three most important things to remember . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n13.9 A digression with some comments on Equation (13.1) . . . . . . . . . . . . . . . . . . . . . . . . . 85\n13.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n14 Hypothesis testing 88", "metadata": {"page": 3}}, {"page_content": "14 Hypothesis testing 88\n14.1 An example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n14.2 Testing the mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n14.3 Random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n14.4 The Chebychev and Central Limit Theorems for random variables . . . . . . . . . . . . . . . . . . . 90", "metadata": {"page": 3}}, {"page_content": "14.5 Testing the variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n14.6 Did Gregor Mendel massage his data? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n14.7 Boston weather 2008 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n14.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n15 Determinants 97", "metadata": {"page": 3}}, {"page_content": "15 Determinants 97\n16 Eigenvalues in biology 99\n16.1 An example from genetics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n16.2 Transition/Markov matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n16.3 Another protein folding example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100", "metadata": {"page": 3}}, {"page_content": "16.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n17 More about Markov matrices 104\n17.1 Solving the equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n17.2 Proving things about Markov matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n17.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109", "metadata": {"page": 3}}, {"page_content": "18 Markov matrices and complex eigenvalues 111\n18.1 Complex eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n18.2 The size of the complex eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n18.3 Another Markov chain example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n18.4 The behavior of a Markov chain ast\u2192\u221e . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114", "metadata": {"page": 3}}, {"page_content": "18.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n19 Symmetric matrices and data sets 116\n19.1 An example from biology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n19.2 A fundamental concern . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116", "metadata": {"page": 3}}, {"page_content": "19.3 A method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n19.4 Some loose ends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n19.5 Some examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n19.6 Small versus reasonably sized eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119", "metadata": {"page": 3}}, {"page_content": "19.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\niii", "metadata": {"page": 3}}, {"page_content": "Preface\nThis is a very slight revision of the notes used for Math 19b in the Spring 2009 semester. These are written by Cliff\nTaubes (who developed the course), but re-formatted and slightly revised for Spring 2010. Any errors you might \ufb01nd\nwere almost certainly introduced by these revisions and thus are not the fault of the original author.\nI would be interested in hearing of any errors you do \ufb01nd, as well as suggestions for improvement of either the text or\nthe presentation.\nPeter M. Gar\ufb01eld", "metadata": {"page": 4}}, {"page_content": "the presentation.\nPeter M. Gar\ufb01eld\ngarfield@math.harvard.edu\n1", "metadata": {"page": 4}}, {"page_content": "CHAPTER\nONE\nData Exploration\nThe subjects of Statistics and Probability concern the mathematical tools that are designed to deal with uncertainty. To\nbe more precise, these subjects are used in the following contexts:\n\u2022To understand the limitations that arise from measurement inaccuracies.\n\u2022To \ufb01nd trends and patterns in noisy data.\n\u2022To test hypothesis and models with data.\n\u2022To estimate con\ufb01dence levels for future predictions from data.", "metadata": {"page": 5}}, {"page_content": "\u2022To estimate con\ufb01dence levels for future predictions from data.\nWhat follows are some examples of scienti\ufb01c questions where the preceding issues are central and so statistics and\nprobability play a starring role.\n\u2022An extremely large meteor crashed into the earth at the time of the disappearance of the dinosaurs. The most\npopular theory posits that the dinosaurs were killed by the ensuing environmental catastrophe. Does the fossil", "metadata": {"page": 5}}, {"page_content": "record con\ufb01rm that the disappearance of the dinosaurs was suitably instantaneous?\n\u2022We read in the papers that fat in the diet is \u201cbad\u201d for you. Do dietary studies of large populations support this\nassertion?\n\u2022Do studies of gene frequencies support the assertion that all extent people are 100% African descent?\n\u2022The human genome project claims to have determined the DNA sequences along the human chromosomes. How", "metadata": {"page": 5}}, {"page_content": "accurate are the published sequences? How much variation should be expected between any two individuals?\nStatistics and probability also play explicit roles in our understanding and modelling of diverse processes in the life\nsciences. These are typically processes where the outcome is in\ufb02uenced by many factors, each with small effect, but\nwith signi\ufb01cant total impact. Here are some examples:\nExamples from Chemistry: What is thermal equilibrium? Does it mean stasis?", "metadata": {"page": 5}}, {"page_content": "Examples from Chemistry: What is thermal equilibrium? Does it mean stasis?\nWhy are chemical reaction rates in\ufb02uenced by temperature? How do proteins fold correctly? How stable are the folded\ncon\ufb01gurations?\nExamples from medicine: How many cases of \ufb02u should the health service expect to see this winter? How to determine\ncancer probabilities? Is hormone replacement therapy safe? Are anti-depressants safe?", "metadata": {"page": 5}}, {"page_content": "cancer probabilities? Is hormone replacement therapy safe? Are anti-depressants safe?\nAn example from genomics: How are genes found in long stretches of DNA? How much DNA is dispensable?\nAn example from developmental biology: How does programmed cell death work; what cells die and what live?\nExamples from genetics: What are the fundemental inheritance rules? How can genetics determine ancestral relation-\nships?\n2", "metadata": {"page": 5}}, {"page_content": "An examples from ecology: How are species abundance estimates determined from small samples?\nTo summarize: There are at least two uses for statistics and probability in the life sciences. One is to tease information\nfrom noisy data, and the other is to develop predictive models in situations where chance plays a pivotal role. Note that\nthese two uses of statistics are not unrelated since a theoretical understanding of the causes for the noise can facilitate\nits removal.", "metadata": {"page": 6}}, {"page_content": "its removal.\nThe rest of this \ufb01rst chapter focuses on the \ufb01rst of these two uses of statistics.\n1.1 Snowfall data\nTo make matters concrete, the discussion that follows uses actual data on snowfall totals in Boston from 1890 through\n2001. Table 1.1 gives snowfall totals (in inches) in Boston from the National Oceanic and Atmospheric Administra-\ntion1. What we do with this data depends on what sort of questions we are going to ask. Noting the high snow falls", "metadata": {"page": 6}}, {"page_content": "1890 42.6 1910 40.6 1930 40.8 1950 29.7 1970 57.3 1990 19.1\n1891 46.8 1911 31.6 1931 24.2 1951 39.6 1971 47.5 1991 22.0\n1892 66.0 1912 19.4 1932 40.6 1952 29.8 1972 10.3 1992 83.9\n1893 64.0 1913 39.4 1933 62.7 1953 23.6 1973 36.9 1993 96.3\n1894 46.9 1914 22.3 1934 45.4 1954 25.1 1974 27.6 1994 14.9\n1895 38.7 1915 79.2 1935 30.0 1955 60.9 1975 46.6 1995 107.6\n1896 43.2 1916 54.2 1936 9.0 1956 52.0 1976 58.5 1996 51.9\n1897 51.9 1917 45.7 1937 50.6 1957 44.7 1977 85.1 1997 25.6", "metadata": {"page": 6}}, {"page_content": "1897 51.9 1917 45.7 1937 50.6 1957 44.7 1977 85.1 1997 25.6\n1898 70.9 1918 21.1 1938 40.3 1958 34.1 1978 27.5 1998 36.4\n1899 25.0 1919 73.4 1939 37.7 1959 40.9 1979 12.7 1999 24.9\n1900 17.5 1920 34.1 1940 47.8 1960 61.5 1980 22.3 2000 45.9\n1901 44.1 1921 37.6 1941 24.0 1961 44.7 1981 61.8 2001 15.1\n1902 42.0 1922 68.5 1942 45.7 1962 30.9 1982 32.7\n1903 72.9 1923 32.3 1943 27.7 1963 63.0 1983 43.0\n1904 44.9 1924 21.4 1944 59.2 1964 50.4 1984 26.6\n1905 37.6 1925 38.3 1945 50.8 1965 44.1 1985 18.1", "metadata": {"page": 6}}, {"page_content": "1904 44.9 1924 21.4 1944 59.2 1964 50.4 1984 26.6\n1905 37.6 1925 38.3 1945 50.8 1965 44.1 1985 18.1\n1906 67.9 1926 60.3 1946 19.4 1966 60.1 1986 42.5\n1907 26.2 1927 20.8 1947 89.2 1967 44.8 1987 52.6\n1908 20.1 1928 45.5 1948 37.1 1968 53.8 1988 15.5\n1909 37.0 1929 31.4 1949 32.0 1969 48.8 1989 39.2\nTable 1.1: NOAA Annual Snowfall Data (in inches)\nin 1992, 1993 and 1995, I ask whether they indicate that winters in the more recent years are snowier than those in", "metadata": {"page": 6}}, {"page_content": "the \ufb01rst half of the record. Thus, I want to compare the snow falls in the years 1890-1945 with those in the years\n1946\u20132001.\n1.2 Data mining\nOne way to accomplish this is to just plot the snowfall amounts in the two cases and see if there is any evident\ndifference in the two plots. These plots are shown in Figure 1.1. Mmmmm. These pictures don\u2019t help much. What I\nneed are criteria for discerning when two data sets are distinctly different.", "metadata": {"page": 6}}, {"page_content": "need are criteria for discerning when two data sets are distinctly different.\nOne approach is to introduce some numerical feature of a data set that can then be compared. There are two such\ninvariants that you will often see used. The \ufb01rst is the mean, this the average of the data values. Thus, if a given data\n1See the website http://www.erh.noaa.gov/er/box/climate/BOS.SNW.\n1.1. Snowfall data 3", "metadata": {"page": 6}}, {"page_content": "1895 1905 1915 1925 1935 1945\n0\n20\n40\n60\n80\n100\n120\nYear\nSnowf\nall (inches)\n. .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": "\u2022\u2022\n\u2022\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n1950 1960 1970 1980 1990 2000\n0\n20\n40\n60\n80\n100\n120\nYear\nSnowf\nall (inches)\n. .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "metadata": {"page": 7}}, {"page_content": "\u2022\n\u2022\n\u2022\u2022\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\nFigure 1.1: Snowfall Data for Years 1890\u20131945 (left) and 1946\u20132001 (right)\nset consists of an ordered list of of some N numbers, {x1,...,x N}, the mean is\n\u00b5= 1\nN (x1 + x2 + \u00b7\u00b7\u00b7 + xN) (1.1)\nIn the cases at hand,\n\u00b51890-1945 = 42.4 and \u00b51946-2001 = 42.3. (1.2)\nThese are pretty close! But, of course, I don\u2019t know how close two means must be for me to say that there is no", "metadata": {"page": 7}}, {"page_content": "statistical difference between the data sets. Indeed, two data sets can well have the same mean and look very different.\nFor example, consider that the three element sets{\u22121,0,1}and {\u221210,0,10}. Both have mean zero, but the spread of\nthe values in one is very much greater than the spread of the values in the other.\nThe preceding example illustrates the fact that means are not necessarily good criteria to distinguish data sets. Looking", "metadata": {"page": 7}}, {"page_content": "at this last toy example, I see that these two data sets are distinguished in some sense by the spread in the data; by\nhow far the points differ from the mean value. Thestandard deviation is a convenient measure of this difference. It is\nde\ufb01ned to be\n\u03c3=\n\u221a\n1\nN \u22121\n(\n(x1 \u2212\u00b5)2 + (x2 \u2212\u00b5)2 + \u00b7\u00b7\u00b7 + (xN \u2212\u00b5)2)\n. (1.3)\nThe standard deviations for the two snowfall data sets are\n\u03c31890-1945 = 16.1 and \u03c31946-2001 = 21.4. (1.4)", "metadata": {"page": 7}}, {"page_content": "\u03c31890-1945 = 16.1 and \u03c31946-2001 = 21.4. (1.4)\nWell, these differ by roughly 5 inches, but is this difference large enough to be signi\ufb01cant? How much difference\nshould I tolerate so as to maintain that the snowfall amounts are \u201cstatistically\u201d identical? How much difference in\nstandard deviations signals a signi\ufb01cant difference in yearly snowfall?\nI can also \u201cbin\u201d the data. For example, I can count how many years have total snow fall less than 10 inches, then how", "metadata": {"page": 7}}, {"page_content": "many 10\u201320 inches, how many 20-30 inches, etc. I can do this with the two halves of the data set and then compare\nbin heights. Here is the result:\n1890\u20131945: 1 2 11 11 16 5 6 4 0 0 0\n1946\u20132001: 0 8 11 9 11 7 5 0 3 1 1 (1.5)\nHaving binned the data, I am yet at a loss to decide if the difference in bin heights really signi\ufb01es a distinct difference\nin snow fall between the two halves of the data set.\n4 Chapter 1. Data Exploration", "metadata": {"page": 7}}, {"page_content": "What follows is one more try at a comparison of the two halves; it is called therank-sum test and it works as follows: I\ngive each year a number, between 1 and 112, by ranking the years in order of increasing total snow-fall. For example,\nthe year with rank 1 is 1936 and the years with rank 109 and 110 are 1993 and 1995. I now sum all of the ranks for\nthe years 1890-1945 to get the rank-sum for the \ufb01rst half of the data set. I then do the same for the years 1946-2001", "metadata": {"page": 8}}, {"page_content": "to get the rank-sum for the latter half. I can now compare these two numbers. If one is signi\ufb01cantly larger than the\nother, the data set half with the larger rank-sum has comparatively more high snowfall years than that with the smaller\nrank-sum. This understood, here are the two rank-sums:\nrank-sum1890-1945 = 3137 and rank-sum 1946-2001 = 3121. (1.6)\nThus, the two rank-sums differ by 16. But, I am again faced with the following question: Is this difference signi\ufb01cant?", "metadata": {"page": 8}}, {"page_content": "How big must the difference be to conclude that the \ufb01rst half of the 20\u2019th century had, inspite of 1995, more snow on\naverage, than the second half?\nTo elaborate now on this last question, consider that there is a hypothesis on the table:\nThe rank-sums for the two halves of the data set indicate that there is a signi\ufb01cant difference between the\nsnowfall totals from the \ufb01rst half of the data set as compared with those from the second.", "metadata": {"page": 8}}, {"page_content": "snowfall totals from the \ufb01rst half of the data set as compared with those from the second.\nTo use the numbers in (1.6) to analyze the validity of this hypothesis, I need an alternate hypothesis for comparison.\nThe comparison hypothesis plays the role here of the control group in an experiment. This \u201ccontrol\u201d is called thenull\nhypothesis in statistics. In this case, the null-hypothesis asserts that the rankings are random. Thus, the null-hypothesis\nis:", "metadata": {"page": 8}}, {"page_content": "is:\nThe 112 ranks are distributed amongst the years as if they were handed out by a blindfolded monkey\nchoosing numbers from a mixed bin.\nSaid differently, the null-hypothesis asserts that the rank-sums in (1.6) are statistically indistinguishable from those\nthat I would obtain I were to randomly select56 numbers from the set {1,2,3,..., 112}to use for the rankings of the\nyears in the \ufb01rst half of the data set, while using the remaining numbers for the second half.", "metadata": {"page": 8}}, {"page_content": "years in the \ufb01rst half of the data set, while using the remaining numbers for the second half.\nAn awful lot is hidden here in the phrase statistically indistinguishable. Here is what this phrase means in the case\nat hand: I should compute the probability that the sum of 56 randomly selected numbers from the set {1,2,..., 112}\ndiffers from the sum of the 56 numbers that are left by at least 16. If this probability is very small, then I have some", "metadata": {"page": 8}}, {"page_content": "indication that the snow fall totals for the years in the two halves of the data set differ in a signi\ufb01cant way. If the\nprobability is high that the rank-sums for the randomly selected rankings differ by 16 or more, then the difference\nindicated in (1.6) should not be viewed as indicative of some statistical difference between the snowfall totals for the\nyears in the two halves of the data set.\nThus, the basic questions are:", "metadata": {"page": 8}}, {"page_content": "years in the two halves of the data set.\nThus, the basic questions are:\n\u2022What is the probability in the case of the null-hypothesis that I should get a difference that is bigger than the\none that I actually got?\n\u2022What probability should be considered \u201csigni\ufb01cant\u201d?\nOf course, I can ask these same two questions for the bin data in (1.5). I can also ask analogs of these question for the", "metadata": {"page": 8}}, {"page_content": "two means in (1.2) and for the two standard deviations in (1.4). However, because the bin data, as well as the means\nand standard deviations deal with the snowfall amounts rather than with integer rankings, I would need a different sort\nof de\ufb01nition to use for the null-hypothesis in the latter cases.\nIn any event, take note that the \ufb01rst question is a mathematical one and the second is more of a value choice. The \ufb01rst", "metadata": {"page": 8}}, {"page_content": "question leads us to study the theory of probability which is the topic in the next chapter. As for the second question, I\ncan tell you that it is the custom these days to take 1\n20 = 0.05 as the cut-off between what is signi\ufb01cant and what isn\u2019t.\nThus,\n1.2. Data mining 5", "metadata": {"page": 8}}, {"page_content": "If the probability of seeing a larger difference in values than the one seen is less than 0.05, then the\nobserved difference is deemed to be \u201csigni\ufb01cant\u201d.\nThis choice of 5 percent is rather arbitrary, but such is the custom.\n1.3 Exercises:\n1. This exercise requires ten minutes of your time on two successive mornings. It also requires a clock that tells\ntime to the nearest second.\n(a) On the \ufb01rst morning, before eating or drinking, record the following data: Try to estimate the passage of", "metadata": {"page": 9}}, {"page_content": "precisely 60 seconds of time with your eyes closed. Thus, obtain the time from the clock, immediately\nclose your eyes and when you feel that 1 minute has expired, open them and immediately read the amount\nof time that has passed on the clock. Record this as your \ufb01rst estimate for 1 minute of time. Repeat this\nprocedure ten time to obtain ten successive estimates for 1 minute.\n(b) On the second morning, repeat this part (a), but \ufb01rst drink a caffeinated beverage such as coffee, tea, or a", "metadata": {"page": 9}}, {"page_content": "cola drink.\n(c) With parts (a) and (b) completed, you have two lists of ten numbers. Compute the means and standard\ndeviations for each of these data sets. Then, combine the data sets as two halves of a single list of 20\nnumbers and compute the rank-sums for the two lists. Thus, your rankings will run from 1 to 20. In the\nevent of a tie between two estimates, give both the same ranking and don\u2019t use the subsequent ranking.", "metadata": {"page": 9}}, {"page_content": "For example, if there is a tie for \ufb01fth, use 5 for both but give the next highest estimate 7 instead of 6.\n2. Flip a coin 200 times. Use n1 to denote the number of heads that appeared in \ufb02ips 1-10, use n2 to denote\nthe number that appeared in \ufb02ips 11-20, and so on. In this way, you generate twenty numbers, {n1,...n 20}.\nCompute the mean and standard deviation for the sets {n1,...n 10}, {n11,...n 20}, and {n1,...n 20}.", "metadata": {"page": 9}}, {"page_content": "Compute the mean and standard deviation for the sets {n1,...n 10}, {n11,...n 20}, and {n1,...n 20}.\n3. The table that follows gives the results of US congressional elections during the 6th year of a President\u2019s term in\nof\ufb01ce. (Note: he had to be reelected.) A negative number means that the President\u2019s party lost seats. Note that\nthere aren\u2019t any positive numbers. Compute the mean and standard deviation for both the Senate and House of", "metadata": {"page": 9}}, {"page_content": "Representatives. Compare these numbers with the line for the 2006 election.\nYear President Senate House\n2006 Bush \u22126 \u221230\n1998 Clinton 0 \u22125\n1986 Reagan \u22128 \u22125\n1974 Ford \u22125 \u221248\n1966 Johnson \u22124 \u221247\n1958 Eisenhower \u221213 \u221248\n1950 Truman \u22126 \u221229\n1938 Roosevelt \u22126 \u221271\n1926 Coolidge \u22126 \u221210\n1918 Wilson \u22126 \u221219\nTable 1.2: Number of seats gained by the president\u2019s party in the election during his sixth year in of\ufb01ce\n6 Chapter 1. Data Exploration", "metadata": {"page": 9}}, {"page_content": "CHAPTER\nTWO\nBasic notions from probability theory\nProbability theory is the mathematics of chance and luck. To elaborate, its goal is to make sense of the following\nquestion:\nWhat is the probability of a given outcome from some set of possible outcomes?\nFor example, in the snow fall analysis of the previous chapter, I computed the rank-sums for the two halves of the\ndata set and found that they differed by 16. I then wondered what the probability was for such rank sums to differ by", "metadata": {"page": 10}}, {"page_content": "more than 16 if the rankings were randomly selected instead of given by the data. We shall eventually learn what it\nmeans to be \u201crandomly selected\u201d and how to compute such probabilities. However, this comes somewhat farther into\nthe course.\n2.1 Talking the talk\nUnfortunately for the rest of us, probability theory has its own somewhat arcane language. What follows is a list of\nthe most signi\ufb01cant terms. Treat this aspect of the course as you would any other language course. In any event, there", "metadata": {"page": 10}}, {"page_content": "are not so many terms, and you will soon \ufb01nd that you don\u2019t have to look back at your notes to remember what they\nmean.\nSample space: A sample space is the set of all possible outcomes of the particular \u201cexperiment\u201d of interest. For\nexample, in the rank-sum analysis of the snowfall data from the previous chapter, I should consider the sample space\nto be the set of all collections of 56 distinct integers from the collection {1,..., 112}.", "metadata": {"page": 10}}, {"page_content": "to be the set of all collections of 56 distinct integers from the collection {1,..., 112}.\nFor a second example, imagine \ufb02ipping a coin three times and recording the possible outcomes of the three \ufb02ips. In\nthis case, the sample space is\nS = {TTT,TTH,THT,HTT,THH,HTH,HHT,HHH }. (2.1)\nHere is a third example: If you are considering the possible birthdates of a person drawn at random, the sample space", "metadata": {"page": 10}}, {"page_content": "consists of the days of the year, thus the integers from 1 to 366. If you are considering the possible birthdates of two\npeople selected at random, the sample space consists of all pairs of the form(j,k) where j and kare integers from 1\nto 366. If you are considering the possible birthdates of three people selected at random, the sample space consists of\nall triples of the form (j,k,m) where j, kand mare integers from 1 to 366.", "metadata": {"page": 10}}, {"page_content": "all triples of the form (j,k,m) where j, kand mare integers from 1 to 366.\nMy fourth example comes from medicine: Suppose that you are a pediatrician and you take the pulse rate of a 1-year\nold child? What is the sample space? I imagine that the number of beats per minute can be any number between0 and\nand some maximum, say 300.\nTo reiterate: The sample space is no more nor less than the collection of all possible outcomes for your experiment.\n7", "metadata": {"page": 10}}, {"page_content": "Events: An event is a subset of the sample space, thus a subset of possible outcomes for your experiment. In the\nrank-sum example, where the sample space is the set of all collections of 56 distinct integers from 1 through 112,\nhere is one event: The subset of collections of 56 integers whose sum is 16 or more greater than the sum of those that\nremain. Here is another event: The subset that consists of the 56 consecutive integers that start at 1. Notice that the", "metadata": {"page": 11}}, {"page_content": "\ufb01rst event contains lots of collections of 56 integers, but the second event contains just{1,2,..., 56}. So, the \ufb01rst\nevent has more elements than the second.\nConsider the case where the sample space is the set of outcomes of two \ufb02ips of a coin, thusS = {HH,HT,TH,TT }.\nFor a small sample space such as this, one can readily list all of the possible events. In this case, there are 16 possible", "metadata": {"page": 11}}, {"page_content": "events. Here is the list: First comes the no element set, this denoted by tradition as \u2205. Then comes the 4 sets with\njust one element, these consist of {HH}, {HT}, {TH}, {TT}. Next come the 6 two element sets, {HH,HT },\n{HH,TH }, {HH,TT }, {HT,TH }, {HT,TT }, {TH,TT }. Note that the order of the elements is of no conse-\nquence; the set {HH,HT }is the same as the set {HT,HH }. The point here is that we only care about the elements,", "metadata": {"page": 11}}, {"page_content": "not how they are listed. To continue, there are 4 distinct sets with three elements, {HH,HT,TH }, {HH,HT,TT },\n{HH,TH,TT }and {HT,TH,TT }. Finally, there is the set with all of the elements, {HH,HT,TH,TT }.\nNote that a subset of the sample space can have no elements, or one element, or two, . . . , up to and including all of the\nelements in the sample space. For example, if the sample space is that given in (1.1) for \ufb02ipping a coin three times,", "metadata": {"page": 11}}, {"page_content": "then HTH is an event. Meanwhile, the event that a head appears on the \ufb01rst \ufb02ip is{HTT,HHT,HTH,HHH }, a\nset with four elements. The event that four heads appears has zero elements, and the set where there are less than four\nheads is the whole sample space. No matter what the original sample space, the event with no elements is called the\nempty set, and is denoted by\u2205.\nIn the case where the sample space consists of the possible pulse rate measurements of a 1-year old, some events are:", "metadata": {"page": 11}}, {"page_content": "The event that the pulse rate is greater than 100. The event that the pulse rate is between80 and 85. The event that the\npulse rate is either between 100 and 110 or between 115 and 120. The event that the pulse rate is either 85 or 95 or\n105. The event that the pulse rate is divisible by 3. And so on.\nBy the way, this last example illustrates the fact that there are many ways to specify the elements in the same event.", "metadata": {"page": 11}}, {"page_content": "Consider, for example, the event that the pulse rate is divisible by3. Let\u2019s call this event E. Another way to E is to\nprovide a list of all of its elements, thus E = {0,3,6,..., 300}. Or, I can use a more algebraic tone: E is the set of\nintegers xsuch that 0 \u2264x\u2264300 and x/3 \u2208{0,1,2,..., 100}. (See below for the de\ufb01nition of the symbol \u201c\u2208\u201d.) For\nthat matter, I can describe Eaccurately using French, Japanese, Urdu, or most other languages.", "metadata": {"page": 11}}, {"page_content": "that matter, I can describe Eaccurately using French, Japanese, Urdu, or most other languages.\nTo repeat: Any given subset of a given sample space is called an event.\nSet Notation: Having introduced the notion of a subset of some set of outcomes, you need to become familiar with\nsome standard notation that is used in the literature when discussing subsets of sets.\n(a) As mentioned above, \u2205is used to denote the \u201cset\u201d with no elements.", "metadata": {"page": 11}}, {"page_content": "(a) As mentioned above, \u2205is used to denote the \u201cset\u201d with no elements.\n(b) If Aand Bare subsets, then A\u222aBdenotes the subset whose elements are those that appear either in Aor in B\nor in both. This subset is called the union of Aand B.\n(c) Meanwhile, A\u2229B denotes the subset whose elements appear in both A and B. It is called the intersection\nbetween Aand B.\n(d) If no elements are shared by Aand B, then these two sets are said to be disjoint. Thus, Aand Bare disjoint if\nand only if A\u2229B = \u2205.", "metadata": {"page": 11}}, {"page_content": "and only if A\u2229B = \u2205.\n(e) If A is given as a subset of a set S, then Ac denotes the subset of Swhose elements are not in A. Thus, Ac and\nAare necessarily disjoint and Ac \u222aA= S. The set Ac is called the complement of A.\n(f) If a subset Ais entirely contained in another subset, B, one writes A \u2282B. For example, if Ais an event in a\nsample space S, then A\u2282S.\n(g) If an element, e, is contained in a set A, one writes e\u2208A. If eis not in A, one writes e\u0338\u2208A.", "metadata": {"page": 11}}, {"page_content": "(g) If an element, e, is contained in a set A, one writes e\u2208A. If eis not in A, one writes e\u0338\u2208A.\n8 Chapter 2. Basic notions from probability theory", "metadata": {"page": 11}}, {"page_content": "What follows are some examples that are meant to illustrate what is going on. suppose that the sample space is the\nset of possible pulse rates of a 1-year old child. Lets us take this set to be {0,1,..., 300}. Consider the case where\nAis the set of elements that are at least 100, and B is the set of elements that are greater than 90 but less than 110.\nThus, A = {100,101,..., 300}, and B = {91,..., 109}. The union of Aand B is the set {91,92,..., 300}. The", "metadata": {"page": 12}}, {"page_content": "intersection of Aand B is the set{100,101,..., 109}. The complement of Ais the set{0,1,..., 99}. The complement\nof Bis the set{0,1,..., 90,110,111,..., 300}. (Note that any given set is disjoint from its complement.) Meanwhile,\n110 \u2208Abut 110 \u0338\u2208B.\n2.2 Axiomatic de\ufb01nition of probability\nA probability function for a given sample space assigns the probabilities to various subsets. For example, if I am", "metadata": {"page": 12}}, {"page_content": "\ufb02ipping a coin once, I would take my sample space to be the set {H,T }. If the coin were fair, I would use the\nprobability function that assigns 0 to the empty set, 1\n2 to each of the subsets {H}and {T}, and then 1 to the whole of\nS. If the coin were biased a bit towards landing heads up, I might give {H}more than 1\n2 and {T}less than 1\n2 .\nThe choice of a probability function is meant to quantify what is meant by the term \u201cat random\u201d. For example, consider", "metadata": {"page": 12}}, {"page_content": "the case for choosing just one number \u201cat random\u201d from the set{1,..., 112}. If \u201cat random\u201d is to mean that that there\nis no bias towards any particular number, then my probability function should assign to each subset that consists of\njust a single integer. Thus, it gives to the subsets{1}, {2}, . . . , etc. If I mean something else by my use of the term \u201cat\nrandom\u201d, then I would want to use a different assignment of probabilities.", "metadata": {"page": 12}}, {"page_content": "random\u201d, then I would want to use a different assignment of probabilities.\nTo explore another example, consider the case where the sample space represents the set of possible pulse rate mea-\nsurements for a 1-year old child. Thus, Sis the set whose elements are {0,1,..., 300}. As a pediatrician, you would\nbe interested in the probability for measuring a given pulse rate. I expect that this probability is not the same for all", "metadata": {"page": 12}}, {"page_content": "of the elements. For example, the number 20 is certainly less probable than the number 90. Likewise, 190 is certainly\nless probable than 100. I expect that the probabilities are greatest for numbers between 80 and 120, and then decrease\nrather drastically away from this interval.\nHere is the story in the generic, abstract setting: Imagine that we have a particular sample space, S, in mind. A", "metadata": {"page": 12}}, {"page_content": "probability function, P, is an assignment of a number no less than 0 and no greater than 1 to various subsets of S\nsubject to two rules:\n\u2022P(S) = 1.\n\u2022P(A\u222aB) = P(A) + P(B) when A\u2229B = \u2205.\n(2.2)\nNote that condition P(S ) = 1 says that there is probability 1 of at least something happening. Meanwhile, the\ncondition P(A\u222aB) = P(A) +P(B) when Aand Bhave no points in common asserts the following: The probability", "metadata": {"page": 12}}, {"page_content": "of something happening that is in either A or B is the sum of the probabilities of something happening from A or\nsomething happening from B.\nTo give an example, consider rolling a standard, six-sided die. If the die is rolled once, the sample space consists of\nthe numbers {1,2,3,4,5,6}. If the die is fair, then I would want to use the probability function that assigns the value\n1", "metadata": {"page": 12}}, {"page_content": "1\n6 to each element. But, what if the die is not fair? What if it favors some numbers over others? Consider, for example,\na probability function with P({1}) = 0, P({2}) = 1\n3 , P({3}) = 1\n2 , P({4}) = 1\n6 , P({5}) = 0 and P({6}) = 0. If this\nprobability function is correct for my die, what is the most probable number to appear with one roll? Should I expect\nto see the number 5 show up at all? What follows is a more drastic example: Consider the probability function where", "metadata": {"page": 12}}, {"page_content": "P({1}) = 1 and P({2}) = P({3}) = P({4}) = P({5}) = P({6}) = 0. If this probability function is correct, I should\nexpect only the number 1 to show up.\nLet us explore a bit the reasoning behind the conditions for P that appear in equation (2.2). To start, you should\nunderstand why the probability of an event is not allowed to be negative, nor is it allowed to be greater than1. This is", "metadata": {"page": 12}}, {"page_content": "to conform with our intuitive notion of what probability means. To elaborate, think of the sample space as the suite of\npossible outcomes of an experiment. This can be any experiment, for example \ufb02ipping a coin three times, or rolling a\ndie once, or measuring the pulse rate of a1-year old child. An event is a subset of possible outcomes. Let us suppose\n2.2. Axiomatic de\ufb01nition of probability 9", "metadata": {"page": 12}}, {"page_content": "that we are interested in a certain event, this a subset denoted by A. The probability function assigns to Aa number,\nP(A). This number has the following interpretation:\nIf the experiment is carried out a large number of times, with the conditions and set up the same each\ntime, then P(A) is a prediction for the fraction of those experiments where the outcome is in the set A.\nAs this fraction can be at worse 0 (no outcomes in the set A), or at best 1 (all outcomes in the set A), so P(A) should", "metadata": {"page": 13}}, {"page_content": "be a number that is not less than 0 nor more than 1.\nWhy should P(S) be equal to 1 in all cases? Well, by virtue of its very de\ufb01nition, the set Sis supposed to be the set of\nall possible outcomes of the experiment. The requirement for P(S ) to equal 1 makes the probability function predict\nthat each outcome must come from our list of all possible outcomes.\nThe second condition that appears in equation (2.2) is less of a tautology. It is meant to model a certain intuition that", "metadata": {"page": 13}}, {"page_content": "we all have about probabilities. Here is the intuition: The probability of a given event is the sum of the probabilities\nof its constituent elements. For example, consider the case where the sample set is the set of possible outcomes when\nI roll a fair die. Thus, the probability is 1\n6 for any given integer from {1,2,3,4,5,6}appearing. Let Adenote the\nprobability of {1}appearing and B the probability of {2}appearing. I expect that the probability of either 1 or 2", "metadata": {"page": 13}}, {"page_content": "appearing, thus A\u222aB = {1,2}, is 1\n6 + 1\n6 = 1\n3 . I would want my probability function to re\ufb02ect this addititivity. The\nsecond condition in equation (2.2) asserts no more nor less than this requirement.\nBy the way, the condition forA\u2229B = \u2205is meant to prevent over-counting. For an extreme example, supposeA= {1}\nand Bis also {1}. Thus both have probability 1\n6 . Meanwhile, A\u222aB = {1}also, so P(A\u222aB) should be 1\n6 , not 1\n6 + 1\n6 .", "metadata": {"page": 13}}, {"page_content": "6 . Meanwhile, A\u222aB = {1}also, so P(A\u222aB) should be 1\n6 , not 1\n6 + 1\n6 .\nHere is a somewhat less extreme example: Suppose that A= {1,2}and B = {2,3}. Both of these sets should have\nprobability . Their union is {1,2,3}. I expect that this set has probability 1\n2 , not 1\n3 + 1\n3 = 2\n3 . The reason I shouldn\u2019t\nuse the formula P(A \u222aB) = P(A) + P(B) for the case where A = {1,2}and B = {2,3}is because the latter", "metadata": {"page": 13}}, {"page_content": "formula counts twice the probability of the shared integer 2; it counts it once from its appearance in Aand again from\nits appearance in B.\nFor a second illustration, consider the case where S is the set of possible pulse rates for a 1-year old child. Take A\nto be the event {100,..., 109}and Bto be the event {120,..., 129}. Suppose that many years of pediatric medicine\nhave given us a probability function, P, for this set. Suppose, in addition that P(A) = 1\n4 and P(B) = 1\n16 . What should", "metadata": {"page": 13}}, {"page_content": "4 and P(B) = 1\n16 . What should\nwe expect for the probability that a measured pulse rate is either in Aor in B? That is, what is the probability that the\npulse rate is in A\u222aB? Since Aand B do not share elements (A \u2229B = \u2205), you might expect that the probability of\nbeing in either set is the sum of the probability of being in Awith that of being in B, thus 5\n16 .\nKeeping this last example in mind, consider the set {109,110,111}. I\u2019ll call this set C. Suppose that our probability", "metadata": {"page": 13}}, {"page_content": "function says that P(C) = 1\n64 . I would not predict that P(A \u222aC) = P(A) + P(C) since Aand C both contain the\nelement 109. Thus, I can imagine that P(A)+P(C) over-counts the probability forA\u222aCsince it counts the probability\nof 109 two times, once from its membership in Aand again from its membership in C.\nI started the discussion prior to equation (2.2) by asking that you imagine a particular sample space and then said that", "metadata": {"page": 13}}, {"page_content": "a probability function on this space is a rule that assigns to each event a number no less than zero and no greater than\n1 to each subspace (event) of the sample space, but subject to the rules that are depicted in (2.2). I expect that many of\nyou are silently asking the following question:\nWho or what determines the probability function P?\nTo make this less abstract, consider again the case of rolling a six-sided die. The corresponding sample space is", "metadata": {"page": 13}}, {"page_content": "S= {1,2,3,4,5,6}. I noted above three different probability functions for S. The \ufb01rst assigned equal probability\nto each element. The second and third assigned different probabilities to different elements. The fact is that there are\nin\ufb01nitely many probability functions to choose from. Which should be used?\nTo put the matter in even starker terms, consider the case where the sample space consists of the possible outcomes", "metadata": {"page": 13}}, {"page_content": "of a single coin \ufb02ip. Thus, S = {H,T }. A probability function on S is no more nor less than an assignment of one\nnumber, P(H), that is not less than 0 nor greater than 1. Only one number is needed because the \ufb01rst line of (2.2)\nmakes P assign 1 to S, and the second line of (2.2) makes P assign 1 \u2212P(H) to T. Thus, P(T) = 1 \u2212P(H). If you\nunderstand this last point, then it follows that there are as many probability functions for the set S = {H,T }as there", "metadata": {"page": 13}}, {"page_content": "10 Chapter 2. Basic notions from probability theory", "metadata": {"page": 13}}, {"page_content": "are real numbers in the interval between 0 and 1 (including the end-points). By any count, there are in\ufb01nitely many\nsuch numbers!\nSo, I have in\ufb01nitely many choices for P. Which should I choose? So as to keep the abstraction to a minimum, let\u2019s\naddress this question to the coin \ufb02ipping case where S = {H,T }. It is important to keep in mind what the purpose of\na probability function is: The probability function should accurately predict the relative frequencies of heads and tails", "metadata": {"page": 14}}, {"page_content": "that appear when a given coin is \ufb02ipped a large number of times.\nGranted this goal, I might proceed by \ufb01rst \ufb02ipping the particular coin some large number of times to generate an\n\u201cexperimentally\u201d determined probability. This I\u2019ll call PE. I then use P E to predict probabilities for all future \ufb02ips\nof this coin. For example, if I \ufb02ip the coin 100 times and \ufb01nd that 44 heads appear, then I might set P E(H) = 0.44", "metadata": {"page": 14}}, {"page_content": "to predict the probabilities for all future \ufb02ips. By the way, we instinctively use experimentally determined probability\nfunctions constantly in daily life. However, we use a different, but not unrelated name for this: We call itexperience.\nThere is a more theoretical way to proceed. I might study how coins are made and based on my understanding of\ntheir construction, deduce a \u201ctheoretically\u201d determined probability. I\u2019ll call this PT. For example, I might deduce that", "metadata": {"page": 14}}, {"page_content": "PT(H) = 0.5. I might then use PT to compute all future probabilities.\nAs I \ufb02ip this coin in the days ahead, I may \ufb01nd that one or the other of these probability functions is more accurate.\nOr, I may suspect that neither is very accurate. How I judge accuracy will lead us to the subject of Statistics.\n2.3 Computing probabilities for subsets\nIf your sample space is a \ufb01nite set, and if you have assigned probabilities to all of the one element subsets from", "metadata": {"page": 14}}, {"page_content": "your sample space, then you can compute the probabilities for all events from the sample space by invoking the rules\nin (2.2). Thus,\nIf you know what P assigns to each element in S, then you know P on every subset: Just add up the\nprobabilities that are assigned to its elements.\nWe\u2019ll talk about the story when S isn\u2019t \ufb01nite later. Anyway, the preceding illustrates the more intuitive notion of", "metadata": {"page": 14}}, {"page_content": "probability that we all have: It says simply that if you know the probability of every outcome, then you can compute\nthe probability of any subset of outcomes by summing up the probabilities of the outcomes that are in the given subset.\nFor example, in the case where my sample space S = {1,..., 112}and each integer in S has probability 1\n112 , then I\ncan compute that the probability of a blindfolded monkey picking either 1 or 2 is 1\n112 + 1\n112 = 1\n112 . Here I invoke the", "metadata": {"page": 14}}, {"page_content": "112 + 1\n112 = 1\n112 . Here I invoke the\nsecond of the rules in (2.2) where Ais the event that 1 is chosen and Bis the event that 2 is chosen. A sequential use\nof this same line of reasoning \ufb01nds that the probability of picking an integer that is less than or equal to 10 is 10\n112 .\nHere is a second example: TakeSto be the set of outcomes for \ufb02ipping a fair coin three times (as depicted in (2.1)). If", "metadata": {"page": 14}}, {"page_content": "the coin is fair and if the three \ufb02ips are each fair, then it seems reasonable to me that the situation is modeled using the\nprobability function, P, that assigns to each element in the set S. If we take this version of P, then we can use the rule\nin (2.2) to assign probabilities 1\n8 to any given subset of S. For example, the subset given by {HHT,HTH,THH }\nhas probability 3\n8 since\nP({HHT,HTH,THH }) = P({HHT,HTH }) + P(THH)\nby invoking (2.2). Invoking it a second time \ufb01nds", "metadata": {"page": 14}}, {"page_content": "P({HHT,HTH,THH }) = P({HHT,HTH }) + P(THH)\nby invoking (2.2). Invoking it a second time \ufb01nds\nP({HHT,HTH }) = P(HHT) + P(HTH),\nand so\nP({HHT,HTH,THH }) = P(HHT) + P(HTH) + P(THH) = 3\n8 .\nTo summarize: If the sample space is a set with \ufb01nite elements, or is a discrete set (such as the positive integers), then\nyou can \ufb01nd the probability of any subset of the sample space if you know the probability for each element.\n2.3. Computing probabilities for subsets 11", "metadata": {"page": 14}}, {"page_content": "2.4 Some consequences of the de\ufb01nition\nHere are some consequences of the de\ufb01nition of probability.\n(a) P(\u2205) = 0.\n(b) P(A \u222aB) = P(A) + P(B) \u2212P(A\u2229B).\n(c) P(A) \u2264P(B) if Ais contained entirely in B.\n(d) P(B) = P(B\u2229A) + P(B\u2229Ac).\n(e) P(Ac) = 1 \u2212P(A).\n(2.3)\nIn the preceding, Ac is the set of elements that are not in A. The set Ac is called the complement of A.\nI want to stress that all of these conditions are simply translations into symbols of intuition that we all have about", "metadata": {"page": 15}}, {"page_content": "probabilities. What follows are the respective English versions of (2.3).\nEquation (2.3a):\nThe probability that no outcomes appear is zero.\nThis is to say that if S is, as required, the list of all possible outcomes, then at least one outcome must occur.\nEquation (2.3b):\nThe probability that an outcome is in either Aor B is the probability that it is in Aplus the probability\nthat it is in Bminus the probability that it is in both.", "metadata": {"page": 15}}, {"page_content": "that it is in Bminus the probability that it is in both.\nThe point here is that if A and B have elements in common, then one is overcounting to obtain P(A \u222aB) by just\nsumming the two probabilities. The sum of P(A) and P(B) counts twice the elements that are both in Aand in B\ncount twice. To see how this works, consider the rolling a standard, six-sided die where the probabilities of any given", "metadata": {"page": 15}}, {"page_content": "side appearing are all the same, thus . Now consider the case where Ais the event that either 1 or 2 appears, while B\nis the event that either 2 or 3 appears. The probability assigned to Ais 1\n3 , that assigned to B is also 1\n3 . Meanwhile,\nA\u222aB = {1,2,3}has probability 1\n2 and A\u2229B = {2}has probability 1\n6 . Since 1\n2 = 1\n3 + 1\n3 \u22121\n6 , the claim in (2.3b)\nholds in this case. You might also consider (2.3b) in a case where A= B.\nEquation (2.3c):", "metadata": {"page": 15}}, {"page_content": "holds in this case. You might also consider (2.3b) in a case where A= B.\nEquation (2.3c):\nThe probability of an outcome from Ais no greater than that of an outcome from B in the case that all\noutcomes from Aare contained in the set B.\nThe point of (2.3c) is simply that if every outcome fromAappears in the set B, then the probability that Boccurs can\nnot be less than that of A. Consider for example the case of rolling one die that was just considered. Take Aagain to", "metadata": {"page": 15}}, {"page_content": "be {1,2}, but now take Bto be the set {1,2,3}. Then P(A) is less than P(B) because Bcontains all of A\u2019s elements\nplus another. Thus, the probability of Boccurring is the sum of the probability of Aoccurring and the probability of\nthe extra element occurring.\n12 Chapter 2. Basic notions from probability theory", "metadata": {"page": 15}}, {"page_content": "Equation (2.3d):\nThe probability of an outcome from the set B is the sum of the probability that the outcome is in the\nportion of Bthat is contained in Aand the probability that the outcome is in the portion of Bthat is not\ncontained in A.\nThis translation of (2.3d) says that if I break B into two parts, the part that is contained in Aand the part that isn\u2019t,\nthen the probability that some element from Bappears is obtained by adding, \ufb01rst the probability that an element that", "metadata": {"page": 16}}, {"page_content": "is both in A and B appears, and then the probability that an element appears that is in B but not in A. Here is an\nexample from rolling one die: Take A = {1,2,4,5}and B = {1,2,3,6}. Since B has four elements and each has\nprobability 1\n6 , so B has probability 2\n3 . Now, the elements that are both in B and in Acomprise the set {1,2}, and\nthis set has probability 1\n3 . Meanwhile, the elements in B that are not in Acomprise the set {3,6}. This set also has\nprobability 1", "metadata": {"page": 16}}, {"page_content": "probability 1\n3 . Thus (2.3d) holds in this case because 1\n3 + 1\n3 = 2\n3 .\nEquation (2.3e):\nThe probability of an outcome that is not in Ais equal to 1 minus the probability that an outcome is in A.\nTo see why this is true, break the sample space up into two parts, the elements inAand the elements that are not in A.\nThe sum of the corresponding two probabilities must equal 1 since any given element is either in Aor not. Consider", "metadata": {"page": 16}}, {"page_content": "our die example where A= {1,2}. Then Ac = {3,4,5,6}and their probabilities do indeed add up to 1.\n2.5 That\u2019s all there is to probability\nYou have just seen most of probability theory for sample spaces with a \ufb01nite number of elements. There are a few new\nnotions that are introduced later, but a good deal of what follows concerns either various consequences of the notions\nthat were just introduced, or else various convenient ways to calculate probabilities that arise in common situations.", "metadata": {"page": 16}}, {"page_content": "Before moving on, it is important to explicitly state something that has been behind the scenes in all of this: When you\ncome to apply probability theory, the sample space and its probability function are chosen by you, the scientist, based\non your understanding of the phenomena under consideration. Although there are often standard and obvious choices\navailable, neither the sample space nor the probability function need be god given. The particular choice constitutes a", "metadata": {"page": 16}}, {"page_content": "theoretical assumption that you are making in your mental model of what ever phenomena is under investigation.\nTo return to an example I mentioned previously, if I \ufb02ip a coin once and am concerned about how it lands, I might\ntake for S the two element set {H,T }. If I think that the coin is fair, I would take my probability function P so that\nP(H) = 1\n2 and P(T) = 1\n2 . However, if I have reason to believe that the coin is not fair, then I should choose P", "metadata": {"page": 16}}, {"page_content": "2 . However, if I have reason to believe that the coin is not fair, then I should choose P\ndifferently. Moreover, if I have reason to believe that the coin can sometimes land on its edge, then I would have to\ntake a different sample space:{H,T,E }.\nHere is an example that puts this choice question into a rather stark light: Given that the human heart can beat\nanywhere from0 to 300 beats per minute, the sample space for the possible measurements of pulse rate is the set", "metadata": {"page": 16}}, {"page_content": "S = {0,1,..., 300}. Do you think that the probability function that assigns equal values to these integers will give\nreasonable predictions for the distribution of the measured pulse rates of you and your classmates?\n2.6 Exercises:\n1. Suppose an experiment has three possible outcomes, labeled 1, 2, and 3. Suppose in addition, that you do the\nexperiment three successive times.\n(a) Give the sample space for the possible outcomes of the three experiments.\n2.5. That\u2019s all there is to probability 13", "metadata": {"page": 16}}, {"page_content": "(b) Write down the subset of your sample space that correspond to the event that outcome 1 occurs in the\nsecond experiment.\n(c) Write down the subset of your sample space that corresponds to the event that outcome 1 appears in at\nleast one experiment.\n(d) Write down the subset of your sample space that corresponds to the event that outcome 1 appears at least\ntwice.\n(e) Under the assumption that each element in your sample space has equal probability, give the probabilities", "metadata": {"page": 17}}, {"page_content": "for the events that are described in parts (b), (c) and (d) above.\n2. Measure your pulse rate. Write down the symbol + if the rate is greater than 70 beats per minute, but write\ndown \u2212if the rate is less than or equal to70 beats per minute. Repeat this four times and so generate an ordered\nset of 4 elements, each a plus or a minus symbol.\n(a) Write down the sample space for the set of possible 4 element sets that can arise in this manner.", "metadata": {"page": 17}}, {"page_content": "(b) Under the assumption that all elements of this set are equally likely, write down the probability for the\nevent that precisely three of the symbols that appear in a given element are identical.\n3. Let Sdenote the set {1,2,..., 10}.\n(a) Write down three different probability functions on S by giving the probabilities that they assign to the\nelements of S.\n(b) Write down a function on S whose values can not be those of a probability function, and explain why such\nis the case.", "metadata": {"page": 17}}, {"page_content": "is the case.\n4. Four apples are set in a row. Each apple either has a worm or not.\n(a) Write down the sample space for the various possibilities for the apples to have or not have worms.\n(b) Let Adenote the event that the apples are worm free and let B denote the event that there is at least two\nworms amongst the four. What is A\u222aBand Ac \u2229B?\n5. A number is chosen at random from 1 to 1000. Let Adenote the event that the number is divisible by 3 and B", "metadata": {"page": 17}}, {"page_content": "the event that it is divisible by 5. What is A\u2229B?\n6. Some have conjectured that changing to a vegetarian diet can help lower cholesterol levels, and in turn lead to\nlower levels of heart disease. Twenty-four mostly hypertensive patients were put on vegetarian diets to see if\nsuch a diet has an effect on cholesterol levels. Blood serum cholesterol levels were measured just before they\nstarted their diets, and 3 months into the diet1.", "metadata": {"page": 17}}, {"page_content": "started their diets, and 3 months into the diet1.\n(a) Before doing any calculations, do you think Table 2.1 shows any evidence of an effect of a vegetarian diet\non cholesterol levels? Why or why not?\nThe sign test is a simple test of whether or not there is a real difference between two sets of numbers. In\nthis case, the \ufb01rst set consists of the24 pre-diet measurements, and the second set consists of the 24 after diet", "metadata": {"page": 17}}, {"page_content": "measurements. Here is how this test works in the case at hand: Associate + to a given measurement if the\ncholesterol level increased, and associate \u2212if the cholesterol decreases. The result is a set of 24 symbols, each\neither + or \u2212. For example, in this case, there are the number of + is 3 and the number of \u2212is 21. One then\nask whether such an outcome is likely given that the diet has no effect. If the outcome is unlikely, then there is", "metadata": {"page": 17}}, {"page_content": "reason to suspect that the diet makes a difference. Of course, this sort of thinking is predicated on our agreeing\non the meaning of the term \u201clikely\u201d, and on our belief that there are no as yet unknown reasons why the outcome\nappeared as it did. To elaborate on the second point, one can imagine that the cholesterol change is due not so\nmuch to the vegetarian nature of the diet, but to some factor in the diet that changed simultaneously with the", "metadata": {"page": 17}}, {"page_content": "change to a vegetarian diet. Indeed, vegetarian diets can be quite bland, and so it may be the case that people use\nmore salt or pepper when eating vegetarian food. Could the cause be due to the change in condiment level? Or\nperhaps people are hungrier sooner after such a diet, so they treat themselves to an ice cream cone a few hours\nafter dinner. Perhaps the change in cholesterol is due not to the diet, but to the daily ice cream intake.", "metadata": {"page": 17}}, {"page_content": "1Rosner, Bernard. Fundamentals of Biostatistics. 4th Ed. Duxbury Press, 1995.\n14 Chapter 2. Basic notions from probability theory", "metadata": {"page": 17}}, {"page_content": "Subject Before Diet After Diet Difference\n1 195 146 \u221249\n2 145 155 10\n3 205 178 \u221227\n4 159 146 \u221213\n5 244 208 \u221236\n6 166 147 \u221219\n7 250 202 \u221248\n8 236 215 \u221221\n9 192 184 \u22128\n10 224 208 \u221216\n11 238 206 \u221232\n12 197 169 \u221228\n13 169 182 13\n14 158 127 \u221231\n15 151 149 \u22122\n16 197 178 \u221219\n17 180 161 \u221219\n18 222 187 \u221235\n19 168 176 8\n20 168 145 \u221223\n21 167 154 \u221213\n22 161 153 \u22128\n23 178 137 \u221241\n24 137 125 \u221212\nTable 2.1: Cholesterol levels before and three months after starting a vegetarian diet", "metadata": {"page": 18}}, {"page_content": "Table 2.1: Cholesterol levels before and three months after starting a vegetarian diet\n(b) To make some sense of the notion of \u201clikely\u201d, we need to consider a probability function on the set of\npossible lists where each list has 24 symbols with each symbol either + or \u2212. What is the sample space\nfor this set?\n(c) Assuming that each subject had a 0.50 probability of an increase in cholesterol, what probability does the", "metadata": {"page": 18}}, {"page_content": "resulting probability function assign to any given element in your sample space?\n(d) Given the probability function you found in part (c), what is the probability of having no + appear in the\n24?\n(e) With this same probability function, what is the probability of only one + appear?\nAn upcoming chapter explains how to compute the probability of any number of + appearing. Another chapter\nintroduces a commonly agreed upon de\ufb01nition for \u201clikely\u201d.\n2.6. Exercises 15", "metadata": {"page": 18}}, {"page_content": "CHAPTER\nTHREE\nConditional probability\nThe notion of conditional probability provides a very practical tool for computing probabilities of events. Here is\ncontext where this notion \ufb01rst appears: You have a sample space, S, with a probability function, P. Suppose that\nAand B are subsets of S and that you have knowledge that the event represented by B has already occurred. Your\ninterest is in the probability of the event Agiven this knowledge about the event B. This conditional probability is", "metadata": {"page": 19}}, {"page_content": "denoted by P (A|B); and it is often different from P(A).\nHere is an example: Write down + if you measure your pulse rate to be greater than 70 beats per minute; but write\ndown \u2212if you measure it to be less than or equal to 70 beats per minute. Make three measurements of your pulse\nrate and so write down three symbols. The set of possible outcomes for the three measurements consists of the eight\nelement set\nS = {+ + +,+ + \u2212,+ \u2212+,+ \u2212\u2212, \u2212+ +,\u2212+ \u2212,\u2212\u2212+,\u2212\u2212\u2212}. (3.1)", "metadata": {"page": 19}}, {"page_content": "element set\nS = {+ + +,+ + \u2212,+ \u2212+,+ \u2212\u2212, \u2212+ +,\u2212+ \u2212,\u2212\u2212+,\u2212\u2212\u2212}. (3.1)\nLet A denote the event that all three symbols are +, and let B denote the event that the \ufb01rst symbol is +. Then\nP (A|B) is the probability that all symbols are + given that the \ufb01rst one is also +. If each of the eight elements has\nthe same probability, 1\n8 , then it should be the case that P (A|B) = 1\n4 since there are four elements in B but only one", "metadata": {"page": 19}}, {"page_content": "8 , then it should be the case that P (A|B) = 1\n4 since there are four elements in B but only one\nof these, (+ + +), is also in A. This is, in fact, the case given the formal de\ufb01nition that follows. Note that in this\nexample, P (A|B) \u0338= P(A) since P(A) = 1\n8 .\nHere is another hypothetical example: Suppose that you are a pediatrician and you get a phone call from a distraught\nparent about a child that is having trouble breathing. One question that you ask yourself is: What is the probability", "metadata": {"page": 19}}, {"page_content": "that the child is having an allergic reaction? Let\u2019s denote by Athe event that this is, indeed, the correct diagnosis. Of\ncourse, it may be that the child has the \ufb02u, or a cold, or any number of diseases that make breathing dif\ufb01cult. Anyway,\nin the course of the conversation, the parent remarks that the child has also developed a rash on its torso. Let us useB\nto denote the probability that the child has a rash. I expect that the probability the child is suffering from an allergic", "metadata": {"page": 19}}, {"page_content": "reaction is much greater given that there is a rash. This is to say that P(A|B) > P(A) in this case. Or, consider an\nalternative scenario, one where the parent does not remark on a rash, but remarks on a fever instead. In this case, I\nwould expect that the probability of the child suffering an allergic reaction is rather small since the symptoms point\nmore towards a cold or \ufb02u. This is to say that I now expect P(A|B) to be less than P(A).\n3.1 The de\ufb01nition of conditional probability", "metadata": {"page": 19}}, {"page_content": "3.1 The de\ufb01nition of conditional probability\nAs noted above, this is the probability that an event inAoccurs given that you already know that an event inBoccurs.\nThe rule for computing this new probability is\nP (A|B) \u2261P(A\u2229B)/P(B). (3.2)\nYou can check that this obeys all of the rules for being a probability. In English, this says:\nThe probability of an outcome occurring from A given that the outcome is known to be in B is the", "metadata": {"page": 19}}, {"page_content": "The probability of an outcome occurring from A given that the outcome is known to be in B is the\nprobability of the outcome being in both Aand Bdivided by the probability of the outcome being in Bin\nthe \ufb01rst place.\n16", "metadata": {"page": 19}}, {"page_content": "Another way to view this notion is as follows: Since we are told that B has happened, one might expect that the\nprobability that Aoccurs is the fraction of B\u2019s probability that is accounted for by the elements that are in bothAand\nB. This is just what (3.2) asserts. Indeed, P(A \u2229B) is the probability of the occurrence of an element that is in both\nAand B, so the ratio P(A\u2229B)/P(B) is the fraction of B\u2019s probability that comes from the elements that are both in\nAand B.", "metadata": {"page": 20}}, {"page_content": "Aand B.\nFor a simple example, consider the case where we roll a die with each face having the same probability of appearing.\nTake B to be the event that an even number appears. Thus, B = {2,4,6}. I now ask: What is the probability that 2\nappears given that an even number has appeared? Without the extra information, the probability that2 appears is 1\n6 . If\nI am told in advance that an even number has appeared, then I would say that the probability that 2 appears is 1\n3 . Note\nthat 1\n3 = 1\n6 /1", "metadata": {"page": 20}}, {"page_content": "3 . Note\nthat 1\n3 = 1\n6 /1\n2 ; and this is just what is said in (3.2) in the case that A= {2}and B = {2,4,6}.\nTo continue with this example, I can also ask for the probability that 1 or 3 appears given that an even number has\nappeared. Set A= {1,3}in this case. Without the extra information, we have P(A) = 1\n3 . However, as neither 1 nor 3\nis an even number, A\u2229B = \u2205. This is to say that Aand Bdo not share elements. Granted this obvious fact, I would", "metadata": {"page": 20}}, {"page_content": "say that P (A|B) = 0. This result is consistent with (3.2) because the numerator that appears on the right hand side\nof (3.2) is zero in this case.\nI might also consider the case where A = {1,2,4}. Here I have P(A) = 1\n2 . What should P (A|B) be? Well, Ahas\ntwo elements from B, and since B has three elements, each element in B has an equal probability of appearing, I\nwould expect P (A|B) = 2\n3 . To see what (3.2) predicts, note that A\u2229B = {2,4}and this has probability 1\n3 . Thus,", "metadata": {"page": 20}}, {"page_content": "3 . To see what (3.2) predicts, note that A\u2229B = {2,4}and this has probability 1\n3 . Thus,\n(3.2)\u2019s prediction for P(A|B) is 1\n3 /1\n2 = 2\n3 also.\nWhat follows is another example with one die, but this die is rather pathological. In particular, imagine a six-sided\ndie, so the sample space is again the set {1,2,3,4,5,6}. Now consider the case where P(1) = 1\n21 , P(2) = 2\n21 ,\nP(3) = 3\n21 , etc. In short, P(n) = n\n21 when n \u2208{1,2,3,4,5,6}. Let B again denote the set {2,4,6}and suppose", "metadata": {"page": 20}}, {"page_content": "21 when n \u2208{1,2,3,4,5,6}. Let B again denote the set {2,4,6}and suppose\nthat A = {1,2,4}. What is P (A|B) in this case? Well, Ahas two of the elements in B. Now B\u2019s probability is\n2\n21 + 4\n21 + 6\n21 = 12\n21 and the elements from Aaccount for 6\n21 , so I would expect that the probability ofAgiven Bis the\nfraction of B\u2019s probability that is accounted for by the elements of A, thus 6\n21 /12\n21 = 1\n2 . This is just what is asserted\nby (3.2).", "metadata": {"page": 20}}, {"page_content": "21 /12\n21 = 1\n2 . This is just what is asserted\nby (3.2).\nWhat follows describe various common applications of conditional probabilities.\n3.2 Independent events\nAn event Ais said to be independent of an event Bin the case that\nP (A|B) = P(A). (3.3)\nIn English: Events A and B are independent when the probability of A given B is the same as that of A with no\nknowledge about B. Thus, whether the outcome is in Bor not has no bearing on whether it is in A.", "metadata": {"page": 20}}, {"page_content": "knowledge about B. Thus, whether the outcome is in Bor not has no bearing on whether it is in A.\nHere is an equivalent de\ufb01nition: Events Aand B are deemed to be independent whenever P(A \u2229B) = P(A)P(B).\nThis is equivalent because P (A|B) = P(A\u2229B)/P(B). Note that the equality between P(A \u2229B) and P(A)P(B)\nimplies that P (B|A) = P(B). Thus, independence is symmetric. Here is the English version of this equivalent", "metadata": {"page": 20}}, {"page_content": "de\ufb01nition: Events Aand Bare independent in the case that the probability of an event being both inAand in Bis the\nproduct of the respective probabilities that it is in Aand that it is in B.\nFor an example, take the sample space S as in (3.1), take Ato be the event that + appears in the third position, and\ntake Bto be the event that + appears in the \ufb01rst position. Suppose that the chosen probability function assigns equal\nweight, 1", "metadata": {"page": 20}}, {"page_content": "weight, 1\n8 , to each element in S. Are Aand B mutually independent? Well, P(A) = 1\n2 is as is P(B). Meanwhile,\nP(A\u2229B) = 1\n4 which is P(A)P(B). Thus, they are indeed independent. By the same token, if Ais the event that \u2212\nappears in the third position, with Bas before, then Aand Bare again mutually independent.\nFor a second example, consider Ato be the event that a plus appears in the \ufb01rst position, and take B to be the event", "metadata": {"page": 20}}, {"page_content": "that a minus appears in the \ufb01rst position. In this case, no elements are in both A and B; thus A\u2229B = \u2205and so\nP(A\u2229B) = 0. On the other hand, P(A)P(B) = 1\n4 . As a consequence, these two events are not independent. (Are you\nsurprised?)\n3.2. Independent events 17", "metadata": {"page": 20}}, {"page_content": "Here is food for thought: Suppose that the sample space in (3.1) represents the set of outcomes that are obtained by\nmeasuring your pulse three times and recording + or \u2212for the respective cases when your pulse rate is over 70 or no\ngreater than 70. Do you expect that the event with the \ufb01rst measurement giving + is independent from that where the\nthird measurement gives +? I would expect that the third measurement is more likely to exceed 70 then not if the \ufb01rst", "metadata": {"page": 21}}, {"page_content": "measurement exceeds 70. If such is the case, then the assignment of equal probabilities to all elements of S does not\nprovide an accurate model for real pulse measurements.\nWhat follows is another example. Take the case of rolling the pathological die. Thus, S = {1,2,3,4,5,6}and if\nnis one of these numbers, then P(n) = n\n21 . Consider the case where B = {2,4,6}and Ais {1,2,4}. Are these\nindependent events? Now, P(A) = 7\n21 , P(B) = 12\n21 and, as we saw P (A|B) = 1\n2 . Since 1\n2 \u0338= 4", "metadata": {"page": 21}}, {"page_content": "independent events? Now, P(A) = 7\n21 , P(B) = 12\n21 and, as we saw P (A|B) = 1\n2 . Since 1\n2 \u0338= 4\n21 = P(A)P(B), these\nevents are not independent.\nSo far, you have seen pairs of events that are not independent. To see an example of a pair of independent events,\nconsider \ufb02ipping a fair coin twice. The sample space in this case consists of four elements,S = {HH,HT,TH,TT }.\nI give Sthe probability function that assigns 1\n4 to each event in S. Let Adenote the event that the \ufb01rst \ufb02ip gives heads", "metadata": {"page": 21}}, {"page_content": "4 to each event in S. Let Adenote the event that the \ufb01rst \ufb02ip gives heads\nand let B denote the event that the second \ufb02ip gives heads. Thus, A = {HH,HT }and B = {HH,TH }. Do\nyou expect these events to be independent? In this case, P(A) = 1\n2 since it has two elements, both with one-fourth\nprobability. For the same reason, P(B ) = 1\n2 . Since A\u2229B = {HH}, so P(A \u2229B) = 1\n4 . Therefore P(A \u2229B) =\nP(A) \u00b7P(B) as required for Aand Bto be independent.", "metadata": {"page": 21}}, {"page_content": "4 . Therefore P(A \u2229B) =\nP(A) \u00b7P(B) as required for Aand Bto be independent.\nTo reiterate, events Aand Bare independent when knowledge that Bhas occurred offers no hints towards whether A\nhas also occurred. Here is another example: Roll two standard die. The sample space in this case, S, has 36 elements,\nthese of the form (a,b) where a= 1,2,..., or 6 and b= 1,2,..., or 6. I give Sthe probability function that assigns", "metadata": {"page": 21}}, {"page_content": "probability to each element. Let B denote the set of pairs that sum to 7. Thus, (a,b) \u2208B when a+ b = 7. Let\nAdenote the event that ais 1. Is Aindependent from B? To determine this, note that there are 6 pairs in B, these\n(1,6), (2,5), (3,4), (4,3), (5,2) and (6,1). Thus, P(B) = 1\n6 . Meanwhile, there are six pairs in A; these are (1,1),\n(1,2), (1,3), (1,4), (1,5) and (1,6). Thus P(A) = 1\n6 . Finally, A\u2229B = (1,6) so P(A\u2229B) = 1\n36 . Since this last is", "metadata": {"page": 21}}, {"page_content": "6 . Finally, A\u2229B = (1,6) so P(A\u2229B) = 1\n36 . Since this last is\nP(A) \u00b7P(B), it is indeed the case that Aand Bare independent.\nHere is a question to ponder: IfCdenotes the set of pairs(a,b) with a= 1 or 2, are Cand Bindependent? The answer\nis again yes since C has twelve elements so probability 1\n3 . Meanwhile, C\u2229B = {(1,6),(2,5)}so P(C\u2229B) = 2\n36 .\nSince this last ratio is equal to P(C) \u00b7P(B), it is indeed the case that Cand Bare independent.\n3.3 Bayes theorem", "metadata": {"page": 21}}, {"page_content": "3.3 Bayes theorem\nBayes theorem concerns the situation where you have knowledge of the outcome and want to use it to infer something\nabout the cause. This is a typical situation in the sciences. For example, you observe a distribution of traits in the\nhuman population today and want to use this information to say something about the distribution of these traits in an\nancestral population. In this case, the \u2018outcome\u2019 is the observed distribution of traits in today\u2019s population, and the", "metadata": {"page": 21}}, {"page_content": "\u2018cause\u2019 is the distribution of traits in the ancestral population.\nTo pose things in a mathematical framework, suppose thatBis a given subset of S; and suppose that we know how to\ncompute the conditional probabilities given B. Thus, P (A|B) for various events A. Suppose that we don\u2019t know that\nBactually occurred, but we do see a particular version ofA. The question on the table is that of using this A\u2019s version", "metadata": {"page": 21}}, {"page_content": "of P (A|B) to compute P (B|A). Said prosaically: What does knowledge of the outcomes say about the probable\n\u2018cause\u2019?\nTo infer causes from outcomes, use the equalities\nP (A|B) = P(A\u2229B)/P(B) and P (B|A) = P(A\u2229B)/P(A)\nto write\nP (B|A) = P (A|B) P(B)\nP(A) . (3.4)\nThis is the simplest form of \u2018Bayes theorem\u2019. It tells us the probability of cause B given that outcome A has been\nobserved. What follows is a sample application.\n18 Chapter 3. Conditional probability", "metadata": {"page": 21}}, {"page_content": "Suppose that 1% of the population have a particular mutation in a certain protein, that20% of people with this mutation\nhave trouble digesting lactose, and that 5% of the population have trouble digesting lactose. If a classmate has trouble\ndigesting lactose, what is the probability that the classmate has the particular mutation? Here, the outcome is a student\nwith trouble digesting lactose, and we want to infer the probability that the cause is the mutant protein. To this end, let", "metadata": {"page": 22}}, {"page_content": "Adenote the event that a person has trouble digesting lactose and letBdenote the event that a person has the mutated\nprotein. We are told that P(A) = 0.05, that P(B) = 0.01 and that P (A|B) = 0.2. According to (3.4), the probability\nof Bgiven that Aoccurs, P (B|A), is equal to 0.04. This is the probability that the classmate with lactose intolerance\nhas the given mutation.\nA similar application of Bayes\u2019 theorem is used when DNA evidence is invoked in a criminal investigation. Suppose,", "metadata": {"page": 22}}, {"page_content": "for example that 10% of the population has a certain sequence of paired DNA bases on a particular stretch of DNA,\nthat 5% of the population have committed a felony, and that 20% of the felonies are committed by people with the\ngiven sequence of bases. An individual is charged with the crime. As it turns out, this individual does exhibit this\nspecial sequence of base pairs. What is the probability that the individual is guilty? To analyze this, letA denote", "metadata": {"page": 22}}, {"page_content": "the event that an individual has the given sequence of base pairs and let B denote the event that an individual has\ncommitted a felony. We are told that P(A) = 0.1, that P(B) = 0.05 and that P (A|B) = 0.2. An application of (3.4)\n\ufb01nds that the probability of interest, P (B|A), is equal to 0.1.\nApplications of Bayes\u2019 theorem in medicine are very common. Suppose you, a pediatrician, see a child that is running", "metadata": {"page": 22}}, {"page_content": "a mild temperature but no other symptoms. You know that95% of children with either a viral or bacterial infection\nrun a temperature, that 5% of children run similar temperatures whether sick or not, and that 1% of children at any\ngiven time have some sort of infection. What is the probability that the child has an infection given that the child has a\nmild temperature? To answer this, we set Ato denote the event that the child has a temperature, Bto denote the event", "metadata": {"page": 22}}, {"page_content": "that the child has an infection. We are told that P(A) = 0.05, P(B) = 0.001 and P (A|B) = 0.95. We are asking for\nP (B|A). Bayes\u2019 theorem \ufb01nds this to equal 0.19.\nWhat follows is an example with my pathological die. Thus S = {1,2,3,4,5,6}and P(n) = n\n21 when n is one\nof the integers from S. Let B denote the set {2,4,6}and let A denote the set {2,4}. We saw previously that\nP (A|B) = 5\n6 . What is P (B|A)? This is the fraction of A\u2019s probability that is accounted for by the elements that", "metadata": {"page": 22}}, {"page_content": "are both in A and in B. Since every element in A is also in B (this is to say that A \u2282B), all A\u2019s probability is\naccounted for by elements of B. Thus, I should conclude that P (B|A) = 1. Meanwhile, Bayes\u2019 theorem \ufb01nds that\nP (B|A) = P (A|B) \u00b7P(B)/P(A) = 5\n6 \u00b712\n21\n/10\n21 = 1 as required.\n3.4 Decomposing a subset to compute probabilities\nIt is often the case (as we will see in subsequent chapters) that it is easier to compute conditional probabilities \ufb01rst;", "metadata": {"page": 22}}, {"page_content": "then use them to compute unconditional probabilities of interest. In fact, this is a very common application of the\nnotion of conditional probabilities.\nWhat follows is a simple example. I have two coins in my pocket, one is fair so that the probability of heads is 1\n2 . The\nother is not fair as the probability of heads is only 1\n4 . I choose one of these coins while blind folded and then \ufb02ip it.", "metadata": {"page": 22}}, {"page_content": "4 . I choose one of these coins while blind folded and then \ufb02ip it.\nWhat is the probability of heads appearing? My logic here is based on the following reasoning: The probability of\nheads is equal to the sum of\n(the probability of heads given that the fair coin) \u00d7(the probability that the coin is fair)\nplus\n(the probability of heads given the unfair coin) \u00d7(the probability that the coin is unfair) .\nThus, I would say that the probability of heads in this case is (1\n2 \u00d71\n2 ) + (1\n4 \u00d71\n2 ) = 3", "metadata": {"page": 22}}, {"page_content": "Thus, I would say that the probability of heads in this case is (1\n2 \u00d71\n2 ) + (1\n4 \u00d71\n2 ) = 3\n8 . I trust that you notice here the\nappearance of conditional probabilities.\nIn general, the use of conditional probabilities to compute unconditional probabilities arises in the following situa-\ntion: Suppose that a sample space, S, is given. In the coin example above, I took S to be set with four elements", "metadata": {"page": 22}}, {"page_content": "{(F,H),(F,T ),(U,H),(U,T)}, where the symbols have the following meaning: First, (F,H) denotes the case\nwhere the coin is fair and heads appears and (F,T ) that where the coin is fair and tails appears. Meanwhile, (U,H)\n3.4. Decomposing a subset to compute probabilities 19", "metadata": {"page": 22}}, {"page_content": "denotes the case where the coin is unfair and heads appears, and (U,T) that where the coin is unfair and tails appears.\nKeep this example in mind as we consider the abstract situation where Sis just some given sample space of interest.\nNow look for a convenient decomposition of S into subsets that do not share elements. Let me use N to denote the\nnumber of such sets. Thus, I write\nS = B1 \u222aB2 \u222a\u00b7\u00b7\u00b7\u222a BN,", "metadata": {"page": 23}}, {"page_content": "number of such sets. Thus, I write\nS = B1 \u222aB2 \u222a\u00b7\u00b7\u00b7\u222a BN,\nwhere B1, B2, ..., BN are subsets of Swith Bk \u2229Bk\u2032 = \u2205when k\u0338= k\u2032. What I mean by \u2018convenient\u2019 is clari\ufb01ed in\nwhat follows. However, one criteria is that the probabilities of the various Bk should be easy to \ufb01nd.\nIn the case of my two coins, I used two sets for such a decomposition; I took B1 to be the event that the coin is fair", "metadata": {"page": 23}}, {"page_content": "and B2 to be the event that the coin is unfair. In this case, I told you that both P(B1) and P(B2) are equal to 1\n2 .\nTo return now to the abstract situation, suppose that Ais an event in S and I want the probability of A. If, for each\n1 \u2264k\u2264N, I know the conditional probability of Agiven Bk, then I can write\nP(A) = P (A|B1) \u00b7P(B1) + P (A|B2) \u00b7P(B2) + \u00b7\u00b7\u00b7 + P (A|BN) \u00b7P(BN). (3.5)\nIn words, this says the following:", "metadata": {"page": 23}}, {"page_content": "In words, this says the following:\nThe probability of an outcome from A is the probability that an outcome from A occurs given that B1\noccurs times the probability of B1, plus the probability that an outcome from A occurs given that B2\noccurs times the probability of B2, plus . . . etc.\nThe formula in (3.5) is useful only to the extent that the conditional probabilities P(A|B1), P (A|B2), . . . , P(A|BN)", "metadata": {"page": 23}}, {"page_content": "and the probabilities of each Bk are easy to compute. This is what I mean by the use of the descriptive \u2018convenient\u2019\nwhen I say that one should look for a \u2018convenient\u2019 decomposition ofSas B1 \u222aB2 \u222a\u00b7\u00b7\u00b7\u222a BN.\nBy the way, do you recognize (3.5) as a linear equation? You might if you denote P(A) by y, each P(Bj) by xj and\nP (A|Bj) by aj so that this reads\ny= a1x1 + a2x2 + \u00b7\u00b7\u00b7 + aNxN .\nThus, linear systems arise!", "metadata": {"page": 23}}, {"page_content": "P (A|Bj) by aj so that this reads\ny= a1x1 + a2x2 + \u00b7\u00b7\u00b7 + aNxN .\nThus, linear systems arise!\nHere is why (3.5) holds: Remember that P(A|B) = P(A\u2229B)/P(B). Thus, P (A|B)\u00b7P(B) = P(A\u2229B). Therefore,\nthe right hand side of (3.5) states that\nP(A) = P(A\u2229B1) + P(A\u2229B2) + \u00b7\u00b7\u00b7 + P(A\u2229BN).\nThis now says that the probability of Ais obtained by summing the probabilities of the parts of Athat appear in each", "metadata": {"page": 23}}, {"page_content": "Bn. That such is the case follows when theBn\u2019s don\u2019t share elements but account for all of the elements ofS. Indeed,\nif, say B1 shared an element with B2, then that element would be overcounted on the right-hand side of the preceding\nequation. On the other hand, if the Bn\u2019s don\u2019t account for all elements inS, then there might be some element in A\nthat is not accounted for by the sum on the right-hand side.", "metadata": {"page": 23}}, {"page_content": "that is not accounted for by the sum on the right-hand side.\nWhat follows is a sample application of (3.5): Suppose that I have six coins where the probability of heads on the \ufb01rst\nis 1\n2 , that on the second is 1\n4 , that on the third is 1\n8 , that on the fourth is 1\n16 , that on the \ufb01fth is 1\n32 , and that on the last\nis 1\n64 . Suppose that I label these coins by 1, 2, . . . ,6 so that the probability of heads on the m\u2019th coin is2\u2212m. Now I", "metadata": {"page": 23}}, {"page_content": "also have my pathological die, the one where the probability of the face with number n \u2208{1,2,..., 6}is n\n21 . I roll\nmy pathological die and the number that shows face up tells me what coin to \ufb02ip. All of this understood, what is the\nprobability of the \ufb02ipped coin showing heads?\nTo answer this question, I \ufb01rst note that the relevant sample space has12 elements, these of the form (n,H) or (n,T),", "metadata": {"page": 23}}, {"page_content": "where ncan be 1, 2, 3, 4, 5, or 6. This is to say that (n,H) is the event that the n\u2019th coin is chosen and heads appears,\nwhile (n,T) is the event that the n\u2019th coin is chosen and tails appears. The set A in this case is the event that H\nappears. To use (3.5), I \ufb01rst decompose my sample space into 6 subsets, {Bn}1\u2264n\u22646, where Bn is the event that the\nn\u2019th coin is chosen. This is a \u2018convenient\u2019 decomposition because I know P(Bn), this n\n21 . I also know P (A|Bn), this", "metadata": {"page": 23}}, {"page_content": "21 . I also know P (A|Bn), this\n2\u2212n. Granted this, then (3.5) \ufb01nds that the probability of heads is equal to\n1\n2 \u00b7 1\n21 + 1\n4 \u00b7 2\n21 + 1\n8 \u00b7 3\n21 + 1\n16 \u00b7 4\n21 + 1\n32 \u00b7 5\n21 + 1\n64 \u00b7 6\n21 = 5\n56.\n20 Chapter 3. Conditional probability", "metadata": {"page": 23}}, {"page_content": "The next example comes from genetics. To start, suppose that I am breeding peas `a la Gregor Mendel, and I have a\nvariety that gives wrinkled peas and another that gives smooth peas. I know further that the gene for wrinkled peas is\nrecessive. This is to say that a parent plant can have one of three \u2018genotypes\u2019, these denoted byww, wsor ss. A plant\nwith wrinkled peas must be of type ww. A plant with wsor sshas smooth peas. Now, I breed a two pea plants, not", "metadata": {"page": 24}}, {"page_content": "knowing whether they give wrinkled peas or smooth peas, and ask for the probability that the offspring is wrinkled.\nThe assumption here is that the offspring inherits one gene from each parent. Thus, if a parent has genotype ws, then\nthe offspring can inherit eitherwor sfrom that parent. If the parent has genotypess, then the offspring inheritssfrom\nthe parent.\nMy sample space for this problem consists of the possible triples that label the genotypes of Parent #1, Parent #2 and", "metadata": {"page": 24}}, {"page_content": "the offspring. For example, (ws,ws,ss ) is an example of a triple that appears in the sample space. This labels the\ncase where both parents have genotype wsand the offspring has genotype ss. Another triple from the sample space\nis (ws,ws,ws ). Here is a third: (ss,ws,ss ). A fourth is (ss,ws,sw ). And so on. Note that only triples of the form\n(\u2212\u2212,\u2212\u2212,ww) give rise to a plant with wrinkled peas.", "metadata": {"page": 24}}, {"page_content": "(\u2212\u2212,\u2212\u2212,ww) give rise to a plant with wrinkled peas.\nLet Adenote the event that the offspring has wrinkled peas. Let B1 denote the event that both parents are of type ww,\nlet B2 denote the event that the \ufb01rst parent is wwand the second is ws, let B3 denote the event that the \ufb01rst parent is\nwsand the second ww, let B4 denote the event that the both parents are ws, and let B5 denote the event that at least", "metadata": {"page": 24}}, {"page_content": "one parent is ss. Note that the collection {Bn}1\u2264n\u22645 are such that their union is all of S, and no two share the same\nelement. Also, I know what P(A|Bn) is for each nif I assume that a parent gives one of its two genes to the offspring,\nand that there is equal probability of giving one or the other. Thus, P (A|B1) = 1, P (A|B2) = P (A|B3) = 1\n2 ,\nP (A|B4) = 1\n4 and P (A|B5) = 0. As a consequence, (3.5) reads\nP(wrinkled offspring) = P(B1) + 1\n2\n(\nP(B2) + P(B3)\n)\n+ 1\n4 P(B4).", "metadata": {"page": 24}}, {"page_content": "P(wrinkled offspring) = P(B1) + 1\n2\n(\nP(B2) + P(B3)\n)\n+ 1\n4 P(B4).\nThus, it is enough for me to know the probabilities for the possible genotypes of the parents.\nThe point here is that the conditional probabilities {P (A|Bn)}1\u2264n\u22645 are easy to compute. Note also that their\ncomputation is based on theoretical considerations. This is to say that we made the hypothesis that \u2018a parent gives", "metadata": {"page": 24}}, {"page_content": "one of its two genes to the offspring, and that there is equal probability of giving one or the other.\u2019 The formula given\nabove for P(wrinkled offspring) should be viewed as a prediction to be con\ufb01rmed or not by experiments.\nWhat follows is another example from biology. Suppose that I am concerned with a stretch of DNA of length N,\nand want to know what the probability of not seeing the base guanine in this stretch. Let Adenote the event of that", "metadata": {"page": 24}}, {"page_content": "there is no guanine in a particular length N stretch of DNA. Let B1 denote the event that there is no guanine in the\nstretch of length N \u22121, and let B2 denote the event that guanine does appear in this length N \u22121 stretch. In this\ncase, P (A|B2) = 0 since A and B2 are disjoint. What about P (A|B1)? This is the probability that guanine is not\nin the Nth site if none has appeared in the previous N \u22121 sites. Of course, the probability of guanine appearing in", "metadata": {"page": 24}}, {"page_content": "the Nth site may or may not be affected by what is happening in the other sites. Let us make the hypothesis that each\nof the four bases has equal probability to appear in any given site. Under this hypothesis, the probability of seeing no\nguanine in theNth site is 3\n4 since there are four bases in all and only one of them, guanine, is excluded. Thus, under\nour hypothesis that each base has equal probability of appearing in any given site, we \ufb01nd that P (A|B1) = 3\n4 . This", "metadata": {"page": 24}}, {"page_content": "4 . This\nunderstood, it then follows from (3.5) that P(A) = 3\n4 P(B1).\nNow we can compute P(B 1) in an analogous fashion by considering the relative probability of no guanine in the\n(N\u22121)st site given that none appears in the previousN\u22122 sites. Under our hypothesis of equal probabilities for the\nbases, this gives P(B1) = 3\n4 times the probability of no guanine in the \ufb01rst N \u22122 sites. We can use this trick again to\ncompute the latter probability; it equals 3", "metadata": {"page": 24}}, {"page_content": "compute the latter probability; it equals 3\n4 times the probability of no guanine in the \ufb01rst N \u22123 sites. Continuing in\nthis vein \ufb01nds P(A) to be equal to the product of N copies of 3\n4 , thus (3\n4 )N.\nThis computation of P(A) = ( 3\n4 )N is now a theoretical prediction based on the hypothesis that the occurrence of any\ngiven base in any given DNA site has the same probability as that of any other base. You are challenged to think of an\nexperiment that will test this prediction.", "metadata": {"page": 24}}, {"page_content": "experiment that will test this prediction.\n3.4. Decomposing a subset to compute probabilities 21", "metadata": {"page": 24}}, {"page_content": "3.5 More linear algebra\nWhat follows is meant as a second illustration of how linear algebra appears when considering probabilities. Let A1,\nA2, A3 and A4 denote the events that a given site in DNA has baseA, G, C, and T. Let B1, B2, B3 and B4 denote the\nanalogous event for the adjacent site to the5\u2032end of the DNA. (The ends of a DNA molecule are denoted3\u2032and 5\u2032for\nreasons that have to do with a tradition of labeling carbon atoms on sugar molecules.) According to the rule in (3.5),\nwe must have", "metadata": {"page": 25}}, {"page_content": "we must have\nP(A1) = P (A1 |B1) \u00b7P(B1) + P (A1 |B2) \u00b7P(B2) + P (A1 |B3) \u00b7P(B3) + P (A1 |B4) \u00b7P(B4),\nP(A2) = P (A2 |B1) \u00b7P(B1) + P (A2 |B2) \u00b7P(B2) + P (A2 |B3) \u00b7P(B3) + P (A2 |B4) \u00b7P(B4),\nP(A3) = P (A3 |B1) \u00b7P(B1) + P (A3 |B2) \u00b7P(B2) + P (A3 |B3) \u00b7P(B3) + P (A3 |B4) \u00b7P(B4),\nP(A4) = P (A4 |B1) \u00b7P(B1) + P (A4 |B2) \u00b7P(B2) + P (A4 |B3) \u00b7P(B3) + P (A4 |B4) \u00b7P(B4).\nSo, we have a 4 \u00d74 matrix M whose entry in row iand column jis P (Ai|Bj). Now write each P(Ai) as yi and each", "metadata": {"page": 25}}, {"page_content": "P(Bj) as xj, and these last four equations can be summarized by the assertion that\nyi =\n\u2211\n1\u2264j\u22644\nMijxj for each i= 1, 2, 3, and 4.\nThis can be viewed as a system of four linear equations!\n3.6 An iterated form of Bayes\u2019 theorem\nSuppose that S = B1 \u222aB2 \u222a\u00b7\u00b7\u00b7\u222a BN is the union of N pairwise disjoint subsets. Suppose also that Ais a given\nsubset of S and we know that an outcome from A appears. Given this knowledge, what is the probability that the", "metadata": {"page": 25}}, {"page_content": "outcome was from some given Bk? Here is an example: Take S to be the set of all diseases and Adiseases where\nthe lungs \ufb01ll with \ufb02uid. Take B1 to be pneumonia, B2 to be ebola viral infection, B3 to be West Nile viral infection,\netc. An old man\u2019s lungs \ufb01ll with \ufb02uid. What is the probability that he has West Nile viral infection? We are therefore\nsearching for P (B3 |A).\nSuppose that we know the probability of the lung \ufb01lling with \ufb02uid with the disease that corresponds to Bk. This", "metadata": {"page": 25}}, {"page_content": "is to say that we know each P (A|Bk). Suppose we also know P(B k); the probability of catching the disease that\ncorresponds to Bk. How can we use this information to compute P (B3 |A)?\nThis is done using the following chain of equalities: I \ufb01rst write\nP (B3 |A) = P(B3 \u2229A)/P(A) = P (A|B3) \u00b7P(B3)/P(A)\nusing the original form of Bayes\u2019 theorem. To compute P(A), I use (3.5). Together, these two equalities imply the\ndesired one:\nP (B3 |A) = P (A|B3) P(B3)\nP (A|B1) \u00b7P(B1) + \u00b7\u00b7\u00b7 + P (A|BN) \u00b7P(BN).", "metadata": {"page": 25}}, {"page_content": "desired one:\nP (B3 |A) = P (A|B3) P(B3)\nP (A|B1) \u00b7P(B1) + \u00b7\u00b7\u00b7 + P (A|BN) \u00b7P(BN).\nOf course, I make such an equation for any given P (Bk|A), and this gives the most general form of Bayes theorem:\nP (Bk|A) = P (A|Bk) P(Bk)\nP (A|B1) \u00b7P(B1) + \u00b7\u00b7\u00b7 + P (A|BN) \u00b7P(BN). (3.6)\nThis equation provides the conditional probability of Bk given that Aoccurs from the probabilities of the various Bk\nand the conditional probabilities that Aoccurs given that any one of these Bk occur.", "metadata": {"page": 25}}, {"page_content": "and the conditional probabilities that Aoccurs given that any one of these Bk occur.\nTo see how this works in practice, return to the example I gave above where I have six coins, these labeled\n{1,2,3,4,5,6}; and where the mth coin has probability 2\u2212m of landing with the head up when \ufb02ipped. I also\nhave my pathological six-sided die, where the probability of the nth face appearing when rolled is n\n21 . As before, I", "metadata": {"page": 25}}, {"page_content": "21 . As before, I\n\ufb01rst roll the die and if the nth face appears, I \ufb02ip the coin with the label n. I don\u2019t tell you which coin was \ufb02ipped, but\nI do tell you that heads appeared. What is the probability that the coin #3 was \ufb02ipped?\n22 Chapter 3. Conditional probability", "metadata": {"page": 25}}, {"page_content": "I can use (3.6) to compute this probability. For this purpose, let A denote the event that heads appears. For each\nn\u2208{1,2,..., 6}, let Bn denote the event that I \ufb02ipped the coin labeled by n. Thus, my question asks for P (B3 |A).\nYou have all of the ingredients to compute P (B3 |A) via (3.6) since you are told that P(A|B n) = 2 \u2212n and\nP(Bn) = n\n21 . In fact, we have already computed the sum that makes up the denominator in (3.6), this being 5\n56 .", "metadata": {"page": 26}}, {"page_content": "56 .\nAs a consequence, the equality in (3.6) \ufb01nds P (B3 |A) equal to 1\n8 \u00b7 3\n21 /5\n56 = 1\n5 .\n3.7 Exercises:\n1. This exercise concerns the sample space Sthat is depicted in (3.1). If Srepresents the outcomes for three pulse\nrate measurements of a given individual, it is perhaps more realistic to take the following probability function:\nThe function P assigns probability 1\n3 to + + + and to \u2212\u2212\u2212 while assigning 1\n18 to each of the remaining\nelements.", "metadata": {"page": 26}}, {"page_content": "3 to + + + and to \u2212\u2212\u2212 while assigning 1\n18 to each of the remaining\nelements.\n(a) Is the event that + appears \ufb01rst independent of the event that + appears last?\n(b) Is the event that + appears second independent for the event that + appears last?\n(c) What is the conditional probability that + appears \ufb01rst given that \u2212appears second?\n2. Suppose that that the probability of a student lying in the in\ufb01rmary is 1%, and that the probability that a student", "metadata": {"page": 26}}, {"page_content": "has an exam on any given day is5%. Suppose as well that 6% of students with exams go to the in\ufb01rmary. What\nis the probability that a student in the in\ufb01rmary has an exam on a given day?\n3. Label the four bases that are used by DNA as {1,2,3,4}.\n(a) Granted this labeling, write down the sample space for the possible bases at two given sites on the molecule.\n(b) Invent a probability function for this sample space.", "metadata": {"page": 26}}, {"page_content": "(b) Invent a probability function for this sample space.\n(c) Let Aj for j = 1, 2, 3, 4 denote the event in this two-site sample space that the \ufb01rst site has the base j,\nand let Bj for j = 1,..., 4 denote the analogous event for the second site. Use the de\ufb01nition of condi-\ntional probability to explain why, for any probability function and for any k, P (A1 |Bk) + P (A2 |Bk) +\nP (A3 |Bk) + P (A4 |Bk) must equal 1.", "metadata": {"page": 26}}, {"page_content": "P (A3 |Bk) + P (A4 |Bk) must equal 1.\n(d) Is there a choice for a probability function on the sample space that makes P (A1 |B1) = P (B1 |A1) in\nthe case that A1 and B1 are not independent? If so, give an example. If not, explain why.\n4. This problem refers to the scenario that I described above where I have six coins, these labeled {1,2,3,4,5,6};\nand where the mth coin has probability 2\u2212m of landing with the head up when \ufb02ipped. I also have my patho-", "metadata": {"page": 26}}, {"page_content": "logical six-sided die, where the probability of thenth face appearing when rolled is n\n21 . As before, I \ufb01rst roll the\ndie and if the nth face appears, I \ufb02ip the coin with the label n. I don\u2019t tell you which coin was \ufb02ipped, but I do\ntell you that heads appeared.\n(a) For n= 1, 2, 4, 5, and 6, give the probability that the coin with the label n was \ufb02ipped.\n(b) For what n, if any, is the event that the nth face appears independent from the event that heads appears.", "metadata": {"page": 26}}, {"page_content": "5. Suppose that Aand B are subsets of a sample space with a probability function, P. Suppose in addition that\nP(A) = 4\n5 and P(B) = 3\n5 . Explain why P (B|A) is at least 1\n2 .\n6. For many types of cancer, early detection is the key to successful treatment. Prostate cancer is one of these. For\nearly detection, the National Cancer Institute suggests screening of patients using the Serum Prostate-Speci\ufb01c", "metadata": {"page": 26}}, {"page_content": "Antigen (PSA) Test. There is controversy due to the lack of evidence showing that early detection of prostate\ncancer and aggressive treatment of early cancers actually reduces mortality. Also, this treatment can often lead\nto complications of impotence and incontinence.\nHere is some terminology that you will meet if you go on to a career in medicine: The sensitivity of a test is the", "metadata": {"page": 26}}, {"page_content": "probability of a positive test when the patient has the disease, and the speci\ufb01city of a test is the probability of a\nnegative test when the patient does not have the disease. In the language of conditional probability,\n3.7. Exercises 23", "metadata": {"page": 26}}, {"page_content": "\u2022Sensitivity is the conditional probability of a positive test given that the disease is present.\n\u2022Speci\ufb01city is the conditional probability of a negative test given that the disease is not present.\nThe standard PSA test to detect early stage prostate cancer has Sensitivity= 0.71 and Speci\ufb01city = 0.91. Thus,\n0.71 is the conditional probability of a positive test given that a person does have prostate cancer. And, 0.91 is", "metadata": {"page": 27}}, {"page_content": "the conditional probability of a negative test given that a person does not have prostate cancer. Note for what\nfollows that roughly 0.7% of the male population is diagnosed with prostate cancer each year.\nGranted this data, here is the question that this problem will answer:\nIf a patient receives a positive test for prostate cancer, what is the probability he truly has cancer?\nTo answer this question, letAdenote the event that a person has a positive test, and letBdenote the event that a", "metadata": {"page": 27}}, {"page_content": "person has prostate cancer. This question is asking for the conditional probability of Bgiven A; thus P (B|A).\nThe data above gives P (A|B) = 0.71 and P(B) = 0.007 and also P (Ac|Bc) = 0.91 where Ac is the event\nthat a person has a negative test, and Bc is the event that a person does not have cancer. As the set of questions\nthat follow demonstrate, the information given is suf\ufb01cient to answer the question posed above.\n(a) Why is P (B|A) = (0.71) \u00d7(0.007)/P(A) \u22480.005/P(A)?", "metadata": {"page": 27}}, {"page_content": "(a) Why is P (B|A) = (0.71) \u00d7(0.007)/P(A) \u22480.005/P(A)?\nIf you answered this, then the task is to \ufb01nd P(A).\n(b) Why is P(A) = P(A\u2229B) + P(A\u2229Bc)?\n(c) Why is P(A \u2229Bc) = P(Bc) \u2212P(Ac \u2229Bc)?\n(d) Why is P(Bc) = 1 \u2212P(B)?\nIf you answered these last three questions, you have discovered that P(A) = P(A\u2229B)+1 \u2212P(B)\u2212P(Ac\u2229Bc),\nand thus, using what you just derived, P(A) = 0.993 +P(A\u2229B) \u2212P(Ac \u2229Bc).\n(e) Why is P(A \u2229B) = (0.71) \u00d7(0.007) \u22480.005?\n(f) Why is P(Bc \u2229Ac) = (0.91) \u00d7(0.993) \u22480.904?", "metadata": {"page": 27}}, {"page_content": "(e) Why is P(A \u2229B) = (0.71) \u00d7(0.007) \u22480.005?\n(f) Why is P(Bc \u2229Ac) = (0.91) \u00d7(0.993) \u22480.904?\nIf you answered (a)\u2013(f), then you have found that P(A) \u22480.094 and so the question in italics asked above is\n\u22480.054.\nFor more info, see: http://imsdd.meb.uni-bonn.de/cancernet/304727.html\n7. Many genetic traits are controlled by a single gene with two alleles, one dominant (A) and one recessive (a), that", "metadata": {"page": 27}}, {"page_content": "are passed on generation by generation. Albinism is one phenotype that can be described this way. A person is\nalbino if he/she has gotten two copies of the recessive alleles (aa); one from each of his/her parents.\nErin knows for certain that both her parents are carriers for the albino phenotype. That is, they both have theAa\ngenotype, one dominant allele and one recessive allele.\n(a) What is the sample space for the possible pairs of alleles that Erin could have inherited from her parents?", "metadata": {"page": 27}}, {"page_content": "Now assume each allele is equally likely to be passed from Erin\u2019s parents to Erin.\n(b) Explain how this information gives a probability function on the sample space that you found in (a).\n(c) Use the probability function from (b) to give the probability that Erin is albino.\n(d) Given that Erin is not albino, what is the probability that she has the albino allele?\nMendel\u2019s paper:http://www.mendelweb.org/Mendel.html\n24 Chapter 3. Conditional probability", "metadata": {"page": 27}}, {"page_content": "CHAPTER\nFOUR\nLinear transformations\nMy purpose here is to give some examples of linear transformations that arise when thinking about probability as\napplied to problems in biology. As you should recall, a linear transformation on Rn can be viewed as the effect of\nmultiplying vectors by a given matrix. If Ais the matrix and \u20d7 vis any given vector, the transformation has the form\n\u20d7 v\u2192A\u20d7 v, where A\u20d7 vis the vector with components\n(A\u20d7 v)j =\n\u2211\nk\nAjkvk. (4.1)\nThus,", "metadata": {"page": 28}}, {"page_content": "\u20d7 v\u2192A\u20d7 v, where A\u20d7 vis the vector with components\n(A\u20d7 v)j =\n\u2211\nk\nAjkvk. (4.1)\nThus,\n(A\u20d7 v)1 = A11v1 + A12v2 + \u00b7\u00b7\u00b7 + A1nvn\n(A\u20d7 v)2 = A21v1 + A22v2 + \u00b7\u00b7\u00b7 + A2nvn\n..\n. (4.2)\n(A\u20d7 v)n = An1v1 + An2v2 + \u00b7\u00b7\u00b7 + Annvn\n4.1 Protein molecules\nA protein molecule is made by attaching certain small molecules end to end. The small molecules are amino acids,\nand there are 20 that commonly appear. The amino acids that comprise a given protein can number in the thousands,", "metadata": {"page": 28}}, {"page_content": "but each is just one of these 20 varieties. The amino acids are numbered from one end. (The ends of the amino acids\nand thus the two ends of a protein are distinguished by their chemical composition.)\nIt is often the case that the protein found in one individual differs from that in another because the 10th or 127th or\n535th amino acid from the start of the chain differ in the two versions. Some such substitutions drastically affect the\nprotein function, others are benign.", "metadata": {"page": 28}}, {"page_content": "protein function, others are benign.\nSuppose that the127th amino acid along a protein molecule can vary amongst a certain subset of5 amino acids without\neffecting the protein\u2019s function. Now, imagine a population of individuals (mice, worms, \ufb01sh, bacteria, whatever) that\nis watched over successive generations. Let pi(t) denote the probability that amino acid i \u2208 {1,...,5}occurs as\nthe 127th amino acid in the tth generation of the given population. Even if a parent in generation thas amino acid", "metadata": {"page": 28}}, {"page_content": "numbered iin the 127th position, a mutation in the coding gene going from generationtto t+1 can change the coding\nso that the offspring has amino acidj \u0338= iin the 127th position. There is some probability Aji for this to happen. This\nnumber may depend on iand j. The reason is that the genetic code for an amino acid is3 letters long, and so changing\nfrom amino acid ito jhas the highest probability if only one letter separates their corresponding codes.\nIn any event, I can write\npi(t+ 1) =\n\u2211\nj", "metadata": {"page": 28}}, {"page_content": "In any event, I can write\npi(t+ 1) =\n\u2211\nj\nAijpj(t). (4.3)\nThis equation says that the probability of seeing amino acid iin the 127th position is obtained by using the formula\nin (3.5). In words: 25", "metadata": {"page": 28}}, {"page_content": "The probability of seeing amino acid iin generation t+ 1 is the conditional probability that the amino\nacid at position 127 is iin the offspring given that it is 1 in the parent times the probability that amino\nacid 1 appears in generation t, plus the conditional probability that the amino acid at position 127 is iin\nthe offspring given that it is 2 in the parent times the probability that amino acid 2 appears in generation\nt, plus . . . etc.", "metadata": {"page": 29}}, {"page_content": "t, plus . . . etc.\nThe point here is that the vector \u20d7 p(t)with entries pi(t) is changed via \u20d7 p(t)\u2192\u20d7 p(t+ 1) = A\u20d7 p(t)after each generation.\nHere, Ais the matrix whose ijentry is Aij.\n4.2 Protein folding\nThe long string of amino acids that comprise a protein does not appear in a cell as a straight string. Rather, the string\nis folded back on itself in a very complicated manner. The geometry of the folding helps determine the activity of the", "metadata": {"page": 29}}, {"page_content": "protein. A simple model has the folding occurring due to the fact that the angles of attachment of one amino acid to\nthe next can vary. If the protein has n amino acids, and each can bond to its neighbor at someDpossible angles, then\nthere are N = nD possible con\ufb01gurations for the protein to fold into. Label the possible con\ufb01gurations by the integers\nstarting at 1 and ending at N.", "metadata": {"page": 29}}, {"page_content": "starting at 1 and ending at N.\nAs it turns out, a protein con\ufb01guration is rarely static. Rather, the protein \ufb02uctuates from one con\ufb01guration to an-\nother over time. This \ufb02uctuation is due to a combination of effects, some classical (such as collisions between other\nmolecules) and some quantum mechanical.\nIn any event, ifiand jboth label con\ufb01gurations, there is some probability,Pij, of the protein being in con\ufb01gurationjat", "metadata": {"page": 29}}, {"page_content": "time tand con\ufb01guration iat time t+1. Those of you with some chemical background may recall that thermodynamical\nconsiderations put\nPij = e\u2212(E(i)\u2212E(j))/kT\nwhere E( \u00b7) is the free energy of the con\ufb01guration, k is Boltzman\u2019s constant and T is the temperature in degrees\nKelvin.\nIn any event, letvj(t) denote the probability that protein is in con\ufb01gurationiat time t. Then, vi(t+1) = \u2211\njPijvj(t).", "metadata": {"page": 29}}, {"page_content": "jPijvj(t).\nThus, the vector \u20d7 v(t+ 1) in RN is obtained from \u20d7 v(t) by the action of the linear transformation whose matrix has\ncomponents Pij.\n26 Chapter 4. Linear transformations", "metadata": {"page": 29}}, {"page_content": "CHAPTER\nFIVE\nHow matrix products arise\nWhat follows are some areas in biology and statistics where matrix products appear.\n5.1 Genomics\nSuppose that a given stretch of DNA coding for cellular product is very mutable, so that there are some number, N,\nof possible sequences that can appear in any given individual (this is called \u2018polymorphism\u2019) in the population. To\nelaborate, a strand of DNA is a molecule that appears as a string of small, standard molecules that are bound end to", "metadata": {"page": 30}}, {"page_content": "end. Each of these standard building blocks can be one of four, labeled C, G, A and T. The order in which they appear\nalong the strand determines any resulting cellular product that the given part of the DNA molecule might produce. For\nexample, AAGCTA may code for a different product than GCTTAA.\nAs it turns out, there are stretches of DNA where the code can be changed without damage to the individual. What", "metadata": {"page": 30}}, {"page_content": "with inheriting genes from both parents, random mutations over the generations can then result in a population where\nthe codes on the given stretch of DNA vary from individual to individual. In this situation, the gene is called \u2018poly-\nmorphic\u2019. Suppose that there are N different possible codes for a given stretch. For example, if one is looking at just\none particular site along a particular DNA strand, there could be at most N = 4 possibilities at that site, namely C, G,", "metadata": {"page": 30}}, {"page_content": "A or T. Looking at two sites gives N = 4 \u00d74 = 16 possibilities.\nI am going to assume in what follows that sites of interest along the DNA is inherited from parent to child only from\nthe mother or only from the father. Alternately, I will assume that I am dealing with a creature such as a bacteria that\nreproduces asexually. This assumption simpli\ufb01es the story that follows.\nLet us now label the possible sequences for the DNA site under discussion by integers starting from 1 and going to N.", "metadata": {"page": 30}}, {"page_content": "At any given generation t, let pj(t) denote the frequency of the appearance of the jth sequence in the population at\ngeneration t. These frequencies then change from one generation to the next in the following manner: The probability\nof any given sequence, say i, appearing in generation t+ 1 can be written as a sum:\npi(t+ 1) = P (i|1) p1(t) + P (i|2) p2(t) + \u00b7\u00b7\u00b7 + P (i|N) pN(t), (5.1)", "metadata": {"page": 30}}, {"page_content": "pi(t+ 1) = P (i|1) p1(t) + P (i|2) p2(t) + \u00b7\u00b7\u00b7 + P (i|N) pN(t), (5.1)\nwhere each P (i|j) can be viewed as the conditional probability that a parent with sequence j produces an offspring\nwith sequence i. This is to say that the probability of sequence iappearing in an individual in generationt+ 1is equal\nto the probability that sequence 1 appears in the parent times the probability of a mutation that changes sequence 1 to", "metadata": {"page": 30}}, {"page_content": "sequence i, plus the probability that sequence 2 appears in the parent times the probability of a mutation that changes\nsequence 2 to sequence i, and so on.\nWe can write the suite of N versions of (5.1) using our matrix notation by thinking of the numbers {pj(t)}1\u2264j\u2264N\nas de\ufb01ning a column vector, \u20d7 p(t), inRN, and likewise the numbers {pi(t+ 1)}1\u2264i\u2264N as de\ufb01ning a second column\nvector, \u20d7 p(t+ 1) in RN. If I introduce the N \u00d7N matrix Awhose entry in the jth column and ith row is P (i|j),", "metadata": {"page": 30}}, {"page_content": "then (5.1) says in very cryptic shorthand:\n\u20d7 p(t+ 1) = A\u20d7 p(t). (5.2)\nI can sample the population at time t = T = now, and thus determine \u20d7 p(T), or at least the proxy that takes pi(T) to\n27", "metadata": {"page": 30}}, {"page_content": "be the percent of people in the population today that have sequence i. One very interesting question is to determine\n\u20d7 p(T\u2032) at some point far in the past, thusT\u2032\u226aT. For example, if we \ufb01nd \u20d7 p(T\u2032) such that all pi(T\u2032) are zero but a very\nfew, this then indicates that the population at time T\u2032was extremely homogeneous, and thus presumably very small.\nTo determine \u20d7 p(T\u2032), we use (5.2) in an iterated form:\n\u20d7 p(T) = A\u20d7 p(T\u22121) = AA\u20d7 p(T\u22122)) = AAA\u20d7 p(T\u22123) = \u00b7\u00b7\u00b7 = A\u00b7\u00b7\u00b7A\u20d7 p(T\u2032), (5.3)", "metadata": {"page": 31}}, {"page_content": "\u20d7 p(T) = A\u20d7 p(T\u22121) = AA\u20d7 p(T\u22122)) = AAA\u20d7 p(T\u22123) = \u00b7\u00b7\u00b7 = A\u00b7\u00b7\u00b7A\u20d7 p(T\u2032), (5.3)\nwhere the \ufb01nal term has T \u2212T\u2032copies of Amultiplying one after the other.\nOn a similar vein, we can use (5.2) to predict the distribution of the sequences in the population at any time T\u2032> T\nby iterating it to read\n\u20d7 p(T\u2032) = A\u00b7\u00b7\u00b7A\u20d7 p(T). (5.4)\nHere, the multiplication is by T\u2032\u2212T successive copies of the matrix A.", "metadata": {"page": 31}}, {"page_content": "\u20d7 p(T\u2032) = A\u00b7\u00b7\u00b7A\u20d7 p(T). (5.4)\nHere, the multiplication is by T\u2032\u2212T successive copies of the matrix A.\nBy the way, here is a bit of notation: This sort of sequence \u20d7 p{(t)}t=0,1,... of vectors of probabilities is an example of\na Markov chain. In general, a Markov chain is a sequence of probabilities, {P(0),P(1),P(2),..., }where the Nth\nprobability P(N) depends only on the probabilities with numbers that are less than N.\n5.2 How bacteria \ufb01nd food", "metadata": {"page": 31}}, {"page_content": "5.2 How bacteria \ufb01nd food\nIf you put certain sorts of bacteria on one side of a petri dish and put some sugar some distance away on the other,\nthe bacteria will migrate towards the sugar. Apparently, the sugar diffuses in the petri dish, so that there is slight\nconcentration everywhere, with most of the concentration in the original spot. The bacteria are sensitive to the different", "metadata": {"page": 31}}, {"page_content": "levels at their front and rear ends and tend to move in the direction of the greater level. At the expense of borrowing\nfrom a multivariable calculus course, the bacteria sense the direction of thegradient of the sugar concentration. Even\nso, their movement from low to high concentration has a certain randomness to it; at any given step there is some\nprobability that they will move in the wrong direction.", "metadata": {"page": 31}}, {"page_content": "probability that they will move in the wrong direction.\nWhat follows is a simplistic model for this: Imagine bacteria moving in steps along the x-axis along the segment that\nstretches from x= 1 to x= N. Here, N is some large integer. Suppose the sugar is placed initially where x = N, and\nthe bacteria is placed initially at x = 1. Our model also supposes that the bacteria moves one unit per second, with", "metadata": {"page": 31}}, {"page_content": "probability q\u2208(0,1) of moving to the right at any given step, and probability1 \u2212qof moving to the left unless it is at\nthe end of the interval. If it is at the x= 1 end, it moves to the right with probability qand stays put with probability\n1 \u2212q. If it is at the x = N end, it stays put with probability q and moves to the left with probability 1 \u2212q. In our\nmodel, qis independent of position, and x\u2208{ 1,...,N }. Such would be roughly the case in the real petri dish where", "metadata": {"page": 31}}, {"page_content": "the sugar concentration gradient is relatively constant. We should take q >1\n2 in the case that the bacteria is attracted\nto the sugar.\nFor each time step t, and j \u2208{1,...,N }, let pj(t) denote the probability that the bacteria is at position j at time t.\nFor example, p1(0) = 1 and pj(0) = 0 for j >1. Our model then says that the probability of \ufb01nding the bacteria at\nposition j \u0338= 1 of N at time step t >0 is equal to the probability that it is at position j\u22121 at time t\u22121 times the", "metadata": {"page": 31}}, {"page_content": "probability that it moves one step to the right, plus the probability that it is at position j+ 1 at time t\u22121 and moves\none step to the left. Thus,\npj(t+ 1) = qpj\u22121(t) + (1\u2212q)pj+1(t) when 2 \u2264j \u2264N \u22121. (5.5)\nBy the same token,\np1(t+ 1) = (1 \u2212q)p1(t) + (1\u2212q)p2(t) and pN(t+ 1) = qpN\u22121(t) + qpN(t). (5.6)\nLet us introduce the vector \u20d7 p(t)in RN whose jth component is pj(t). Then (5.5) and (5.6) assert that \u20d7 p(t+ 1) is", "metadata": {"page": 31}}, {"page_content": "obtained from \u20d7 p(t)by the action of a linear transformation: \u20d7 p(t+ 1) = A\u20d7 p(t), whereAis the N \u00d7N matrix whose\nonly non-zero entries are:\nA11 = 1 \u2212q,Ajj\u22121 = q for 2 \u2264j \u2264N, A jj+1 = 1 \u2212q for 1 \u2264j \u2264N \u22121, and ANN = q. (5.7)\n28 Chapter 5. How matrix products arise", "metadata": {"page": 31}}, {"page_content": "For example, in the case that N = 4, the matrix Ais\nA=\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 \u2212q 1 \u2212q 0 0\nq 0 1 \u2212q 0\n0 q 0 1 \u2212q\n0 0 q q\n\uf8f9\n\uf8fa\uf8fa\uf8fb. (5.8)\nIn any case, we can iterate the equation \u20d7 p(t+ 1) = A\u20d7 p(t)to \ufb01nd that\n\u20d7 p(t) =A\u00b7\u00b7\u00b7A\u20d7 p(0) (5.9)\nwhere A\u00b7\u00b7\u00b7A signi\ufb01es t copies of A multiplied one after the other. By the way, a common shorthand for some n\ncopies of any given matrix, A, successively multiplying one after the other is An.\n5.3 Growth of nerves in a developing embryo", "metadata": {"page": 32}}, {"page_content": "5.3 Growth of nerves in a developing embryo\nHere is a very topical question in the study of development: Nerve cells connect muscles and organs to the brain.\nWhen an organism develops, how do its nerves know where to connect? A given nerve cell stretches out extremely\nlong and thin \u2018appendages\u2019 called axons. Any given axon may end abutting a muscle cell, or another nerve cell in a\nchain of nerves that ends in the brain. How do the axons \u2018know\u2019 where they are supposed to attach? A simple model", "metadata": {"page": 32}}, {"page_content": "proposes that the tip of the growing axon in an embryo is guided by chemical gradients in much the same fashion as\nthe bacteria in the previous discussion is guided to the sugar.\n5.4 Enzyme dynamics\nAn enzyme is a protein molecule that facilitates a chemical reaction that would take a long time to occur otherwise.\nMost biological reactions are facilitated by enzymes. One typical mode of operation is for the enzyme to facilitate a", "metadata": {"page": 32}}, {"page_content": "reaction between molecules of type\u03b1and molecules of type \u03b2 to produce a new molecule of type \u03b3. It can do this\nif it simultaneously attracts the \u03b1and \u03b2 molecules. In doing so, the enzyme holds the two sorts near each other for\nenough time that they can bind together to form the \u03b3molecule. If this \u03b3molecule does not have a strong attraction to\nthe enzyme, it breaks away and so frees the enzyme to attract another \u03b1and another \u03b2to make more \u03b3.", "metadata": {"page": 32}}, {"page_content": "the enzyme, it breaks away and so frees the enzyme to attract another \u03b1and another \u03b2to make more \u03b3.\nNow, it is often the case that the enzyme only works ef\ufb01ciently if it is folded in the right conformation. Folded in the\nwrong way, the parts of the molecule that attract either \u03b1or \u03b2molecules might \ufb01nd themselves covered by parts that\nare indifferent to \u03b1or \u03b2. Due to random collisions and other effects, any given molecule, and thus our enzyme, will", "metadata": {"page": 32}}, {"page_content": "change its fold con\ufb01guration. Suppose that there is some probability, q, that the enzyme is folded correctly to attract\n\u03b1and \u03b2 in any given unit of time, and thus probability (1 \u2212q) that it is not. Suppose in addition, that when folded\ncorrectly, the enzyme makes 2 units of \u03b3per unit of time. Meanwhile, suppose that the freely diffusing \u03b3falls apart or\nis degraded by other enzymes at a rate of 1 unit per unit time.", "metadata": {"page": 32}}, {"page_content": "is degraded by other enzymes at a rate of 1 unit per unit time.\nFor any integer j, let pj(t) denote the probability of \ufb01nding level jof \u03b3after some tunits of time. Then the scenario\njust outlined \ufb01nds\npj(t+ 1) = qpj\u22121(t) + (1\u2212q)pj+1(t) when j \u22651 and p0(t+ 1) = (1 \u2212q)(p1(t) + p0(t)) (5.10)\nas long as \u03b1and \u03b2are well supplied. This last equation is another version of the one that is depicted in (5.2).\n5.5 Exercises:", "metadata": {"page": 32}}, {"page_content": "5.5 Exercises:\nExercises 1\u20133 concern the example above where the position of the bacteria at any given timetis labeled by an integer,\nx(t), in the set {1,...,N }. Don\u2019t assume that we know where the bacteria is att= 0.\n5.3. Growth of nerves in a developing embryo 29", "metadata": {"page": 32}}, {"page_content": "1. Suppose T is a positive integer.\n(a) What is the sample space for the collection of positions, {x(0),x(1),...,x (T)}, of the bacteria at the\ntimes t= 0,1,...,T .\n(b) Fix some j \u2208{1,...,N }and t\u2208{0,1,...,T \u22121}. Let C denote the event that x(t+ 1) = j. For each\ni \u2208{1,...,N }, let Bk denote the event that x(t) = k. Explain why the equation in (5.5) has the form\nP(C) = P (C|B1) P(B1) + \u00b7\u00b7\u00b7 + P (C|BN) P(BN).\n2. Recall that events Band Care said to be independent when P(B\u2229C) = P(B)P(C).", "metadata": {"page": 33}}, {"page_content": "2. Recall that events Band Care said to be independent when P(B\u2229C) = P(B)P(C).\n(a) Explain why this de\ufb01nition is equivalent to the assertion that the conditional probability of B happening\ngiven Cis equal to the probability of Bhappening.\n(b) Suppose that k\u2208{2,...,N \u22121}and that t\u22652. Derive a relation between the respective probabilities that\nx(t) = k\u22121 and x(t) = k+ 1 so as to insure that the event that x(t+ 1) = kis independent of the event\nthat x(t) = k\u22121.", "metadata": {"page": 33}}, {"page_content": "that x(t) = k\u22121.\n3. Suppose now that q+ and q\u2212are non-negative numbers whose sum is less than 1. Change the bacteria model so\nthat if the bacteria is at positionk\u2208{2,...,N \u22121}at time t, then it moves to positionk+ 1with probability q+,\nto position k\u22121 with probability q\u2212and stays put with probability 1 \u2212q+ \u2212q\u2212. If the bacteria is at position 1\nat time t, then it stays put with probability 1 \u2212q+ and moves to position 2 with probability q+. If the bacteria is", "metadata": {"page": 33}}, {"page_content": "at position N at time t, then in stays put with probability 1 \u2212q\u2212and moves to position N \u22121 with probability\nq\u2212.\n(a) Write down the analog of (5.5) for this model.\n(b) Write down the analog of the matrix in (5.8) for the N = 4 case.\n(c) Write down the probability that the bacteria is at position N at t= N \u22121 given that the bacteria starts at\nposition 1 at time 0.\n4. Write down a version of (5.10) that would hold in the case that the enzyme when folded correctly produces", "metadata": {"page": 33}}, {"page_content": "some L = 1, or L >2 units of \u03b3 per unit time. To be more explicit, assume \ufb01rst that when the enzyme is\nfolded correctly, it only makes 1 unit of \u03b3 per unit time to see how (5.10) will change. Then see how (5.10)\nmust change if the enzyme makes 3 units per unit time. Finally, consider the case where it makes Lunits per\nunit time and write (5.10) in terms of this number L.\n30 Chapter 5. How matrix products arise", "metadata": {"page": 33}}, {"page_content": "CHAPTER\nSIX\nRandom variables\nIn favorable circumstances, the different outcomes of any given experiment have measurable properties that distinguish\nthem. Of course, if a given outcome has a certain probability, then this is also the case for any associated measurement.\nThe notion of a \u2018random variable\u2019 provides a mathematical framework for studying these induced probabilities on the\nmeasurements.", "metadata": {"page": 34}}, {"page_content": "measurements.\nHere is a simple example: Suppose that I have some large number, say100, coins, all identical and all with probability\n1\n2 of landing heads when \ufb02ipped. I am going to \ufb02ip them all, and put a dollar in your bank account for each head that\nappears. You can\u2019t see me \ufb02ipping the coin, but you can go to your bank tomorrow and measure the size of your bank\naccount. You might be interested in knowing the probability of your account increasing by any given amount. The", "metadata": {"page": 34}}, {"page_content": "amount in your account is a random variable.\nTo explore this example a bit, note that the con\ufb01guration space of possible outcomes from \ufb02ipping 100 coins consists\nof sequences that are 100 letters long, each letter being either H or T. There are 2100 \u22481.2 \u00d71030 elements in\nthe con\ufb01guration space! If s is such a 100 letter sequence, let f(s) denote the number of heads that appear in the\nsequence s. Thus, f(s) can be any integer from 0 through 100. The assignment s\u2192f(s) is an example of a random", "metadata": {"page": 34}}, {"page_content": "variable. It is a function of sorts on the con\ufb01guration space. Of interest to you are the probabilities for the appearances\nof the various possible values of this functionf. This is to say that your concern is the probability function on the 101\nelement set {0,..., 100}that gives the probability of your bank account increasing by any given amount.\nHere is another example that is slightly less contrived: You are in charge of stocking a lake with trout. You put some", "metadata": {"page": 34}}, {"page_content": "large number, sayN, of trout in a lake. Due to predation, the odds are 50\u201350 that any given trout will be eaten after\none year. Meanwhile, the trout do not breed for their \ufb01rst year, so you are interested in the number of trout that survive\nto the second year. This number can be anywhere from0 to N. What is the probability of \ufb01nding a given number in\nthis range? Note that in the case that N = 100, this question is essentially identical to the question just posed about\nyour bank account.", "metadata": {"page": 34}}, {"page_content": "your bank account.\nThe model for this is as follows: There is a sample space, S, whose elements consist of sequences of N letters, where\neach letter is either a D(for dead) or L(for live). Thus, S has 2N elements. I assign to each element in S a number,\nthis the number of L\u2019s in the sequence. This assignment of a number to each element in S is a function on S, and of\ninterest to me are the probabilities for the possible values of this function. Note that these probabilities are not for the", "metadata": {"page": 34}}, {"page_content": "elements of S; rather they are for the elements in a different sample space, the set of integers from 0 through N.\n6.1 The de\ufb01nition of a random variable\nA random variable is no more nor less than a function on the sample space. In this regard, such a function assigns a\nnumber to each element in the sample space. One can view the function as giving the results of measurements of some\nproperty of the elements of the sample space.", "metadata": {"page": 34}}, {"page_content": "property of the elements of the sample space.\nSometimes, the notion is extended to consider a function from the sample space to another set. For example, suppose\nthat S is the set of possible 3 letter long sequences that are made from the 4 bases, guanine, cytosine, adenine and\ntyrosine, that appear in DNA molecules. Each such 3 letter sequence either codes for one of the 20 amino acids or\n31", "metadata": {"page": 34}}, {"page_content": "is the \u2018stop\u2019 signal. This is the genetic code. The code is thus a function from a set with 64 elements to one with 21\nelements\nMost often, random variables take real number values. For example, let Sdenote the 20 possible amino acids that can\noccupy the 127th position from the end of a certain enzyme (a type of protein molecule) that helps the cell metabolize\nthe sugar glucose. Now, let f denote the function on S that measures the rate of glucose metabolism in growing", "metadata": {"page": 35}}, {"page_content": "bacteria with the given enzyme at the given site. In this case, f associates to each element in a 20 element set a real\nnumber.\n6.2 Probability for a random variable\nSuppose that S is our sample space and P is a probability function on S. If f is a random variable and r a possible\nvalue for f, then the probability that f takes value ris by de\ufb01nition, the probability of the subset ofSwhere f is equal\nto r; thus P(Event that f = r). This is number is given by\nP(f = r) =\n\u2211\ns\u2208S:f(s)=r\nP(s). (6.1)", "metadata": {"page": 35}}, {"page_content": "to r; thus P(Event that f = r). This is number is given by\nP(f = r) =\n\u2211\ns\u2208S:f(s)=r\nP(s). (6.1)\nIn words:\nThe probability that f = ris the sum of the probabilities of those elements in Swhere f is equal to r.\nFor an example of what happens in (6.1), consider the situation that I described at the outset where I \ufb02ip 100 coins\nand pay you one dollar for each head that appears. As noted, the sample space Sis the 2100 element set whose typical", "metadata": {"page": 35}}, {"page_content": "element is a sequence, s, of 100 letters, each either H or T. The random variable, f, assigns to any given s \u2208S the\nnumber of heads that appear in the sequence s.\nTo explore (6.1) in this case, let us agree that the probability of any given element in sis 2\u2212100. This is based on my\ntelling you that each of the 100 coins is fair. It also assumes that the appearance of H or T on any one coin has no", "metadata": {"page": 35}}, {"page_content": "bearing on whether H or T appear on any other. (I can say this formally as follows: The event that H appears on any\ngiven coin is independent from the event that H appears on any other coin.) Thus, no matter what sis, the value P(s)\nthat appears in (6.1) is equal to 2\u2212100. This understood, (6.1) asserts that the probability that f is equal to any given\ninteger r \u2208{0,..., 100}is obtained by multiplying 2\u2212100 times the number of elements in sthat are sequences with\nprecisely rheads.", "metadata": {"page": 35}}, {"page_content": "precisely rheads.\nFor example, P(f = 0) = 2\u2212100 because there is just one element in swith no heads at all, this the element TT \u00b7\u00b7\u00b7T.\nThus, it is a good bet that you will get at least one dollar. On the other hand, P(f = 100) is also 2\u2212100 since only\nHH \u00b7\u00b7\u00b7H has 100 heads. So, it is a good bet that I will lose less than 100 dollars. Consider next the probability for\nf to equal 1. There are 100 sequences from S with 1 head. These being HTT \u00b7\u00b7\u00b7T, THT \u00b7\u00b7\u00b7T, . . . ,T\u00b7\u00b7\u00b7THT ,", "metadata": {"page": 35}}, {"page_content": "T\u00b7\u00b7\u00b7TTH . Thus, P(f = 1) is 1002\u2212100. This is still pretty small, on the order of 10\u221228. How about the probability\nfor 2 dollars? In this case, there are 1\n2 100 \u00b799 elements in S with two heads. If you buy this count, then P(f = 2) is\n50 \u00b799 \u00b72\u2212100. We shall learn in a subsequent chapter that P(f = r) is (100 \u00d799 \u00d7\u00b7\u00b7\u00b7\u00d7 (100 \u2212r))/(1 \u00d72 \u00d7\u00b7\u00b7\u00b7\u00d7\nr) \u00d72\u2212100.\nFor a second example, takeSto be the set of 20 possible amino acids at the 127th position from the end of the glucose", "metadata": {"page": 35}}, {"page_content": "metabolizing enzyme. Let f now denote the function fromSto the 10 element set that is obtained by measuring to the\nnearest 10% the fraction of glucose used in one hour by the growing bacteria. Number the elements ofSfrom 1 to 20,\nand suppose that P assigns the kth amino acid probability 1\n10 if k \u22645, probability 1\n20 if 6 \u2264k \u226410 and probability\n1\n40 if k >10. Meanwhile, suppose that f(k) = 1 \u2212 k\n10 if k \u226410 and f(k) = 0 if k \u226510. This understood, it then\nfollows using (6.1) that P(f = n", "metadata": {"page": 35}}, {"page_content": "10 if k \u226410 and f(k) = 0 if k \u226510. This understood, it then\nfollows using (6.1) that P(f = n\n10 ) is equal to\n0 for n= 10, 1\n10 for 5 \u2264n\u22649, 1\n20 for 1 \u2264n\u22644, and 3\n10 for n= 0. (6.2)\nBy the way, equation (6.1) can be viewed (at least in a formal sense) as a matrix equation in the following way:\nIntroduce a matrix by writing Ars = 1 if f(s) = rand Ars = 0 otherwise. Then, P(f = r) = \u2211\ns\u2208SArsP(s) is a\n32 Chapter 6. Random variables", "metadata": {"page": 35}}, {"page_content": "matrix equation. Of course, this is rather silly unless the set Sand the possible values for f are both \ufb01nite. Indeed, if\nS is \ufb01nite, say with nelements, number them from 1 to n. Even if f has real number values, one typically makes its\nrange \ufb01nite by rounding off at some decimal place anyway. This understood, there exists some number,N, of possible\nvalues for f. Label the latter by the integers between 1 and N. Using these numberings of S and the values of f, the", "metadata": {"page": 36}}, {"page_content": "matrix Acan be thought of as a matrix with ncolumns and N rows.\n6.3 A probability function on the possible values of f\nReturn now to the abstract situation whereSis a sample space andf is a function on S. As it turns out, the assignment\nr \u2192P(f = r) of a non-negative number to each of the possible values for f de\ufb01nes a probability function on the set\nof all possible values for f. Let us call this new sample space Sf, and the new probability function P f(r). Thus, if", "metadata": {"page": 36}}, {"page_content": "r\u2208Sf, then Pf(r) is given by the sum on the right hand side of (6.1).\nTo verify that it is a probability function, observe that it is never negative by the nature of its de\ufb01nition. Also, summing\nthe values of Pf over all elements inSf gives 1. Indeed, using (6.1), the latter sum can be seen as the sum of the values\nof P over all elements of S.\nThe example in (6.2) illustrates this idea of associating a new sample space and probability function to a random", "metadata": {"page": 36}}, {"page_content": "variable. In the example from (6.2), the new sample space is the11 element set of fractions of the form n\n10 where\nn\u2208{0,..., 10}. The function Pf is that given in (6.2). You can verify on your own that\u2211\n0\u2264n\u226410 P(f = n) = 1.\nWhat follows is a less abstract example. There is a gambling game known as \u2018craps\u2019, that is played with two standard,\nsix-sided dice. The dice are rolled, and the two numbers that appear are summed. When played in a casino, the one", "metadata": {"page": 36}}, {"page_content": "who roles the dice must pay a certain amount to play. The casino pays the player an amount that depends on the sum\nof the two numbers that appear. Thus, of interest are the probabilities for the various possible sums of two numbers,\neach chosen at random from the set{1,2,..., 6}.\nTo analyze the probabilities for the various sums in the language of random variables, note \ufb01rst that the possible", "metadata": {"page": 36}}, {"page_content": "outcomes for rolling two fair dice is a set with 36 elements, these pairs of the form(a,b) with aand bfrom the set\n{1,2,..., 6}. Let S denote this set of pairs of integers. If the diced are fair and if the roll of one is independent of\nthat of the other, then I model this situation with the probability function on Sthat assigns probability 1\n36 to each pair\nfrom S. The sum of the two elements in a pair de\ufb01nes a function on S, this denoted in what follows by f. Thus,", "metadata": {"page": 36}}, {"page_content": "f(a,b) = a+ b. By de\ufb01nition, this sum function is a random variable.\nOf interest to casinos and to those who play craps are the probabilities for the various values of f. In this regard, note\nthat f can be any integer from 2 through 12. To use (6.1) for computing the probability that f = r, note that this case\nis such that each P(s) that appears on the right hand side of (6.1) is equal to 1\n36 . This understood, it then follows that\nP(f = r) is obtained by multiplying 1", "metadata": {"page": 36}}, {"page_content": "36 . This understood, it then follows that\nP(f = r) is obtained by multiplying 1\n36 times the number of pairs in S whose components sum to r. Here is what\nresults:\nP(f = 2) = P(f = 12) = 1\n36\nP(f = 3) = P(f = 11) = 2\n36\nP(f = 4) = P(f = 10) = 3\n36\nP(f = 5) = P(f = 9) = 4\n36\nP(f = 6) = P(f = 8) = 5\n36\nP(f = 7) = 6\n36 .\nThese values de\ufb01ne the probability function, Pf, on the set Sf = {2,..., 12}.\n6.4 Mean and standard distribution for a random variable", "metadata": {"page": 36}}, {"page_content": "6.4 Mean and standard distribution for a random variable\nStatisticians are partial to using a one or two numbers to summarize what might be a complicated story. The mean\nand standard deviation of a random variable are very commonly employed for this purpose. To some extent, the mean\n6.3. A probability function on the possible values of f 33", "metadata": {"page": 36}}, {"page_content": "of a random variable is the best guess for its value. However, the mean speaks nothing of the expected variation. The\nmean and standard deviation together give both an idea as to the expected value of the variable, and also some idea of\nthe spread of the values of the variable about the expected value.\nWhat follows are the formal de\ufb01nitions. To this end, suppose that f is a random variable on a sample space S, in", "metadata": {"page": 37}}, {"page_content": "this case just a function that assigns a number to each element in S. The mean of f is the \u2018average\u2019 of these assigned\nnumbers, but with the notion of average de\ufb01ned here using the probability function. The mean is typically denoted as\n\u00b5; here is its formula:\n\u00b5=\n\u2211\ns\u2208S\nf(s)P(s) (6.3)\nA related notion is that of the standard deviation of the random variable f. This is a measure of the extent to which f", "metadata": {"page": 37}}, {"page_content": "differs from its mean. The standard deviation is often denoted by the Greek letter \u03c3and it is de\ufb01ned so that its square\nis the mean of (f \u2212\u00b5). To be explicit,\n\u03c32 =\n\u2211\ns\u2208S\n(f(s) \u2212\u00b5)2P(s). (6.4)\nThus, the standard deviation is larger when f differs from its mean to a greater extent. The standard deviation is zero\nonly in the case that f is the constant function. By the way, the right hand side of (6.4) can be written using (6.3) as\n\u03c32 =\n\u2211\ns\u2208S\nf(s)2P(s) \u2212\u00b52. (6.5)", "metadata": {"page": 37}}, {"page_content": "\u03c32 =\n\u2211\ns\u2208S\nf(s)2P(s) \u2212\u00b52. (6.5)\nSee if you can derive one version from the other.\nIt is a common mistake to write the mean as 1\nN\n\u2211\ns\u2208Sf(s) where N here denotes the number of elements in S. This\nlast expression is correct if each element element in S has the same probability. Thus, when P(s) = 1\nN for all s. In\ngeneral, the formula in (6.1) must be used. To see why, consider the following situation: I have a six-sided die, but one", "metadata": {"page": 37}}, {"page_content": "where the probability of the number 1 appearing is 1 and the probability of any other number appearing is zero. Thus,\nmy sample space is the set {1,2,..., 6}, P(1) = 1, and P(s) = 0 for s >1. Let\u2019s take the random variable, f, to be\nthe function f(s) = s. In this case, (6.2) gives \u00b5= 1 as one might expect. Meanwhile, 1\nN\n\u2211\ns\u2208Sf(s) is equal to 3.5.\nTo see the utility of the standard deviation, consider the case where I have slightly less pathological six sided die, this\nwhere P(1) =1", "metadata": {"page": 37}}, {"page_content": "where P(1) =1\n2 , P(6) = 1\n2 and P(s) = 0 if sis neither 1 nor 6. As before, I take f so that f(s) = s. In this case\nthe expression on the right side of (6.3) is equal to 3.5, this identical to what I would get for a fair die, one where the\nprobability is for any given number appearing. On the other hand, the standard deviation for the case of a fair die is\u221a\n35\n12 \u22481.7, while the standard deviation for the case of this pathological die is 2.5.", "metadata": {"page": 37}}, {"page_content": "35\n12 \u22481.7, while the standard deviation for the case of this pathological die is 2.5.\nTo see how these de\ufb01nitions play out in another example, consider the case of the game of craps. Recall that the\nsample space in this example is the set, S, of pairs of the form (a,b) where aand bcan be any integer from 1 through\n6. In this case, each element in Shas the same probability, this being 1\n36 . The random variable here is the function, f,", "metadata": {"page": 37}}, {"page_content": "36 . The random variable here is the function, f,\nthat assigns a+ bto the pair (a,b). The mean of f in this case is 7. The standard deviation is\n\u221a\n35\n6 .\nFor a third example, consider again the case that is relevant to (6.2). The sum for the mean in this case is\n1 \u00b70 + 9\n10 \u00b7 1\n10 + 8\n10 \u00b7 1\n10 + 7\n10 \u00b7 1\n10 + 6\n10 \u00b7 1\n10 + 5\n10 \u00b7 1\n10 + 4\n10 \u00b7 1\n20 + 3\n10 \u00b7 1\n20 + 2\n10 \u00b7 1\n20 + 1\n10 \u00b7 1\n20 + 0 \u00b7 3\n10\nwhich equals 2\n5 . Thus, \u00b5= 2", "metadata": {"page": 37}}, {"page_content": "10 + 4\n10 \u00b7 1\n20 + 3\n10 \u00b7 1\n20 + 2\n10 \u00b7 1\n20 + 1\n10 \u00b7 1\n20 + 0 \u00b7 3\n10\nwhich equals 2\n5 . Thus, \u00b5= 2\n5 . The standard deviation in this example is the number whose square is the sum\n9\n25 \u00b70 + 25\n100 \u00b7 1\n10 + 16\n100 \u00b7 1\n10 + 9\n100 \u00b7 1\n10 + 4\n100 \u00b7 1\n10 + 1\n100 \u00b7 1\n10 + 0\u00b7 1\n20 + 1\n100 \u00b7 1\n20 + 4\n100 \u00b7 1\n20 + 9\n100 \u00b7 1\n20 + 4\n25 \u00b7 3\n10\nwhich equals 11\n100 . Thus, \u03c3=\n\u221a\n11\n10 \u22480.33.\n6.5 Random variables as proxies", "metadata": {"page": 37}}, {"page_content": "20 + 4\n25 \u00b7 3\n10\nwhich equals 11\n100 . Thus, \u03c3=\n\u221a\n11\n10 \u22480.33.\n6.5 Random variables as proxies\nOf ultimate interest are the probabilities for the points in S, but it is often only possible to directly measure the\nprobabilities for some random variable, a given function on S. In this case, a good theoretical understanding of the\n34 Chapter 6. Random variables", "metadata": {"page": 37}}, {"page_content": "measurements and the frequencies of occurrences of the various measured values can be combined so as to make an\neducated guess for the probabilities of the elements of S.\nTo give an example, consider again the casino game called \u2018craps\u2019. Recall that the game is played as follows: The\nplayer pays the casino a certain amount of money and then rolls two six sided dice. The two numbers showing are", "metadata": {"page": 38}}, {"page_content": "summed and the player\u2019s winnings are determined by this sum. This sum is a random variable on the sample space\nwhich in this case is the36 element set of pairs of the form(a,b) where aand bcan be any two numbers from1 through\n6. Suppose now that you are watching people play this game from a far off seat in the casino. You are too far away to\nsee the pair(a,b) that arise on any given play, but you hear the casino croupier call out the sum. You make note in a", "metadata": {"page": 38}}, {"page_content": "ledger book of these values as you watch. So, in effect, each play of the game is an experiment and your noting the\ndeclared sum after each play is a measurement. Your interest here is in the probabilities for the various elements in\nthe sample space, but you aren\u2019t privy to the frequency with which they occur. You are only privy to the frequency of\noccurrence of the values of the random variable. This is to say that after a day\u2019s worth of watching the game, the data", "metadata": {"page": 38}}, {"page_content": "in your ledger book provides the relative frequencies of the various possible values of the suma+b. To elaborate, your\ndata consists of a sequence of 11 numbers, {Y2,Y3,...,Y 11,Y12}, where Yk is the fraction of the time that the sum\nof the two numbers on the dice is k. Thus, each Yk is some number from 0 through 1, and Y1 + Y2 + \u00b7\u00b7\u00b7 + Y12 = 1.\nFor example, if the dice are fair and the face showing on one has no bearing on the face that shows on the other, then", "metadata": {"page": 38}}, {"page_content": "I would expect that(Y1,...,Y 12) should be nearly ( 1\n36 , 2\n36 , 3\n36 , 4\n36 , 5\n36 , 6\n36 , 5\n36 , 4\n36 , 3\n36 , 2\n36 , 1\n36 ). If the casino is using\nloaded dice, then the values of the various Yk may differ from those just listed.\nThe question is now: Can these relative frequencies be used to make an educated guess for the probabilities of the\nvarious pairs(a,b) that appear? Put starkly, can you use the data {Y1,...,Y 12}to decide if the dice being used are\nfair?", "metadata": {"page": 38}}, {"page_content": "fair?\nHere is how this is typically accomplished in the generic setting where you are doing some experiment to measure a\nrandom variable, f, on some sample space S. The experiment is done many times and the frequencies of occurrence\nof the possible values forf are then taken as a reasonable approximation for the probability function Pf. This is to say\nthat we declare P(f = r) to equal the measured frequency that the value rwas obtained for f. These experimentally", "metadata": {"page": 38}}, {"page_content": "determined values are substituted for P(f = r) in (6.1) to turn the latter equation where the right-hand side is known\nand the various s\u2208S versions of P(s) on the left-hand side are desired. In short, the goal is to view (6.1) as a linear\nequation where the left-hand side is given by our experimental approximation for P f and the various s \u2208S versions\nof P(s) are the unknowns to be found. Granted that we can solve this equation, then the resulting solutions will give", "metadata": {"page": 38}}, {"page_content": "us a reasonable guess as to the true probability function on S.\nReturn to our hypothetical data from watching the game of craps. The version of (6.1) where {Y2,...,Y 12}are used\non the left-hand side for the values of Pf yields 11 equations for 36 unknowns:\nY2 = X(1,1)\nY3 = X(1,2) + X(2,1)\nY4 = X(1,3) + X(2,2) + X(3,1)\nY5 = X(1,4) + X(2,3) + X(3,2) + X(4,1)\nY6 = X(1,5) + X(2,4) + X(3,3) + X(4,2) + X(5,1)\nY7 = X(1,6) + X(2,5) + X(3,4) + X(4,3) + X(5,2) + X(6,1)", "metadata": {"page": 38}}, {"page_content": "Y7 = X(1,6) + X(2,5) + X(3,4) + X(4,3) + X(5,2) + X(6,1)\nY8 = X(2,6) + X(3,5) + X(4,4) + X(5,3) + X(6,2)\nY9 = X(3,6) + X(4,5) + X(5,4) + X(6,3)\nY10 = X(4,6) + X(5,5) + X(6,4)\nY11 = X(5,6) + X(6,5)\nY12 = X(6,6)\nHere X(a,b) is our unknown proxy for the probability function, P, on the sample space of pairs of the form (a,b)\nwhere aand bare integers from 1 through 6.\nTo see how this works in the general case, suppose that S is a sample space and f a random variable on S. Suppose", "metadata": {"page": 38}}, {"page_content": "that there is some \ufb01nite set of possible values for f, these labeled as {r1,...,r N}. When k \u2208{1,...,N }, let yk\ndenote the frequency that rk appears as the value for f in our experiments. Label the elements in S as {s1,...,s n}.\n6.5. Random variables as proxies 35", "metadata": {"page": 38}}, {"page_content": "Now introduce the symbol xj to denote the unknown but desired P(sj). Thus, the subscript jon xcan be any integer\nin the set {1,...,n }. The goal is then to solve for the collection {xj}1\u2264j\u2264n by writing (6.1) as the linear equation\ny1 = a11x1 + \u00b7\u00b7\u00b7 + a1nxn\n..\n. (6.6)\nyN = aN1x1 + \u00b7\u00b7\u00b7 + aNnxn,\nwhere akj = 1 if f(sj) = rk and akj = 0 otherwise. Note that this whole strategy is predicated on two things: First,", "metadata": {"page": 39}}, {"page_content": "that the sample space is known. Second, that there is enough of a theoretical understanding to predict apriori the values\nfor the measurement f on each element in S.\nTo see something of this in action, consider \ufb01rst the example from the game of craps. In this case there are 11\nequations for 36 unknowns, so there are in\ufb01nitely many possible choices for the collection {X(a,b)}for any given", "metadata": {"page": 39}}, {"page_content": "set {Y2,...,Y 12}. Even so, the equations determine X(1,1) and X(6,6). If we expect that X(a,b) = X(b,a), then\nthere are 21 unknowns and the equations now determine X(1,2) and X(5,6) also.\nConsider next the example from (6.2). For the sake of argument, suppose that the measured frequency of P(f = n\n10 )\nare exactly those given in (6.2). Label the possible values of f using r1 = 0, r2 = 1\n10 ,\u00b7\u00b7\u00b7 ,r11 = 1. This done, the\nrelevant version of (6.6)) is the following linear equation:\n3", "metadata": {"page": 39}}, {"page_content": "10 ,\u00b7\u00b7\u00b7 ,r11 = 1. This done, the\nrelevant version of (6.6)) is the following linear equation:\n3\n10 = x10 + \u00b7\u00b7\u00b7 + x20\n1\n20 = x9\n1\n20 = x8\n1\n20 = x7\n1\n20 = x6\n1\n10 = x5\n1\n10 = x4\n1\n10 = x3\n1\n10 = x2\n1\n10 = x1\n0 = 0\nAs you can see, this determines xj = P(sj) for j \u22649, but there are in\ufb01nitely many ways to assign the remaining\nprobabilities.\nWe shall see in subsequent lessons how to choose a \u2018best possible\u2019 solution of a linear equation that has more unknowns\nthan knowns.\n6.6 A biology example", "metadata": {"page": 39}}, {"page_content": "than knowns.\n6.6 A biology example\nHere is some background: It is typical that a given gene along a DNA molecule is read by a cell for its information\nonly if certain nearby sites along the DNA are bound to certain speci\ufb01c protein molecules. These nearby sites are\ncalled \u2018promoter\u2019 regions (there are also \u2018repressor\u2019 regions) and the proteins that are involved are called \u2018promoters\u2019.", "metadata": {"page": 39}}, {"page_content": "The promoter regions are not genes per se, rather they are regions of the DNA molecule that attract proteins. The\neffect of these promoter regions is to allow for switching behavior: The gene is \u2018turned on\u2019 when the corresponding\npromoter is present and the gene is \u2018turned off\u2019 when the promoter is absent. For example, when you go for a walk,\nyour leg muscle cells do work and need to metabolize glucose to supply the energy. Thus, some genes need to be", "metadata": {"page": 39}}, {"page_content": "turned on to make the required proteins that facilitate this metabolism. When you are resting, these proteins are not\nneeded\u2014furthermore, they clutter up the cells. Thus, these genes are turned off when you rest. This on/off dichotomy\nis controlled by the relative concentrations of promoter (and repressor) proteins. A simpli\ufb01ed version of the how this\ncomes about is as follows: The nerve impulses to the muscle cell cause a change in the folding of a few particular\n36 Chapter 6. Random variables", "metadata": {"page": 39}}, {"page_content": "proteins on the cell surface. This change starts a chain reaction that ultimately frees up promoter proteins which then\nbind to the promoter regions of the DNA, thus activating the genes for the glucose metabolizing machinery. The latter\nthen make lots of metabolic products for use while walking.\nWith this as the background, here is my example: Let S denote the set of positive integers up to some large number", "metadata": {"page": 40}}, {"page_content": "N, and let P(s) denote the probability that a given protein is attached to a given promoting stretch of DNA for the\nfraction of time s\nN. We measure the values of a function,f, which is the amount of protein that would be produced by\nthe cell were the promoter operative. Thus, we measure P(f = r), the frequencies of \ufb01nding level rof the protein. A\nmodel from biochemistry might tell us how the value of f depends on the fraction of time that the promoter protein is", "metadata": {"page": 40}}, {"page_content": "attached to the promoter region of the DNA. With the model in hand, we could then write\nPf(r) =\n\u2211\ns\narsP(s), (6.7)\nwhere ars is obtained from the theoretical model. Note that our task then is to solve for the collection {P(s)},\neffectively solving a version of the linear equation in (6.7).\n6.7 Independent random variables and correlation matrices\nBefore discussing these notions in the general context, I brie\ufb02y describe an example from epidemiology. LetSdenote", "metadata": {"page": 40}}, {"page_content": "the sample space that is the set of adults in the United States. Letfdenote the function onSthat assigns to each person\nthat person\u2019s blood pressure. Let gdenote the function on S that assigns to each person the level of salt taken on an\naverage day. Question: Does knowledge of gsay anything about the frequency of occurrence of the various values of\nf? Said differently, can the values of f and gvary independently, or are there correlations between the values of these\ntwo functions?", "metadata": {"page": 40}}, {"page_content": "two functions?\nTo explore these questions in a simple example, suppose that S is the sample space that consists of triples (a,b,c)\nwhere each entry is an integer from 1 through 3. Thus, S has 27 elements. Suppose, in addition, that S has the\nprobability function, P, where all elements have the same probability. Thus, P assigns 1\n27 to each element in S.\nIntroduce two random variables on S. The \ufb01rst, f, is given by f(a,b,c) = a+ b. The second, g, is given by", "metadata": {"page": 40}}, {"page_content": "g(a,b,c) = b+ c. The values of both f and g can be any number from the 5 element set {2,3,4,5,6}. Here is a\nquestion: Does knowledge of the values of ggive any information about those of f? This question can be reposed as\nfollows: If rand \u03c1are integers from {2,3,4,5,6}, is the event that f = rindependent from the event that g= \u03c1? The\nrandom variables f and gare called independent random variables when such is the case for all rand all \u03c1.", "metadata": {"page": 40}}, {"page_content": "Remember that sets Aand B from a sample space S are called independent when P(A \u2229B) = P(A)P(B). Thus, to\nanswer the question posed above, we should look at\nP(Event that f = rand g= \u03c1) \u2212P(Event that f = r) \u00b7P(Event that g= \u03c1).\nIf this is zero for all values of rand \u03c1, then f and gare independent. Consider \ufb01rst the case where r = 2 and \u03c1= 2.\nThen the set where f = 2 and g = 2 consists of just the triple (1,1,1) and so it has probability 1\n27 . Meanwhile, the", "metadata": {"page": 40}}, {"page_content": "27 . Meanwhile, the\nset where f = 2 consists of 3 triples, (1,1,1), (1,1,2) and (1,1,3). Thus, it has probability 3\n27 . Likewise, the set\nwhere g = 2 has probability 3\n27 since it consists of the three elements (1,1,1), (2,1,1) and (3,1,1). The quantity\nP(f = 2 and g = 2) \u2212P(f = 2)P(g = 2) is equal to 2\n81 . Since this is not zero, the random variables f and gare not\nindependent.\nFor a second example, takeS, P and f as just described, but now consider the case wheregis the random variable that", "metadata": {"page": 40}}, {"page_content": "assigns c\u2212bto the triple (a,b,c). In this case, gcan take any integer in the range from \u22122 to 2. Consider f = 2 and\ng = 0. In this case, the set where both f = 2 and g = 0 consists of (1,1,1) and so has probability 1\n27 . The set where\ng = 0 consists of 9 elements, these of the form (a,b,b ) where aand bcan be any integers from 1 through 3. Thus,\nP(g = 0) = 9\n27 = 1\n3 . Since 1\n27 = 1\n9 \u00b71\n3 , the event that a+ b = 2 is independent from the event that c\u2212b = 0. Of", "metadata": {"page": 40}}, {"page_content": "3 . Since 1\n27 = 1\n9 \u00b71\n3 , the event that a+ b = 2 is independent from the event that c\u2212b = 0. Of\ncourse, to see if f and gare independent random variables, we need to consider other values for f and for g.\nTo proceed with this task, consider the case where f = 2 and g = \u22122. The event that g = \u22122 consists of three\nelements, these of the form (a,3,1) where acan be any integer from 1 to 3. As a consequence, the event that g= \u22122\n6.7. Independent random variables and correlation matrices 37", "metadata": {"page": 40}}, {"page_content": "has probability 3\n27 = 1\n9 . Thus, the product of the probability of the event that f = 2 with that of the event that g= \u22122\nis 1\n81 . On the other hand, the event that both f = 2 and g = \u22122 is the empty set. As the empty set has probability 0,\nso f and gare not independent.\nAn example where f and a function gare independent is that where g(a,b,c) = c.\nTurn now to the general case where S is any given sample space, P is a probability measure on S, and both f and", "metadata": {"page": 41}}, {"page_content": "g are random variables on S. We are again interested in whether knowledge of g tells us something about f. I can\nformalize the discussion just held by introducing the correlation matrix of the two random variables. The correlation\nmatrix measures the extent to which the event that f has a given value is independent of the event that ghas a given\nvalue.\nTo see how this matrix is de\ufb01ned, label the possible values forf as {r1,...,r N}and label those of gas {\u03c11,...,\u03c1 M}.", "metadata": {"page": 41}}, {"page_content": "Here, N need not equal M, and there is no reason for the r\u2019s to be the same as the \u03c1\u2019s. Indeed, f can concern apples\nand goranges: The r\u2019s might be the weights of apples, rounded to the nearest gram; and the \u03c1\u2019s might be the acidity\nof oranges, measured in pH to two decimal places. Or, the values of f might be blood pressure readings rounded to\nthe \ufb01rst decimal place, and those of gthe intake (in milligrams) of salt per day.", "metadata": {"page": 41}}, {"page_content": "the \ufb01rst decimal place, and those of gthe intake (in milligrams) of salt per day.\nIn any event, for a givenf and g, the correlation matrix is the N\u00d7M matrix Cwith coef\ufb01cients (Ck,j)1\u2264k\u2264N,1\u2264j\u2264M\nwhere\nCkj = P(f = rk and g= \u03c1j) \u2212P(f = rk)P(g= \u03c1j). (6.8)\nIn this last equation, P(f = rk and g= \u03c1j) is the probability of the event that f has value rk and g has value \u03c1j; it\nis the sum of the values of P on the elements s \u2208S where f(s) = rk and g(s) = \u03c1j. Thus, Ckj = 0 if and only if", "metadata": {"page": 41}}, {"page_content": "the event that f = rk is independent from the event that g = \u03c1j. If all entries are zero, the random variables f and g\nare said to be independent random variables. This means that the probabilities for the values of f have no bearing on\nthose for gand vice-versa.\nConsider this in our toy model from (6.2): Suppose thatgmeasures the number of cell division cycles in six hours from\nour well-fed bacteria. Suppose, in particular, that the values of grange from 0 to 2, and that g(k) = 2 if k \u2208{1, 2},", "metadata": {"page": 41}}, {"page_content": "that g(k) = 1 if 3 \u2264k\u22647, and that g(k) = 0 if k\u22657. In this case, the probability that ghas value \u03c1\u2208{0,1,2}is\n2\n5 for \u03c1= 0, 2\n5 for \u03c1= 1, and 1\n5 for \u03c1= 2. (6.9)\nLabel the values of f so that r1 = 0, r2 = 1\n10 , ..., r10 = 9\n10 , r10 = 1. Meanwhile, label those of g as in the order they\nappear above, \u03c11 = 0, \u03c12 = 1 and \u03c13 = 2. The correlation matrix in this case is an 11 \u00d73 matrix. For example, here\nare the coef\ufb01cients in the \ufb01rst row:\nC11 = 11\n50 , C 12 = \u22123\n25 , C 13 = \u22123\n25 .", "metadata": {"page": 41}}, {"page_content": "are the coef\ufb01cients in the \ufb01rst row:\nC11 = 11\n50 , C 12 = \u22123\n25 , C 13 = \u22123\n25 .\nTo explain, note that the event thatf = 0 consists of the subset{10,..., 20}in the set of integers from1 to 20. This set\nis a subset of the event that gis zero since the latter set is {7,..., 20}. Thus, P(f = 0 and g= 0) = P(f = 0) = 3\n10 ,\nwhile there are no events where f is 0 and gis either 1 or 2.", "metadata": {"page": 41}}, {"page_content": "10 ,\nwhile there are no events where f is 0 and gis either 1 or 2.\nBy the way, this example illustrates something of the contents of the correlation matrix: If Ckj >0, then the outcome\nf = rk is relatively likely to occur when g = \u03c1j. On the other hand, if Ckj <0, then the outcome f = rk is unlikely\nto occur when g= \u03c1j. Indeed, in the most extreme case, the function f is never rk when gis \u03c1j and so\nCkj = \u2212P(f = rk)P(g= \u03c1j).", "metadata": {"page": 41}}, {"page_content": "Ckj = \u2212P(f = rk)P(g= \u03c1j).\nAs an addendum to this discussion about correlation matrices, I say again that statisticians are want to use a single\nnumber to summarize behavior. In the case of correlations, they favor what is known as the correlation coef\ufb01cient.\nThe latter, c(f,g), is obtained from the correlation matrix and is de\ufb01ned as follows:\nc(f,g) = 1\n\u03c3(f)\u03c3(g)\n\u2211\nk,j\n(rk \u2212\u00b5(f))(\u03c1j \u2212\u00b5(g))Ckj. (6.10)", "metadata": {"page": 41}}, {"page_content": "c(f,g) = 1\n\u03c3(f)\u03c3(g)\n\u2211\nk,j\n(rk \u2212\u00b5(f))(\u03c1j \u2212\u00b5(g))Ckj. (6.10)\nHere, \u00b5(f) and \u03c3(f) are the respective mean and standard deviation of f, while \u00b5(g) and \u03c3(g) are their counterparts\nfor g.\n38 Chapter 6. Random variables", "metadata": {"page": 41}}, {"page_content": "6.8 Correlations and proteomics\nLet\u2019s return to the story about promoters for genes. In principle, a given protein might serve as a promoting protein\nfor one or more genes, or it might serve as a promoter for some genes and a repressor for others. Indeed, one way a\nprotein can switch off a gene is to bind to the DNA in such a way as to cause all or some key part of the gene coding\nstretch to be covered.", "metadata": {"page": 42}}, {"page_content": "stretch to be covered.\nAnyway, suppose that f measures the level of protein #1 and gmeasures that of protein #2. The correlation matrix for\nthe pair f and gis a measure of the extent to which the levels of f and gtend to track each other. If the coef\ufb01cients\nof the matrix are positive, then f and g are typically both high and both low simultaneously. If the coef\ufb01cients are\nnegative, then the level of one tends to be high when the level of the other is low. This said, note that a reasonable", "metadata": {"page": 42}}, {"page_content": "approximation to the correlation matrix can be inferred from experimental data: One need only measure simultaneous\nlevels of f and gfor a cell, along with the frequencies that the various pairs of levels are observed.\nBy the way, the search for correlations in protein levels is a major preoccupation of cellular biologists these days.\nThey use rectangular \u2018chips\u2019 that are covered with literally thousands of beads in a regular array, each coated with a", "metadata": {"page": 42}}, {"page_content": "different sort of molecule, each sort of molecule designed to bind to a particular protein, and each \ufb02uorescing under\nultraviolet light when the protein is bound. Crudely said, the contents of a given kind of cell at some known stage in\nits life cycle are then washed over the chip and the ultraviolet light is turned on. The pattern and intensity of the spots\nthat light up signal the presence and levels in the cell of the various proteins. Pairs of spots that tend to light up under", "metadata": {"page": 42}}, {"page_content": "the same conditions signal pairs of proteins whose levels in the cell are positively correlated.\n6.9 Exercises:\n1. A number from the three element set {\u22121,0,1}is selected at random; thus each of \u22121, 0 or 1 has probability 1\n3\nof appearing. This operation is repeated twice and so generates an ordered set (i1,i2) where i1 can be any one\nof \u22121, 0 or 1, and likewise i2. Assume that these two selections are done independently so that the event that i2", "metadata": {"page": 42}}, {"page_content": "has any given value is independent from the value of i1.\n(a) Write down the sample space that corresponds to the possible pairs {i1,i2}.\n(b) Let f denote the random variable that assignsi1 + i2 to any given (i1,i2) in the sample space. Write down\nthe probabilities P(f = r) for the various possible values of r.\n(c) Compute the mean and standard deviation of f.\n(d) Let gdenote the random variable that assigns |i1|+ |i2|to any given (i1,i2). Write down the probabilities", "metadata": {"page": 42}}, {"page_content": "P(g= \u03c1) for the various possible values of \u03c1.\n(e) Compute the mean and standard deviation of g.\n(f) Compute the correlation matrix for the pair (f,g).\n(g) Which pairs of (r,\u03c1) with r a possible value for f and \u03c1 one for g are such that the event f = r is\nindependent from the event g= \u03c1?\n2. Let Sdenote the same sample space that you used in Problem 1, and let P denote some hypothetical probability", "metadata": {"page": 42}}, {"page_content": "function on S. Label the elements in Sby consecutive integers starting from1, and also label the possible values\nfor f by consecutive integers starting from 1. Let xj denote P(sj) where sj is the jth element of the sample\nspace. Meanwhile, let yk denote the P(f = rk) where rk is the kth possible value for f. Write down the linear\nequation that relates {yk}to {xj}.\n3. Repeat Problem 1b through 1e in the case that the probability of selecting either \u22121 or 1 in any given selection\nis 1", "metadata": {"page": 42}}, {"page_content": "is 1\n4 and that of selecting 0 is 1\n2 .\n4. Suppose that N is a positive integer, and N selections are made from the set {\u22121,0,1}. Assume that these\nare done independently so that the probability of any one number arising on the kth selection is independent of\nany given number arising on any other selection. Suppose, in addition, that the probability of any given number\narising on any given selection is 1\n3 .\n6.8. Correlations and proteomics 39", "metadata": {"page": 42}}, {"page_content": "(a) How many elements are in the sample space for this problem?\n(b) What is the probability of any given element?\n(c) Let f denote the random variable that assigns to any given(i1,...,i N) their sum, thus: f = i1 +\u00b7\u00b7\u00b7+iN.\nWhat are P(f = N) and P(f = N \u22121)?\n5. There are 32 teams in the National Football league, and each team plays 16 games.\nThese teams are divided into 8 divisions, and any given team plays each of the other three teams in its division", "metadata": {"page": 43}}, {"page_content": "twice during the season. Now, give each team 1 point for a win and 0 for a loss and 1\n2 for a tie. Adding up\nthese numbers for each team yields an ordered sequence of numbers, \u20d7 n= (n1,n2,...,n 32) where each nk is\nnon-negative, but no greater than 16. Also, n1 + n2 + \u00b7\u00b7\u00b7 + n32 = 256. Our sample space is the set, S, of\nsuch ordered sequences, thus the set of all possible vectors \u20d7 n\u2208R32 of the sort just described. For each integer,", "metadata": {"page": 43}}, {"page_content": "k \u2208{0,1,..., 16}, de\ufb01ne a function, fk : S \u2192{0, 1,..., 32}, so that fk(\u20d7 n) =number of teams with point\ntotal k or k+ 1\n2 (remember that a team gets 1\n2 point for a tie). To see what these random variables look like,\nde\ufb01ne a vector, \u20d7f in R17 whose jth component is fj\u22121. The columns in the table that follows gives the actual\nvector \u20d7f for the past ten seasons in the NFL1.\nWins 2009 2008 2007 2006 2005 2004 2003 2002 2001 2000 Count Wins\n0 1 1 0\n1 1 1 1 1 4 1\n2 1 2 1 1 1 1 1 8 2", "metadata": {"page": 43}}, {"page_content": "0 1 1 0\n1 1 1 1 1 4 1\n2 1 2 1 1 1 1 1 8 2\n3 1 1 1 1 1 1 2 8 3\n4 2 2 4 2 5 2 4 2 2 25 4\n5 3 3 2 2 3 4 6 2 3 3 31 5\n6 1 1 3 4 5 3 2 4 2 25 6\n7 3 2 7 3 3 3 5 6 3 35 7\n8 5 5 4 8 1 4 2 3 2 2 36 8\n9 5 5 2 4 4 4 1 6 2 4 37 9\n10 3 2 5 3 3 3 6 4 3 4 36 10\n11 3 4 2 6 1 1 3 3 4 27 11\n12 1 4 2 1 2 4 3 2 3 22 12\n13 2 1 3 2 2 1 1 2 1 15 13\n14 1 1 1 1 1 1 6 14\n15 1 1 15\n16 1 1 16\nTable 6.1: NFL teams counted by number of wins, by season", "metadata": {"page": 43}}, {"page_content": "14 1 1 1 1 1 1 6 14\n15 1 1 15\n16 1 1 16\nTable 6.1: NFL teams counted by number of wins, by season\n(a) Should you expect f15 and f16 to be independent random variables? Explain your answer.\n(b) Should you expect f8 and any fk to be independent random variables? Explain your answer.\n(c) From the table above, what are the mean and standard deviation of the data representing f8 for the last ten\nyears? See equations (1.1) and (1.3).", "metadata": {"page": 43}}, {"page_content": "years? See equations (1.1) and (1.3).\n1Note: The Houston Texans did not start play until the 2002 season, so for the \ufb01rst two seasons in our table there are only 31 teams. Also, there\nwere two tie games in this timespan: Eagles-Bengals in 2008 and Falcons-Steelers in 2002.\n40 Chapter 6. Random variables", "metadata": {"page": 43}}, {"page_content": "CHAPTER\nSEVEN\nThe statistical inverse problem\nHere is a basic issue in statistic: Experiments are often done to distinguish various hypothesis about the workings of a\ngiven system. This is to say that you have various models that are meant to predict the behavior of the system, and you\ndo experiments to see which model best predicts the experimental outcomes. The ultimate goal is to use the observed\ndata to \ufb01nd the correct model.", "metadata": {"page": 44}}, {"page_content": "data to \ufb01nd the correct model.\nA more realistic approach is to use the observed data to generate a probability function on the set of models that\nare under consideration. The assigned probability should give the odds for the correctness of a given model. There\nare many ways to do this. Usually, any two of the resulting probability functions assign different probabilities to the", "metadata": {"page": 44}}, {"page_content": "models. For this reason, if for no other, some serious thought must be given as to which (if any) to use. In any event, I\ndescribe some of these methods in what follows.\nTo indicate the \ufb02avor of what is to follow, consider \ufb01rst an example. Here is some background: Suppose that a given\nstretch of DNA on some chromosome has N sites. If I look at this stretch in different people, I will, in general,", "metadata": {"page": 44}}, {"page_content": "not get the same sequence of DNA. There are typically some number of sites where the bases differ. Granted this\nbasic biology, I then sample this site in a large number of people. If I \ufb01nd that one particular DNA sequence occurs\nmore often then any other, I deem it to be theconsensus sequence. Suppose for simplicity that there is, in fact, such a\nconsensus sequence. My data gives me more than just a consensus sequence; I also have numbers{p0,p1,p2,...,p N}", "metadata": {"page": 44}}, {"page_content": "where any given pk is the fraction of people whose DNA differs at ksites from the consensus sequence. For example,\np0 is the fraction of people whose DNA for this N site stretch is identical to the consensus sequence. For a second\nexample, p52 is the fraction of people whose DNA for this stretch differs at 52 sites from the consensus sequence.\nI wish to understand how these number {p0,p1,...,p N}arise. If I believe that this particular stretch of DNA is \u2018junk", "metadata": {"page": 44}}, {"page_content": "DNA\u2019, and so has no evolutionary function, I might propose the following: There is some probability for a substituted\nDNA base to appear at any given site of thisN site long stretch of DNA. The simplest postulate I can make along\nthese lines is that the probability of a substitution is independent of the particular site. I am interested in \ufb01nding out\nsomething about this probability of a substitution. To simplify matters, I look for probabilities of the formm\n100 where", "metadata": {"page": 44}}, {"page_content": "100 where\nm\u2208{0,1,2,..., 100}. So, my basic question is this:\nGiven the data {p0,...,p N}, what is the probability that a given \u03b8 = m\n100 is the true probability for a\nsingle site substitution in the given stretch of DNA?\nThe interesting thing here is that if I know this probability, thus the number \u03b8from the set {0, 1\n100 , 2\n100 ,..., 99\n100 ,1},\nthen I can predict the sequence {p0,p1,...,p N}. We will give a general formula for this in a later chapter. However,", "metadata": {"page": 44}}, {"page_content": "for small values of N, you needn\u2019t know the general formula as we can work things out directly.\nConsider \ufb01rst N = 1. Then there is just p0 and p1. The probability, given \u03b8for one site change is \u03b8(by de\ufb01nition),\nthus the probability for no site changes is 1 \u2212\u03b8. Hence, given \u03b8, I would predict\nProbability of 0 site changes (given \u03b8) is 1 \u2212\u03b8.\nProbability of 1 site change (given \u03b8) is \u03b8.", "metadata": {"page": 44}}, {"page_content": "Probability of 0 site changes (given \u03b8) is 1 \u2212\u03b8.\nProbability of 1 site change (given \u03b8) is \u03b8.\nI can then compare these numbers with what I observed, p0 and p1. Doing so, I see that there is an obvious choice for\n\u03b8, that with p0 = 1 \u2212\u03b8and p1 = \u03b8. Of course, if I restrict \u03b8to fractions of 100, then the I should choose the closest\n41", "metadata": {"page": 44}}, {"page_content": "such fraction to p1. Note, by the way, that p0 + p1 = 1 by virtue of their de\ufb01nition as fractions, so the two conditions\non \u03b8amount to one and the same thing.\nConsider next the case N = 2. Given \u03b8, I would then predict\nProbability of 0 site changes (given \u03b8) is (1 \u2212\u03b8)2.\nProbability of 1 site change (given \u03b8) is 2\u03b8(1 \u2212\u03b8). (7.1)\nProbability of 2 site changes (given \u03b8) is \u03b82.\nTo see why this is, think of \ufb02ipping two coins, with heads= site change and tails= no site change; and with probability", "metadata": {"page": 45}}, {"page_content": "equal to \u03b8for any given coin to have heads. The probability of the \ufb01rst coin/site having heads is\u03b8and of it having tails\nis (1 \u2212\u03b8). Likewise for the second coin/site. So, they both end up tails with probability (1 \u2212\u03b8)2. Likewise, they both\nend up heads with probability \u03b82. Meanwhile, there are two ways for one head to appear, either on the \ufb01rst coin/site,\nor on the second. Each such event has probability \u03b8(1 \u2212\u03b8), so the probability of one coin/site to have heads is twice\nthis.", "metadata": {"page": 45}}, {"page_content": "this.\nThis same sort of comparison with coin \ufb02ipping leads to the following for the N = 3 case:\nProbability of 0 site changes (given \u03b8) is (1 \u2212\u03b8)3.\nProbability of 1 site change (given \u03b8) is 3\u03b8(1 \u2212\u03b8)2.\nProbability of 2 site changes (given \u03b8) is 3\u03b82(1 \u2212\u03b8). (7.2)\nProbability of 3 site changes (given \u03b8) is \u03b83.\nHowever, note that in the case N = 2 the data {p0,p1,p2}need not lead to any obvious candidate for \u03b8. This is\nbecause the equations\np0 = (1 \u2212\u03b8)2 and p1 = 2\u03b8(1 \u2212\u03b8) and p2 = \u03b82", "metadata": {"page": 45}}, {"page_content": "because the equations\np0 = (1 \u2212\u03b8)2 and p1 = 2\u03b8(1 \u2212\u03b8) and p2 = \u03b82\nneed not have a solution. Even using the fact thatp0 + p1 + p2 = 1, there are still two conditions for the one unknown,\n\u03b8. Consider, for example, p0 = 9\n16 , p1 = 3\n16 , p2 = 1\n4 . The equations p2 = \u03b82 and p0 = (1 \u2212\u03b8)2 lead to very different\nchoices for \u03b8. Indeed, the equation p2 = \u03b82 gives \u03b8 = 1\n2 and the equation p0 = (1 \u2212\u03b8)2 gives \u03b8 = 1\n4 ! This same\nproblem gets worse as N gets ever larger.", "metadata": {"page": 45}}, {"page_content": "2 and the equation p0 = (1 \u2212\u03b8)2 gives \u03b8 = 1\n4 ! This same\nproblem gets worse as N gets ever larger.\nWhat\u2019s to be done? Here is one approach: Use the data {p0,...,p N}to \ufb01nd a probability function on the possible\nvalues of \u03b8. This would be a probability function on the sample space \u0398 = {0, 1\n100 , 2\n100 ,..., 99\n100 ,1}that would give\nthe probability for a given to be the true probability of a site substitution given the observed data {p0,...,p N}and", "metadata": {"page": 45}}, {"page_content": "given that my model of equal chance of a site substitution at each of the N sites along the DNA strand is correct.\nI will use Pto denote a probability function on \u0398. This is to say that P( m\n100 ) is the probability that Passigns to any\ngiven \u03b8. For example, P( 3\n100 ) is the probability that 3\n100 is the true probability of a mutation occurring in any given\ncell. Likewise, P( 49\n100 ) is the probability that 49\n100 is the true probability of a mutation occurring in any given cell.", "metadata": {"page": 45}}, {"page_content": "100 is the true probability of a mutation occurring in any given cell.\nImagine for the moment that I have a probability function, P, on the sample space \u0398. I can then use Pto make a\ntheoretical prediction of what the observed data should be. This uses the notion of conditional probability. Here is\nhow it works in the case N = 2:\nProb(ksite changes) = P (k|0) P(0) + P\n(\nk| 1\n100\n)\nP( 1\n100 ) + \u00b7\u00b7\u00b7 + P (k|1) P(1), (7.3)", "metadata": {"page": 45}}, {"page_content": "Prob(ksite changes) = P (k|0) P(0) + P\n(\nk| 1\n100\n)\nP( 1\n100 ) + \u00b7\u00b7\u00b7 + P (k|1) P(1), (7.3)\nwhere the notation uses P (k|\u03b8) to denote the probability of there being k site changes given that \u03b8 is the correct\nprobability. Note that I use the conditional probability notation from Chapter 3, although the context here is slightly\ndifferent. Anyway, grant me this small abuse of my notation and agree to view P(k|\u03b8) as an honest conditional", "metadata": {"page": 45}}, {"page_content": "probability. Note in this regard that it obeys all of the required rules: Each P (k|\u03b8) is non-negative, and the sum\nP (0 |\u03b8) + P (1 |\u03b8) + \u00b7\u00b7\u00b7 + P (N|\u03b8) = 1 for each \u03b8.\nThe key point here is that I know what these conditional probabilities P (k|\u03b8) are. For example, they in the case\nN = 2, they are given in (7.1) and in the case N = 3 they are given in (7.3). Thus, the various versions of (7.3) for\n42 Chapter 7. The statistical inverse problem", "metadata": {"page": 45}}, {"page_content": "N = 2 are\nProb(0 site changes) =\n\u2211\nm=0,1,...100\n(1 \u2212 m\n100 )2P( m\n100 )\nProb(1 site change) =\n\u2211\nm=0,1,...100\n2 m\n100 (1 \u2212 m\n100 )P( m\n100 ) (7.4)\nProb(2 site changes) =\n\u2211\nm=0,1,...100\n( m\n100 )2P( m\n100 ).\nOf course, I don\u2019t yet have a probability functionP. Indeed, I am looking for one; and I hope to use the experimental\ndata {p0,...,p N}to \ufb01nd it. This understood, it makes some sense to consider only those versions of Pthat give the", "metadata": {"page": 46}}, {"page_content": "correct experimental results. This is to say that I consider as reasonable only those probability functions on the sample\nspace that give Prob(k site changes) = pk for all choices of k \u2208{0,...,N }. For example, in the case N = 2, these\ngive the conditions\np0 =\n\u2211\nm=0,1,...100\n(1 \u2212 m\n100 )2P( m\n100 )\np1 =\n\u2211\nm=0,1,...100\n2 m\n100 (1 \u2212 m\n100 )P( m\n100 ) (7.5)\np2 =\n\u2211\nm=0,1,...100\n( m\n100 )2P( m\n100 ).\nTo make it even more explicit, suppose again that I measured p0 = 3\n4 , p1 = 3\n16 and p2 = 1", "metadata": {"page": 46}}, {"page_content": "100 ).\nTo make it even more explicit, suppose again that I measured p0 = 3\n4 , p1 = 3\n16 and p2 = 1\n16 . Then what is written\nin (7.5) would read:\n3\n4 = P(0) + ( 99\n100 )2P( 1\n100 ) + \u00b7\u00b7\u00b7 + ( 1\n100 )2P( 99\n100 )\n3\n16 = 2( 1\n100 ) 99\n100 )P( 1\n100 ) + 2( 2\n100 )( 98\n100 )P( 2\n100 ) \u00b7\u00b7\u00b7 + 2( 99\n100 )( 1\n100 )P( 1\n100 ) (7.6)\n1\n16 = ( 1\n100 )2P( 1\n100 ) + \u00b7\u00b7\u00b7 + ( 99\n100 )2P( 99\n100 ) + P(1).", "metadata": {"page": 46}}, {"page_content": "100 )( 1\n100 )P( 1\n100 ) (7.6)\n1\n16 = ( 1\n100 )2P( 1\n100 ) + \u00b7\u00b7\u00b7 + ( 99\n100 )2P( 99\n100 ) + P(1).\nNotice, by the way, that what is written here constitutes a system of 3 equations for the 101 unknown numbers,\n{P(0),P( 1\n100 ),..., P(1)}. As we know from our linear algebra, there will be in\ufb01nitely many solutions. In the general\ncase of a site with length N, the analog of (7.5) has N + 1 equations, with these reading\np0 =\n\u2211\nm=0,1,...100\nP\n(\n0 | m\n100\n)\nP( m\n100 )\np1 =\n\u2211\nm=0,1,...100\nP\n(\n1 | m\n100", "metadata": {"page": 46}}, {"page_content": "p0 =\n\u2211\nm=0,1,...100\nP\n(\n0 | m\n100\n)\nP( m\n100 )\np1 =\n\u2211\nm=0,1,...100\nP\n(\n1 | m\n100\n)\nP( m\n100 )\n..\n. (7.7)\npN =\n\u2211\nm=0,1,...100\nP\n(\nN| m\n100\n)\nP( m\n100 )\nGiven that I know what P(k|\u03b8) is for any given k, this constitutes a system of N+ 1 equations for the 101 unknowns\n{P(0),P( 1\n100 ),..., P(1)}. In particular, if N >100, then there will be more equations than unknowns and so there\nmay be no solutions! To make this look exactly like a system of linear equation, change the notation so as to denote", "metadata": {"page": 46}}, {"page_content": "the conditional probability P\n(\n0 | m\n100\n)\nas a0m, P\n(\n1 | m\n100\n)\nas a01m, etc. Use xm for P( m\n100 ). And, use y0 for p0, y1 for\np1, etc. This done, then (7.7) reads\ny0 = a00x0 + a01x1 + \u00b7\u00b7\u00b7\ny1 = a10x1 + a11x1 + \u00b7\u00b7\u00b7 (7.8)\n.\n.\n.\nIn any event, here is a lesson from this:\n43", "metadata": {"page": 46}}, {"page_content": "In general, the equations in(7.5) for the case N = 2, or their analogs in(7.7) for N >2 do not determine\na God-given probability function on the sample space.\nThe task of \ufb01nding a probability function on the sample space from the experimental data, one that comes reasonably\nclose to solving (7.7) and makes good biological sense, is an example of what I call the statistical inverse problem.\n7.1 A general setting", "metadata": {"page": 47}}, {"page_content": "7.1 A general setting\nThis sort of problem arises in the following general setting: Suppose that I am interested in some system or phenomena,\nand have a set, \u0398, of models of how it works. I shall assume in what follows that \u0398 is a \ufb01nite set. In the example\noutlined above, the set \u0398 consists of the fractions of the form m\n100 with m \u2208{0, 1,..., 100}. I plan to take data to\nexplore the phenomena. Suppose that there is a set, K, of possible experimental outcomes. In the example of the", "metadata": {"page": 47}}, {"page_content": "introduction, K, consists of the numbers 0,1,...,N + 1, where the integer khere denotes the number of sites on the\ngiven N-site DNA strand that differ from the consensus strand. Even in the general case, if there are but a \ufb01nite set of\npossible outcomes, I can label them by consecutive integers starting from zero and this I now do.\nAs I know a competent model builder, I make sure that each model from \u0398 determines a probability function on K.", "metadata": {"page": 47}}, {"page_content": "This is to say any given model \u03b8 \u2208\u0398 gives a predicted probability for any given outcome in K. I write \u03b8\u2019s version\nof the probability of the kth outcome as P (k|\u03b8). Thus, P (k|\u03b8) is the probability of having the kth outcome were\n\u03b8 the correct model. In the N = 2 example from the introduction, P (0 |\u03b8) = (1 \u2212\u03b8)2, P (1 |\u03b8) = 2 \u03b8(1 \u2212\u03b8) and\nP (2 |\u03b8) = \u03b82.\nI take lots of data and so generate an experimentally determined probability function, Pexp, on K. Here, Pexp(k) is the", "metadata": {"page": 47}}, {"page_content": "fraction of the experiments where kth outcome appeared. In the example from the introduction, the number P exp(k)\nis what I called pk.\nThe statistical inverse problem in this context is as follows: Use the experimentally determined probability function\nPexp to determine a probability function, P, on the set \u0398. You are asked to use what you know, the data derived\nfrequencies Pexp and the set of conditional probabilities, the various P (k|\u03b8), to \ufb01nd those model parameters from \u0398", "metadata": {"page": 47}}, {"page_content": "that are more likely to be correct, and which are less likely to be correct.\nKeep in mind that there is, in general, no God-given way to generate a probability function on the set \u0398. Even so,\nthere are various popular methods and one or more may well be useful in any given context.\n7.2 The Bayesian guess\nThe Bayesian turns things upside down and reasons is as follows: Suppose that I have a conditional probability that", "metadata": {"page": 47}}, {"page_content": "tells me the probability that the model labeled by \u03b8 is correct given that the kth outcome in K appears. Suppose I\nhave such a conditional probability for each outcome kand each parameter \u03b8. I use P (\u03b8|k) to denote this conditional\nprobability. If I had such a collection of conditional probabilities, then I could use the general form of Bayes\u2019 theorem\nin (3.5) to write a probability function on the set \u0398. This would be the function whose value on any given \u03b8is", "metadata": {"page": 47}}, {"page_content": "P(\u03b8) = P (\u03b8|0) Pexp(0) + P (\u03b8|1) Pexp(1) + \u00b7\u00b7\u00b7 + P (\u03b8|N) Pexp(N) (7.9)\nIn our N = 2 case from the introduction, there are 101 versions of (7.9), one for each fraction {m\n100 }m=0,1,...100, and\nthe mth version reads\nP( m\n100 ) = P\n(m\n100 |0\n)\nPexp(0) + P\n(m\n100 |1\n)\nPexp(1) + P\n(m\n100 |2\n)\nPexp(2) (7.10)\nOf course, this is wishful thinking without some scheme to obtain the various conditional probabilities P (\u03b8|k).", "metadata": {"page": 47}}, {"page_content": "The Bayesian makes the following rather ad-hoc proposal: Let\u2019s use forP the formula\nPBayes (\u03b8|k) = P(k|\u03b8)\nZ(k) (7.11)\n44 Chapter 7. The statistical inverse problem", "metadata": {"page": 47}}, {"page_content": "where Z(k) = \u2211\n\u03b8\u2208\u0398 P(k|\u03b8) is needed so that the PBayes (\u03b8|k) can be interpreted as a conditional probability.\nThe point here is that the probability of some \u03b8 happening given k is 1, so with any k \ufb01xed, the sum of the various\nPBayes (\u03b8|k) for all of the \u03b8in \u0398 has to be 1. This is guaranteed with the factor 1\nZ(k) present, but may not be the case\nwithout this factor. Using PBayes in (7.9) gives what is called the Bayesian version of a probability function on \u0398:\nPBayes(\u03b8) = P(0 |\u03b8) 1", "metadata": {"page": 48}}, {"page_content": "PBayes(\u03b8) = P(0 |\u03b8) 1\nZ(0) Pexp(0) + P(1 |\u03b8) 1\nZ(1) Pexp(1) + \u00b7\u00b7\u00b7 . (7.12)\nNote that the presence of Zguarantees that the probabilities de\ufb01ned by the left-hand side of (7.12) sum to 1.\nIn effect, the Bayesian probability function on \u0398 is obtained by approximating the unknown conditional probability\nfor \u03b8given outcome kby the known conditional probability of outcome kgiven \u03b8(times the factor 1\nZ(k) ). This may", "metadata": {"page": 48}}, {"page_content": "Z(k) ). This may\nor may not be a good approximation. In certain circumstances it is, and in others, it isn\u2019t.\n7.3 An example\nTo see how this works in an example, consider theN = 2 case of the introduction. In this case,\u03b8is one of the fractions\nfrom the set \u0398 = {m\n100 }m=0,1,...100 and (7.1) \ufb01nds that P(0 |\u03b8) = (1 \u2212\u03b8)2, P(1 |\u03b8) = 2\u03b8(1 \u2212\u03b8) and P(2 |\u03b8) = \u03b82.\nThis understood, then\nZ(0) = 1 + ( 99\n100 )2 + ( 98\n100 )2 + \u00b7\u00b7\u00b7 + ( 1\n100 )2 \u224833.835\nZ(1) = 2( 1\n100 )( 99\n100 ) + 2( 98\n100 )( 2", "metadata": {"page": 48}}, {"page_content": "100 )2 + ( 98\n100 )2 + \u00b7\u00b7\u00b7 + ( 1\n100 )2 \u224833.835\nZ(1) = 2( 1\n100 )( 99\n100 ) + 2( 98\n100 )( 2\n100 ) + \u00b7\u00b7\u00b7 + 2( 99\n100 )( 1\n100 ) \u224833.33 (7.13)\nZ(2) = ( 1\n100 )2 + ( 2\n100 )2 + \u00b7\u00b7\u00b7 + 1 \u224833.835.\nUsing this in (7.12) gives the Bayesian probability function whose value on m\n100 is\nPBayes( m\n100 ) = (1 \u2212 m\n100 )2 1\n33.835 p0 + 2( m\n100 )(1 \u2212 m\n100 ) 1\n33.33 p1 + ( m\n100 )2 1\n33.835 p2. (7.14)\nFor example, if it turned out that p0 = 3\n4 , p1 = 3\n16 and p2 = 1", "metadata": {"page": 48}}, {"page_content": "100 )2 1\n33.835 p2. (7.14)\nFor example, if it turned out that p0 = 3\n4 , p1 = 3\n16 and p2 = 1\n16 , then the Bayesian guess for a probability function\nwould be\nPBayes( m\n100 ) = (1 \u2212 m\n100 )2 1\n33.835\n3\n4 + 2( m\n100 )(1 \u2212 m\n100 ) 1\n33.33\n3\n16 + ( m\n100 )2 1\n33.835\n1\n16 .\nAssume that you calculate PBayes in a given situation. You must then ask: What in the world does it tell me about my", "metadata": {"page": 48}}, {"page_content": "set of possible models? It gives a probability function on this set, but so what. There are lots of probability functions\non any given set. The fact is that before you use PBayes (or any of other version of P), you need to think long and hard\nabout whether it is really telling you anything useful.\nBy the way, the formula in (7.14) for PBayes is another disguised version of what we are doing in linear algebra (as is", "metadata": {"page": 48}}, {"page_content": "the formula in (7.12)). To unmask this underlying linear algebra, agree to change to the linear algebra book\u2019s notation\nand so use x0 to denote p0, use x1 to denote p1 and x2 to denote p2. Then use ym to denote P( m\n100 ) and use am1 to\ndenote the number (1 \u2212 m\n100 )2, use am2 to denote 2( m\n100 )(1 \u2212 m\n100 ) and am3 to denote ( m\n100 )2. This done, then the 101\nversions of (7.14) read\ny0 = a00x0 + a01x1 + a02x2\ny1 = a10 x0 + a11 x1 + a12x2\n\u00b7\u00b7\u00b7 etc.\n7.4 Gregor Mendel\u2019s peas", "metadata": {"page": 48}}, {"page_content": "y0 = a00x0 + a01x1 + a02x2\ny1 = a10 x0 + a11 x1 + a12x2\n\u00b7\u00b7\u00b7 etc.\n7.4 Gregor Mendel\u2019s peas\nWhat follows gives another example of how the statistical inverse problem can arise. This involves a classic experiment\ndone by Gregor Mendel. For those unfamiliar with the name, here is part of what Wikipedia has to say about Mendel\n(http://en.wikipedia.org/wiki/Mendel):\n7.3. An example 45", "metadata": {"page": 48}}, {"page_content": "Gregor Johann Mendel (July 20[1], 1822 \u2013 January 6, 1884) was an Augustinian abbot who is often called\nthe \u201cfather of modern genetics\u201d for his study of the inheritance of traits in pea plants. Mendel showed\nthat the inheritance of traits follows particular laws, which were later named after him. The signi\ufb01cance\nof Mendel\u2019s work was not recognized until the turn of the 20th century. Its rediscovery prompted the\nfoundation of genetics.", "metadata": {"page": 49}}, {"page_content": "foundation of genetics.\nWhat follows describes the experiment: A given pea plant, when self-pollinated, can have either round or angular\nseeds. It can also have either yellow or green seeds. Mendel took10 plants, where each plant can have either round or\nangular seeds, and either yellow or green seeds. Each plant was self-pollinated, and Mendel kept track of the number\nof seeds that were round and the number that were angular. He also kept track of the number that were yellow and the", "metadata": {"page": 49}}, {"page_content": "number that were green. Here are the published results (from http://www.mendelweb.org/Mendel.html):\nExperiment 1 Experiment 2\n(Shape) (Color)\nPlant # Round Angular Yellow Green\n1 45 12 25 11\n2 27 8 32 7\n3 24 7 14 5\n4 19 10 70 27\n5 32 11 24 13\n6 26 6 20 6\n7 88 24 32 13\n8 22 10 44 9\n9 28 6 50 14\n10 25 7 44 18\nTotals 336 101 355 123\nUsing the totals at the bottom \ufb01nds that round seeds appear 77% of the time. Meanwhile, yellow seeds appear 74%", "metadata": {"page": 49}}, {"page_content": "of the time. Let us agree to use pr = 0.77 for the experimentally determined probability for a seed to be round, and\npy = 0.74 for the experimentally determined probability for a seed to be yellow.\nThe conventional wisdom explains the numbers of round and angular seeds by postulating that each plant has two\ndifferent genes that convey instructions for seed shape. These are denoted here by \u2018R\u2019 (for round) and \u2018a\u2019 (for angular).", "metadata": {"page": 49}}, {"page_content": "The parent in each case is hypothesized to be type Ra. The offspring inherits either type RR, Ra or aa, where one of\nthe genes, either R or a, comes from the pollen and the other from the ovule. The hypothesis is that R is dominant so\ntypes RR and Ra come from round seeds. Meanwhile, offspring of type aa come from angular seeds.\nThere is a similar story for the color of the seed. Each plant also has two genes to control seed color. These are denoted", "metadata": {"page": 49}}, {"page_content": "here by \u2018Y\u2019 and \u2018g\u2019. The parent is type Yg, and the offspring can be of type YY , Yg or gg. The Y is postulated to be\nthe dominant of the two genes, so offspring of type YY or Yg have yellow seeds, while those of type gg have round\nseeds.\nThe conventional wisdom assigns probability 1\n2 for any given pollen grain or ovule to receive the dominant gene for\nshape, and likewise probability 1\n2 to receive the dominant gene for color. One should make it a habit to question", "metadata": {"page": 49}}, {"page_content": "2 to receive the dominant gene for color. One should make it a habit to question\nconventional wisdom! In particular, suppose that we wish to compare the conventional probability model with another\nmodel, this giving some probability, \u03b8, for any given pollen grain or ovule to have the dominant allele. To keep things\nsimple, I will again assume that \u03b8can be any number {m\n100 }m\u2208{0,1,...,100}. In this case, our set of models is again the\n101 element set \u0398 = {m\n100 }m\u2208{0,1,...100}.", "metadata": {"page": 49}}, {"page_content": "101 element set \u0398 = {m\n100 }m\u2208{0,1,...100}.\n7.5 Another candidate for P(\u03b8): A maximum likelihood candidate.\nWhat follows describes a simplistic thing that can be done with the data so as to determine a probability function on\n\u0398. Look again at Mendel\u2019s data. In a sense, 915 different experiments are represented in this table, one for each\n46 Chapter 7. The statistical inverse problem", "metadata": {"page": 49}}, {"page_content": "seedling with regards to round versus angular seed, and one from each seed with regards to yellow versus green color.\nTo see how to make sense of this, consider a sample space S with 2915 elements, where each element is a vector in\nR915 whose entries are either 0 or 1. Each entry represents one of the 915 possible outcomes for the round/angular\nand yellow/green traits. The value 0 occurs in any given entry if the recessive trait is present, and the value 1 occurs", "metadata": {"page": 50}}, {"page_content": "if the dominant trait is present. Each \u03b8 \u2208{ m\n100 }m\u2208{0,...100}gives us a probability function on this sample space S.\nFor example, the probability that there all 915 cases have the recessive trait is ((1 \u2212\u03b8)2)915, and the probability that\nall have the dominant trait is (2\u03b8\u2212\u03b82)915. Thus, when \u03b8 = 1\n2 , the vector in S with all 915 entries equal to 0 has\nprobability (1\n4 )915, and the vector in S with all 915 entries equal to 1 has probability (3\n4 )915. Here is how to think", "metadata": {"page": 50}}, {"page_content": "4 )915. Here is how to think\nof this probability function: Imagine \ufb02ipping a coin 915 times keeping track of the number of heads and the number\nof tails; here the coin is biased so that a head appears with probability (2\u03b8\u2212\u03b82) and a tail appears with probability\n(1 \u2212\u03b8)2. You are asked in a homework problem how to compute the probability of any given element inS.\nConsider next the following random variable on S: Suppose that \u02c6a = (a1,...,a 915) \u2208S. Set f(\u02c6a) to be the sum of", "metadata": {"page": 50}}, {"page_content": "the entries. Thus, f(\u02c6a) = a1 + a2 + \u00b7\u00b7\u00b7 + a915. This is the total number of dominant traits that appear if the vector \u02c6a\nrepresents the outcome of the 915 experiments. For example, a vector in Sthat gives Mendel\u2019s data, the value off is\n691.\nNow, as remarked above, each\u03b8from \u0398 determines a probability function on S. As described in the previous chapter,\nsuch a probability function on S can be used to obtain a probability function on the set of possible values for the", "metadata": {"page": 50}}, {"page_content": "random variable f. This set is the set of integers starting at 0 and ending at 915. Let me use P f (k|\u03b8) to denote the\nprobability function on the set {0,1,..., 915}that is determined by \u03b8. Said differently, if k \u2208{0,1,..., 915}, then\nPf (k|\u03b8) is the probability as determined by \u03b8of the set of elements in Swhose entries sum to k. This can be viewed\nas a sort of conditional probability for f to equal kgiven that \u03b8is the parameter used to determine the probabilities on", "metadata": {"page": 50}}, {"page_content": "S. This is why I use the notation P f (k|\u03b8). Upcoming chapters will give us the tools to compute P f (k|\u03b8). For now,\njust imagine this to be some function on the set {0,1,..., 915}whose values are non-negative and sum up to 1.\nIf, for a given \u03b8, the probability Pf (691 |\u03b8) is small relative to other values of \u03b8, then this particular value of \u03b8should\nhave small probability of being correct. If, for a given \u03b8, the probability P f (691 |\u03b8) is large relative to others, then", "metadata": {"page": 50}}, {"page_content": "I might expect that this particular \u03b8 should have greater probability of being true. These observations motivate the\nintroduction of the following probability function on the set \u0398:\nPML(\u03b8) = Pf (691 |\u03b8) /Z where Z =\n\u2211\nm=0,1,2,...100\nPf\n(\n691 | m\n100\n)\n. (7.15)\nNote that the factor of Z = Pf (691 |0) + Pf\n(\n691 | 1\n100\n)\n+ \u00b7\u00b7\u00b7 + Pf (691 |1) is necessary so as to guarantee that\nthe sum of the values of P ML over all 101 elements in \u0398 is equal to 1. What is written in (7.15) is called by some a", "metadata": {"page": 50}}, {"page_content": "maximum likelihood probability function on the set \u0398.\nOf course, this is all pretty abstract if Pf (691 |\u03b8) can\u2019t be computed. As we shall see in an upcoming chapter, it is, in\nfact, computable:\nPf (691 |\u03b8) = 915 \u00d7914 \u00d7\u00b7\u00b7\u00b7\u00d7 1\n(224 \u00d7223 \u00d7\u00b7\u00b7\u00b7\u00d7 1) (691\u00d7690 \u00d7\u00b7\u00b7\u00b7\u00d7 1)(2\u03b8\u2212\u03b82)691((1 \u2212\u03b8)2)224. (7.16)\nIn any event, this probability function assigns the greatest probability to the model whose version of P f (691 |\u03b8) is", "metadata": {"page": 50}}, {"page_content": "largest amongst all models under consideration. This is to say that the model that PML makes most probable is the one\nthat gives the largest probability to the number 691.\nTo see this approach in a simpler case, suppose that instead of 915 seedlings, I only had 4, and suppose that three of\nthe four exhibited the dominant trait, and one of the four exhibited the recessive trait.\nMy sample spaceSfor this simple example consists of 24 = 16 elements, these\n\u2022(0,0,0,0)", "metadata": {"page": 50}}, {"page_content": "My sample spaceSfor this simple example consists of 24 = 16 elements, these\n\u2022(0,0,0,0)\n\u2022(1,0,0,0),(0,1,0,0),(0,0,1,0),(0,0,0,1)\n\u2022(1,1,0,0),(1,0,1,0),(1,0,0,1),(0,1,1,0),(0,1,0,1),(0,0,1,1) (7.17)\n\u2022(1,1,1,0),(1,1,0,1),(1,0,1,1),(0,1,1,1)\n\u2022(1,1,1,1).\n7.5. Another candidate for P(\u03b8): A maximum likelihood candidate. 47", "metadata": {"page": 50}}, {"page_content": "To simplify the notation, set \u03b1 = (1 \u2212\u03b8)2. This is the probability of seeing the recessive trait for the model where\n\u03b8 \u2208{ m\n100 }m\u2208{0,1,...,100} gives the probability that any given pollen grain or ovule has the dominant allele. Thus,\n(1 \u2212\u03b1) is the probability of seeing the dominant trait given the value of \u03b8. For example, \u03b1= 1\n4 when \u03b8 = 1\n2 . In any\nevent with \u03b8given, then the probability function on Sthat is determined by \u03b8gives the following probabilities:\nP = \u03b14 to (0,0,0,0)", "metadata": {"page": 51}}, {"page_content": "P = \u03b14 to (0,0,0,0)\nP = \u03b13(1 \u2212\u03b1) to everything in the second row of (7.17)\nP = \u03b12(1 \u2212\u03b1)2 to everything in the third row of (7.17) (7.18)\nP = \u03b1(1 \u2212\u03b1)3 to everything in the fourth row of (7.17)\nP = (1 \u2212\u03b1)4 to (1,1,1,1).\n(You are asked to justify (7.18) in a homework problem.)\nIn this baby example, the possible values of f are {0,1,2,3,4}, and the set of elements in Sthat give the value f = k\nconsists of the kth row in (7.17). This being the case, then", "metadata": {"page": 51}}, {"page_content": "consists of the kth row in (7.17). This being the case, then\nPf (0 |\u03b8) = \u03b14, Pf (1 |\u03b8) = 4\u03b13(1 \u2212\u03b1), Pf (2 |\u03b8) = 6\u03b12(1 \u2212\u03b1)2,\nPf (3 |\u03b8) = 4\u03b1(1 \u2212\u03b1)3, Pf (2 |\u03b8) = (1 \u2212\u03b1)4.\nIn particular, Pf (3 |\u03b8) = 4\u03b1(1 \u2212\u03b1)3. In terms of \u03b8, this says that Pf (3 |\u03b8) = 4(1 \u2212\u03b8)2(2\u03b8\u2212\u03b82)3.\nThe analog of (7.10) for this example sets\nPML(\u03b8) = 1\n20.3175 4(1 \u2212\u03b8)2(2\u03b8\u2212\u03b82)3. (7.19)\nIn this case Z = 20.3175. I\u2019ll leave it as an exercise for those who remember their one variable calculus to verify that", "metadata": {"page": 51}}, {"page_content": "the function \u03b8\u2192P(\u03b8) has its maximum at \u03b8= 1\n2 .\nAs with the Bayesian probability function, you must still ask yourself whether the assigned probabilities to the models\nare useful or not. For example, the probability function in (7.19) \ufb01nds PML(1\n2 ) \u22480.02. Meanwhile, PML(2\n3 ) \u22480.015.\nIs it really the case that the probabilities for \u03b8 = 1\n2 and \u03b8 = 2\n3 should be so nearly equal? The fact is that these", "metadata": {"page": 51}}, {"page_content": "2 and \u03b8 = 2\n3 should be so nearly equal? The fact is that these\nprobabilities are close is due to the fact that the data from 4 seedlings is not nearly enough to distinguish these two\nmodels. This is what I meant when I said at the outset that you must think hard about whether any probability function\non your set of possible models is worth looking at. The fact that these two probabilities are so close really says nothing", "metadata": {"page": 51}}, {"page_content": "about peas and genetics, and everything about the fact that you don\u2019t have enough data to discriminate.\nThis gets to the heart of the matter with regards to using statistics: A good deal of common sense must be used to\ninterpret the mathematics.\n7.6 What to remember from this chapter\nThis has been somewhat of a rambling chapter, so I thought to tell you what are the important things to keep in mind:\n\u2022A fundamental purpose of statistics is to provide tools that allow exper-", "metadata": {"page": 51}}, {"page_content": "\u2022A fundamental purpose of statistics is to provide tools that allow exper-\nimental data to assign probabilities to various proposed models for pre-\ndicting the outcomes of the experiment.\n\u2022There are many such tools. (7.20)\n\u2022Depending on the circumstances, some of these tools will be more use-\nful than others. In particular, some techniques can give misleading or\nnonsensical conclusions.", "metadata": {"page": 51}}, {"page_content": "ful than others. In particular, some techniques can give misleading or\nnonsensical conclusions.\nLater chapters introduce other methods of assessing whether a given experimental model is likely to be valid. However,\nin using any such technique, it is important to keep your wits about you \u2013 use common sense and your understanding\nof the experimental protocol to decide whether a given statistical technique will be useful or not. Always remember:\n48 Chapter 7. The statistical inverse problem", "metadata": {"page": 51}}, {"page_content": "Mathematics can\u2019t turn processed pig feed into gold.\nThe point is that the your data may not be suf\ufb01ciently powerful to distinguish various models; and if this is the case,\nthen no amount of fancy mathematics or statistics is going to provide anything useful.\n7.7 Exercises:\n1. Suppose that the probability that any given pollen grain or ovule has the recessive allele is some number \u03b1 \u2208\n[0,1], and so the probability that either has the dominant gene is (1 \u2212\u03b1).", "metadata": {"page": 52}}, {"page_content": "[0,1], and so the probability that either has the dominant gene is (1 \u2212\u03b1).\n(a) Consider an experiment where 4 seedlings are examined for the dominant or recessive trait. Thus, the\noutcome of such an experiment is a vector in R4 whose kth entry is 0 if the kth seedling has the recessive\ntrait and it is 1 if the kth seedling has the dominant trait. Let Sdenote the 24 = 16 element set of possible\nexperimental outcomes. Let s\u2208Sdenote an element with some m\u2208{0,..., 4}entries equal to 1 and the", "metadata": {"page": 52}}, {"page_content": "experimental outcomes. Let s\u2208Sdenote an element with some m\u2208{0,..., 4}entries equal to 1 and the\nremaining entries equal to zero. Explain why shas probability (1 \u2212\u03b1)m\u03b14\u2212m.\n(b) Consider now the analogous experiment where 915 seedlings are examined for the dominant or recessive\ntrait. Thus, the outcome of such an experiment is a vector in R915 whose kth entry is 0 if the kth seedling\nhas the recessive trait and it is 1 if the kth seedling has the dominant trait. Let S denote the 2915 element", "metadata": {"page": 52}}, {"page_content": "set of possible experimental outcomes. Let s\u2208Sdenote an element with some m\u2208{0,..., 915}entries\nequal to 1 and the remaining entries equal to zero. Explain why the mhas probability (1 \u2212\u03b1)m\u03b1915\u2212m.\n(c) In the case of part (a) above, de\ufb01ne the random variable f : S \u2192{0, 1,2,3,4}where f(s) is the sum of\nthe entries of s. Compute the induced probability function Pf on the set {0,1,2,3,4}.\n7.7. Exercises 49", "metadata": {"page": 52}}, {"page_content": "CHAPTER\nEIGHT\nKernel and image in biology\nImagine a complicated cellular process that involves some ngenes that are presumably \u2018turned on\u2019 by someN other\ngenes that act at an earlier time. We want to test whether the effect of the early genes on the late genes involves a\ncomplicated synergy, or whether their affects simply add. To do this, I could try to derive the consequences of one", "metadata": {"page": 53}}, {"page_content": "or the other of these possibilities and then devise an experiment to see if the predicted consequences arise. Such an\nexperiment could vary the expression level of the early genes from their normal levels (either+ or \u2212) and see how the\nvariation in the level of expression of the late genes from their normal levels changes accordingly.\nTo elaborate on this strategy, note that when a gene is expressed, its genetic code (a stretch of the DNA molecule) is", "metadata": {"page": 53}}, {"page_content": "used to code for a molecule much like DNA called mRNA. Here, the \u2018m\u2019 stands for \u2018messenger\u2019 and the RNA part is\na sequence of small molecules strung end to end. Any of these can be one of four and the resulting sequence along the\nRNA string is determined by the original sequence on the coding stretch of DNA. This messenger RNA subsequently\nattaches to the protein making part of a cell (a \u2018ribosome\u2019) where its sequence is used to construct a particular protein", "metadata": {"page": 53}}, {"page_content": "molecule. In any event, the level of any given mRNA can be measured at any given time, and this level serves as a\nproxy for the level of expression of the gene that coded it in the \ufb01rst place.\nOne more thing to note: The level of expression of a gene can often be varied with some accuracy in an experiment by\ninserting into the cell nucleus certain tailored molecules to either promote or repress the gene expression. Such is the\nmagic of modern biotechnology.", "metadata": {"page": 53}}, {"page_content": "magic of modern biotechnology.\nTo make a prediction that is testable by measuring early and late gene expression, let us suppose that the affects of\nthe early genes are simply additive and see where this assumption leads. For this purpose, label these early genes\nby integers from1 to N, and let uk to denote the deviation, either positive or negative, of the level of expression of\nthe kth early gene from its normal level. For example, we can take uk to denote the deviation from normal of the", "metadata": {"page": 53}}, {"page_content": "concentration of the mRNA that comes from the kth early gene.\nMeanwhile, label the late genes by integers from 1 to n, and use pj to denote the deviation of the latter\u2019s mRNA from\nits normal level. If the affects of the early genes on the late genes are simply additive, we might expect that any given\npj has the form\npj = Aj1u1 + Aj2u2 + \u00b7\u00b7\u00b7 + AjNuN, (8.1)\nwhere each Ajk is a constant. This is to say that the level pj is a sum of factors, the \ufb01rst proportional to the amount", "metadata": {"page": 53}}, {"page_content": "of the \ufb01rst early gene, the second proportional to the amount of the second early gene, and so on. Note that when\nAjk is positive, then the kth early gene tends to promote the expression of the jth late gene. Conversely, when Ajk is\nnegative, the kth early gene acts to repress the expression of the jth late gene.\nIf we use \u20d7 uto denote the N-component column vector whose kth entry is uk, and if we use \u20d7 pto denote the n-", "metadata": {"page": 53}}, {"page_content": "component column vector whose jth component is pj, then the equation in (8.1) is the matrix equation\u20d7 p= A\u20d7 u. Thus,\nwe see a linear transformation from an N-dimensional space to an n-dimensional one.\nBy the way, note that the relation predicted by (8.1) can, in principle, be tested by experiments that vary the levels of\nthe early genes and see if the levels of the late genes change in a manner that is consistent with (8.1). Such experiments", "metadata": {"page": 53}}, {"page_content": "will also determine the values for the matrix entries{Ajk}. For example, to \ufb01nd A11, vary u1 while keeping all k> 1\nversions of uk equal to zero. Measure p1 as these variations are made and see if the ratio p1/u1 is constant as u1\nchanges with each k >1 version of uk equal zero. If so, the constant is the value to take for A11. If this ratio is not\n50", "metadata": {"page": 53}}, {"page_content": "constant as these variations are made, then the linear model is wrong. One can do similar things with the other pj and\nuk to determine all Ajk. One can then see about changing more than one uk from zero and see if the result conforms\nto (8.1).\nThe question now arises as to the meaning of the kernel and the image of the linear transformationAfrom RN to Rn.\nTo make things explicit here, suppose that nand N are both equal to 3 and that Ais the matrix\nA=\n\uf8ee\n\uf8f0\n1 1 2\n2 1 1\n1 0\u22121\n\uf8f9\n\uf8fb (8.2)", "metadata": {"page": 54}}, {"page_content": "A=\n\uf8ee\n\uf8f0\n1 1 2\n2 1 1\n1 0\u22121\n\uf8f9\n\uf8fb (8.2)\nAs you can check, this matrix has kernel equal to the scalar multiples of the vector\n\uf8ee\n\uf8f0\n1\n\u22123\n1\n\uf8f9\n\uf8fb (8.3)\nMeanwhile, its image is the span of the vectors\n\uf8ee\n\uf8f0\n0\n1\n1\n\uf8f9\n\uf8fb and\n\uf8ee\n\uf8f0\n1\n1\n0\n\uf8f9\n\uf8fb. (8.4)\nThus, it consists of all vectors inR3 that can be written as a constant times the \ufb01rst vector in (8.4) plus another constant\ntimes the second.\nHere is the meaning of the kernel: Vary the early genes 1, 2 and 3 from their normal levels in the ratio u1/u3 = 1", "metadata": {"page": 54}}, {"page_content": "and u2/u3 = \u22123 and there is no affect on the late genes. This is to say that if the expression levels of early genes 1\nand 3 are increased by any given amount r while that of early gene 2 is decreased by 3r, then there is no change to\nthe levels of expression of the three late genes. In a sense, the decrease in the level by a factor of3 of the second early\ngene exactly offsets the affect of increasing the equal increases in the levels of the \ufb01rst and third early genes.", "metadata": {"page": 54}}, {"page_content": "As to the meaning of the image, what we \ufb01nd is that only certain deviations of the levels of expression of the three late\ngenes from their background values can be obtained by modifying the expression levels of the three early genes. For\nexample, both vectors in (8.4) are orthogonal to the vector\n\uf8ee\n\uf8f0\n1\n\u22121\n1\n\uf8f9\n\uf8fb. (8.5)\nThus, values ofp1, p2 and p3 with the property thatp1 +p3 \u0338= p2 can not be obtained by any variation in the expression", "metadata": {"page": 54}}, {"page_content": "levels of the three early genes. Indeed, the dot product of the vector \u20d7 pwith the vector in (8.5) is p1 \u2212p2 + p3 and this\nmust be zero in the case that is a linear combination of the vectors in (8.4).\nGranted that the matrix A is that in (8.2), then the preceding observation has the following consequence for the\nbiologist: If values of p1, p2 and p3 are observed in a cell with p1 + p3 \u0338= p2, then the three early genes can not be the", "metadata": {"page": 54}}, {"page_content": "sole causative agent for the expression levels of the three late genes.\n51", "metadata": {"page": 54}}, {"page_content": "CHAPTER\nNINE\nDimensions and coordinates in a scienti\ufb01c\ncontext\nMy purpose here is to give some toy examples where the notion of dimension and coordinates appear in a biological\ncontext.\n9.1 Coordinates\nHere is a hypothetical situation: Suppose that a cell has genes labeled{1,2,3}. The level of the corresponding product\nthen de\ufb01nes vector in R3 where the obvious coordinates, x1, x2 and x3, measure the respective levels of the products", "metadata": {"page": 55}}, {"page_content": "of gene 1, gene 2 and gene 3. However, this might not be the most useful coordinate system. In particular, if some\nsubsets of genes are often turned on at the same time and in the same amounts, it might be better to change to a basis\nwhere that subset gives one of the basis vectors. Suppose for the sake of argument, that it is usually the case that the\nlevel of the product from gene2 is three times that of gene 1, while the level of the product of gene 3 is half that of", "metadata": {"page": 55}}, {"page_content": "gene 1. This is to say that one usually \ufb01nds x2 = 3x1 and x3 = x1. Then it might make sense to switch from the\nstandard coordinate bases,\n\u20d7 e1 =\n\uf8ee\n\uf8f0\n1\n0\n0\n\uf8f9\n\uf8fb, \u20d7 e 2 =\n\uf8ee\n\uf8f0\n0\n1\n0\n\uf8f9\n\uf8fb, \u20d7 e 3 =\n\uf8ee\n\uf8f0\n0\n0\n1\n\uf8f9\n\uf8fb, (9.1)\nto the coordinate system that uses a basis \u20d7 v1, \u20d7 v2 and \u20d7 v3 where\n\u20d7 v1 =\n\uf8ee\n\uf8f0\n1\n3\n1\n2\n\uf8f9\n\uf8fb, \u20d7 v 2 = \u20d7 e2, \u20d7 v 3 = \u20d7 e3. (9.2)\nTo explain, suppose I measure some values for x1, x2 and x3. This then gives a vector,\n\u20d7 x=\n\uf8ee\n\uf8f0\nx1\nx2\nx3\n\uf8f9\n\uf8fb= x1\u20d7 e1 + x2\u20d7 e2 + x3\u20d7 e3. (9.3)", "metadata": {"page": 55}}, {"page_content": "\u20d7 x=\n\uf8ee\n\uf8f0\nx1\nx2\nx3\n\uf8f9\n\uf8fb= x1\u20d7 e1 + x2\u20d7 e2 + x3\u20d7 e3. (9.3)\nNow, I can also write this vector in terms of the basis in (9.2) as\n\u20d7 x= c1\u20d7 v1 + c2\u20d7 v2 + c3\u20d7 v3. (9.4)\nWith \u20d7 v1, \u20d7 v2 and \u20d7 v3 as in (9.2), the coordinates c1, c2 and c3 that appear in (9.5) are\nc1 = x1, c 2 = x2 \u22123x1, and c3 = x3 \u2212x1. (9.5)\nAs a consequence, the coordinates c2 describes the deviation of x2 from its usual value of 3x1. Meanwhile, the\ncoordinate c3 describes the deviation of x3 from its usual value of x1.\n52", "metadata": {"page": 55}}, {"page_content": "Here is another example: Suppose now that there are again three genes with the levels of their corresponding products\ndenoted as x1, x2, and x3. Now suppose that it is usually the case that these levels are correlated in thatx3 is generally\nvery close to 2x2 + x1. Any given set of measured values for these products determines now a column vector as\nin (9.3). A useful basis in this case would by one where the coordinates c1, c2 and c3 has\nc1 = x1, c 2 = x2, and c3 = x3 \u22122x2 \u2212x1. (9.6)", "metadata": {"page": 56}}, {"page_content": "c1 = x1, c 2 = x2, and c3 = x3 \u22122x2 \u2212x1. (9.6)\nThus, c3 again measures the deviation from the expected values. The basis with this property is that where\n\u20d7 v1 =\n\uf8ee\n\uf8f0\n1\n0\n1\n\uf8f9\n\uf8fb, \u20d7 v 2 =\n\uf8ee\n\uf8f0\n0\n1\n2\n\uf8f9\n\uf8fb, and \u20d7 v3 =\n\uf8ee\n\uf8f0\n0\n0\n1\n\uf8f9\n\uf8fb. (9.7)\nThis is to say that if\u20d7 v1, \u20d7 v2, and \u20d7 v3 are as depicted in (9.7), and if\u20d7 xis then expanded in this basis asc1\u20d7 v1 +c2\u20d7 v2 +c3\u20d7 v3,\nthen c1, c2 and c3 are given by (9.6).\n9.2 A systematic approach", "metadata": {"page": 56}}, {"page_content": "then c1, c2 and c3 are given by (9.6).\n9.2 A systematic approach\nIf you are asking how I know to take the basis in (9.7) to get the coordinate relations in (9.6), here is the answer:\nSuppose that you have coordinates x1, x2 and x3 and you desire new coordinates, c1, c2 and c3 that are related to the\nx\u2019s by a linear transformation: \uf8ee\n\uf8f0\nc1\nc2\nc3\n\uf8f9\n\uf8fb= A\n\uf8ee\n\uf8f0\nx1\nx2\nx3\n\uf8f9\n\uf8fb, (9.8)\nwhere Ais an invertible, 3 \u00d73 matrix. In this regard, I am supposing that you have determined already the matrix A", "metadata": {"page": 56}}, {"page_content": "and are simply looking now to \ufb01nd the vectors \u20d7 v1, \u20d7 v2 and \u20d7 v3 that allow you to write \u20d7 x= c1\u20d7 v1 + c2\u20d7 v2 + c3\u20d7 v3 with c1,\nc2 and c3 given by (10.8). As explained in the linear algebra text, the vectors to take are:\n\u20d7 v1 = A\u22121\u20d7 e1, \u20d7 v 2 = A\u22121\u20d7 e2, \u20d7 v 3 = A\u22121\u20d7 e3. (9.9)\nTo explain why (9.9) holds, take the equation\u20d7 x= c1\u20d7 v1 +c2\u20d7 v2 +c3\u20d7 v3 and act on both sides by the linear transformation", "metadata": {"page": 56}}, {"page_content": "A. According to (9.8), the left-hand side, A\u20d7 x, is the vector\u20d7 cwhose top component is c1, middle component is c2 and\nbottom component is c3. This is to say that A\u20d7 x= c1\u20d7 e1 + c2\u20d7 e2 + c3\u20d7 e3. Meanwhile, the left-hand side of the resulting\nequation is c1A\u20d7 v1 + c2A\u20d7 v2 + c3A\u20d7 v3. Thus,\nc1\u20d7 e1 + c2\u20d7 e2 + c3\u20d7 e3 = c1A\u20d7 v1 + c2A\u20d7 v2 + c3A\u20d7 v3. (9.10)\nNow, the two sides of (9.10) are supposed to be equal for all possible values of c1, c2 and c3. In particular, they are", "metadata": {"page": 56}}, {"page_content": "equal when c1 = 1 and c2 = c3 = 0. For these choices, the equality in (9.10) asserts that \u20d7 e1 = A\u20d7 v1; this the left-most\nequality in (9.9). Likewise, setting c1 = c3 = 0 and c2 = 1 in (9.10) gives the equivalent of the middle equality\nin (9.9); and setting c1 = c2 = 0 and c3 = 1 in (9.10) gives the equivalent of the right-most equality in (9.9).\n9.3 Dimensions\nWhat follows is an example of how the notion of dimension arises in a scienti\ufb01c context. Consider the situation in", "metadata": {"page": 56}}, {"page_content": "Section 9.1, above, where the system is such that the levels of x2 and x3 are very nearly x2 \u22483x1 and x3 \u2248x1. This\nis to say that when we use the coordinate c1, c2 and c3 in (9.5), then |c2|and |c3|are typically very small. In this case,\na reasonably accurate model for the behavior of the three gene system can be had by simply assuming that c2 and\nc3 are always measured to be identically zero. As such, the value of the coordinate c1 describes the system to great", "metadata": {"page": 56}}, {"page_content": "accuracy. Since only one coordinate is needed to describe the system, it is said to be \u20181-dimensional\u2019.\nA second example is the system that is described by c1, c2 and c3 as depicted in (9.6). If it is always the case that x3\nis very close to 2x2 + x1, then the system can be described with good accuracy with c3 set equal to zero. This done,\n9.2. A systematic approach 53", "metadata": {"page": 56}}, {"page_content": "then one need only specify the values of c1 and c2 to describe the system. As there are two coordinates needed, this\nsystem would be deemed \u20182-dimensional\u2019.\nIn general, some sort of time dependent phenomena is deemed \u2018n-dimensional\u2019 when ncoordinates are required to\ndescribe the behavior to some acceptable level of accuracy. Of course, it is typically the case that the value of n\ndepends on the desired level of accuracy.\n9.4 Exercises:", "metadata": {"page": 57}}, {"page_content": "depends on the desired level of accuracy.\n9.4 Exercises:\n1. Suppose that four genes have corresponding products with levels x1, x2, x3 and x4 where x4 is always very\nclose to x1 + 4x2 while x3 is always very close to 2x1 + x2. Find a new set of basis vectors for R4 and\ncorresponding coordinates c1, c2, c3 and c4 with the following property: The values of x1, x2, x3 and x4 for\nthis four gene system are the points in the (c1,c2,c3,c4) coordinate system where c3 and c4 are nearly zero.", "metadata": {"page": 57}}, {"page_content": "2. Suppose that two genes are either \u2018on\u2019 of \u2018off\u2019, so that there are affectively, just four states for the two gene\nsystem, {++,+\u2212,\u2212+,\u2212\u2212}, where ++ means that both genes are on; +\u2212means that the \ufb01rst is on and the\nsecond is off; etc. Assume that these four states have respective probabilities 1\n2 , 1\n6 , 1\n6 , 1\n6 .\n(a) Is the event that the \ufb01rst gene is on independent from the event that the second gene is on?", "metadata": {"page": 57}}, {"page_content": "(a) Is the event that the \ufb01rst gene is on independent from the event that the second gene is on?\nNow suppose that these two genes jointly in\ufb02uence the levels of two different products. The levels of the \ufb01rst\nproduct are given by {3,2,1,0}in the respective states ++, +\u2212, \u2212+, \u2212\u2212. The levels of the second are\n{4,2,3,1}in these same states.\n(b) View the levels of the two products as random variables on the sample space S that consists of", "metadata": {"page": 57}}, {"page_content": "(b) View the levels of the two products as random variables on the sample space S that consists of\n{++,+\u2212,\u2212+,\u2212\u2212}with the probabilities as stated. Write down the mean and standard deviations for\nthese two random variables.\n(c) Compute the correlation matrix in equation (6.8) of Chapter 6 for these two random variables to prove that\nthey are not independent.\n54 Chapter 9. Dimensions and coordinates in a scienti\ufb01c context", "metadata": {"page": 57}}, {"page_content": "CHAPTER\nTEN\nMore about Bayesian statistics\nThe term \u2018Bayesian statistics\u2019 has different meanings for different people. Roughly, Bayesian statistics reverses\n\u2018causes\u2019 and \u2018effects\u2019 so as to make an educated guess about the causes given the known effects. The goal is de-\nduce a probability function on the set of possible causes granted that we have the probabilities of the various effects.", "metadata": {"page": 58}}, {"page_content": "Take note that I have underlined the words \u2018educated guess\u2019. There are situations when the Bayesian strategy seems\nreasonable, and others where it doesn\u2019t.\n10.1 A problem for Bayesians\nThere is a sample space of interest, S, with a known function (i.e. random variables) f to another \ufb01nite set, W. A\nprobability function for the set W is in hand, but what is needed is one for the set S.", "metadata": {"page": 58}}, {"page_content": "probability function for the set W is in hand, but what is needed is one for the set S.\nHere is a simple situation that exempli\ufb01es this: Flip two distinct coins, coin #1 and coin #2. Move to the right one step\n(x\u2192x+ 1) for each heads that appears and to the left one step (x \u2192x\u22121) for each tails. Let W denote the set of\npossible positions after two \ufb02ips, thus W = {\u22122,0,2}. Meanwhile, the sample space is S = {HH,HT,TH,TT }.", "metadata": {"page": 58}}, {"page_content": "We can do this experiment many times and so generate numbers Pexp(\u22122), Pexp(0) and Pexp(2) that give the respective\nfrequencies that \u22122, 0 and 2 are the resulting positions. How can we use these frequencies to determine the probability\nof getting heads on coin #1, and also the probability of getting heads on coin #2? In this regard, we don\u2019t want to\nassume that these coins are fair, nor do we want to assume that the probability of heads for coin #1 is the same as that\nfor coin #2.", "metadata": {"page": 58}}, {"page_content": "for coin #2.\n10.2 A second problem\nA six-sided die, hidden from view, is rolled twice and the resulting pair of numbers (each either1, 2, ..., 6) are added\nto obtain a single number, thus an integer that can be as small as 2 or as large as 12. We are told what this sum is, but\nnot the two integers that appeared. If this is done many times, how can the relative frequencies for the various values\nof the sum be used to determine a probability function for the sample space?", "metadata": {"page": 58}}, {"page_content": "of the sum be used to determine a probability function for the sample space?\n10.3 Meet the typical Bayesian\nTo set the stage, remember that if P is any given probability function on S, then P induces one on W by the rule we\nsaw in Chapter 5. Indeed, if the latter is denoted by Pf, the rule is that Pf(r) is the probability as measured by P of the\nsubset of points in Swhere f has value r. Thus,\nPf(r) =\n\u2211\ns\u2208S with f(s)=r\nP(s). (10.1)\n55", "metadata": {"page": 58}}, {"page_content": "This last equation can be written in terms of conditional probabilities as follows:\nPf(r) =\n\u2211\ns\u2208S\nP (r|s) P(s) (10.2)\nwhere P (r|s) is the conditional probability that f = rgiven that you are at the point s\u2208S. Of course, this just says\nthat P (r|s) is 1 if f(s) = rand 0 otherwise.\nThe problem faced by statisticians is to deduce P, or a reasonable approximation, given only knowledge of some", "metadata": {"page": 59}}, {"page_content": "previously determined probability function, PW, on the set W. In effect, we want to \ufb01nd a probability function P on\nSwhose corresponding Pf is the known function PW.\nYour typical Bayesian will derive a guess for P using the following strategy:\nStep 1: Assume that there is some conditional probability,K(s; r), that gives the probability of obtaining any given\nsfrom S granted that the value of f is r. If such a suite of conditional probabilities were available, then one could\ntake\nPguess(s) =\n\u2211", "metadata": {"page": 59}}, {"page_content": "take\nPguess(s) =\n\u2211\nr\u2208W\nK(s; r)PW(r). (10.3)\nThe problem is that the points in W are the values of a function of the points in S, not vice-versa. Thus, there is often\nno readily available K(s; r).\nStep 2: A Bayesian is not deterred by this state of affairs. Rather, the Bayesian plows ahead by using what we have,\nwhich is P (r|s). We know its values in all cases; it is 1 when f(s) = r and zero otherwise. Why not, asks the\nBayesian, take\nK(s; r) = 1\nZ(r)P (r|s) , (10.4)", "metadata": {"page": 59}}, {"page_content": "Bayesian, take\nK(s; r) = 1\nZ(r)P (r|s) , (10.4)\nwhere Z(r) is the number of points in Son which f has value r. This, is to say that\nZ(r) =\n\u2211\ns\u2208S\nP (r|s) . (10.5)\nTo explain the appearance of Z(r), remember that a conditional probability of the form P (A|B) is a probability\nfunction in its own right on the sample space S. Thus, P (S|B) must be 1 if Sis the whole sample space. This need\nnot be the case for K(S; r) were the factor of 1/Z(r) absent.", "metadata": {"page": 59}}, {"page_content": "not be the case for K(S; r) were the factor of 1/Z(r) absent.\nStep 3: To summarize: Our typical Bayesian takes the following as a good guess for the probability function onS:\nPBayes(s) =\n\u2211\nr\u2208W\n1\nZ(r)P (r|s) PW(r). (10.6)\nNote that disentangling the de\ufb01nitions, there is really no summation involved in (10.6) because there is just one value\nof rthat makes P (r|s) non-zero for any given s, this the value r = f(s). Thus, (10.6) is a very roundabout way of\nsaying that\nPBayes(s) = 1", "metadata": {"page": 59}}, {"page_content": "saying that\nPBayes(s) = 1\nZ(f(s))PW(f(s)). (10.7)\nThis is our Bayesian\u2019s guess for the probability function onS.\n10.4 A \ufb01rst example\nConsider the problem in Part a with \ufb02ipping coin #1 and coin #2. As noted there, W denotes the set of possible\npositions after the two coin \ufb02ips, thus W = {\u22122,0,2}. The set S = {HH,HT,TH,TT }. Suppose \ufb01rst that our\n56 Chapter 10. More about Bayesian statistics", "metadata": {"page": 59}}, {"page_content": "two coins have the same probability for heads, some number q \u2208(0,1). Thus T has probability 1 \u2212q, then the true\nprobabilities for the elements in S are q2, q(1 \u2212q), q(1 \u2212q) and (1 \u2212q)2 in the order they appear above. These\nprobability assignments give P true on S. With these true probabilities, the frequencies of appearances of the three\nelements in W are (1 \u2212q)2, 2q(1 \u2212q) and q2. I\u2019ll take these for my de\ufb01nition of the probability function PW on W.", "metadata": {"page": 60}}, {"page_content": "Let\u2019s now see what the Bayesian would \ufb01nd for PBayes. For this purpose, note that the only non-zero values of P (r|s)\nthat appear in the relevant version of (9.6) are\n\u2022P(\u22122,TT ) = 1\n\u2022P(0,HT ) = P(0,TH ) = 1\n\u2022P(2,HH ) = 1\n(10.8)\nThus, Z(\u00b12) = 1 and Z(0) = 2. Plugging this into (10.7) \ufb01nds\nPBayes(HH) = q2, PBayes(TT) = (1 \u2212q)2 and P Bayes(HT) = PBayes(TH) = q(1 \u2212q). (10.9)\nThus, the Bayesian guess for probabilities is the true probability.\n10.5 A second example", "metadata": {"page": 60}}, {"page_content": "Thus, the Bayesian guess for probabilities is the true probability.\n10.5 A second example\nLet us now change the rules in the coin \ufb02ip game and consider the case where the \ufb01rst \ufb02ip uses a fair coin (probability\n1\n2 ) for either H or T, and the second uses a biased coin, with probability qfor H and thus (1 \u2212q) for T. In this case,\nthe true probability on Sis given by\nPtrue(HH) = 1\n2 q, Ptrue(HT) = 1\n2 (1 \u2212q), Ptrue(TH) = 1\n2 q, and P true(TT) = 1\n2 (1 \u2212q). (10.10)", "metadata": {"page": 60}}, {"page_content": "Ptrue(HH) = 1\n2 q, Ptrue(HT) = 1\n2 (1 \u2212q), Ptrue(TH) = 1\n2 q, and P true(TT) = 1\n2 (1 \u2212q). (10.10)\nThe frequencies of appearance of the three positions in W are now 1\n2 (1 \u2212q), 1\n2 , 1\n2 q. I use these three numbers for the\nprobabilities given by PW. As the conditional probabilities in (10.8) do not change, we can employ then in (10.6) to\n\ufb01nd the Bayesian guess:\nPBayes(HH) = 1\n2 q, PBayes(HT) = 1\n4 , PBayes(TH) = 1\n4 , and P Bayes(TT) = 1\n2 (1 \u2212q). (10.11)", "metadata": {"page": 60}}, {"page_content": "PBayes(HH) = 1\n2 q, PBayes(HT) = 1\n4 , PBayes(TH) = 1\n4 , and P Bayes(TT) = 1\n2 (1 \u2212q). (10.11)\nThus, the Bayesian guess goes bad when qdeviates from 1\n2 .\nRoughly speaking, the Bayesian guess cannot distinguish between those points in the sample space that give the same\nvalue for random variable f.\n10.6 Something traumatic\nLet me show you something that is strange about the Bayesian\u2019s guess in (10.11). Suppose we ask for the probability", "metadata": {"page": 60}}, {"page_content": "as computed by PBayes that H appears on the \ufb01rst coin. According to our rules of probability,\nPBayes(coin #1 = H) = PBayes(HH) + PBayes(HT) = 1\n2 q+ 1\n4 . (10.12)\nThis is also the probability PBayes(coin #2 = H) since PBayes(HT) = PBayes(TH). Now, note that\nPBayes(HH) \u0338= PBayes(coin #1 = H)PBayes(coin #2 = H) (10.13)\nunless q = 1\n2 since the left-hand side is 1\n2 q and the right is (1\n2 q+ 1\n4 )2. Thus, the Bayesian \ufb01nds that the event of", "metadata": {"page": 60}}, {"page_content": "2 q and the right is (1\n2 q+ 1\n4 )2. Thus, the Bayesian \ufb01nds that the event of\ncoin #1 = H is not independent of the event that coin #2 = H!! (Remember that events A and B are deemed\nindependent when P(A\u2229B) = P(A)P(B).)\n10.5. A second example 57", "metadata": {"page": 60}}, {"page_content": "10.7 Rolling dice\nConsider here the case where the die is rolled twice and the resulting two integers are added. The sample space, S,\nconsists of the 36 pairs of the form (a,b) where aand bare integers from the set {1,..., 6}. The random variable\n(a.k.a. function of S) is the function that assigns a+ bto any given (a,b) \u2208S. Thus, the set of possible outcomes in\nW = {2,..., 12}.\nSuppose, for the sake of argument, that the true probabilities for rolling 1,2,..., 6 on any given throw of the die are", "metadata": {"page": 61}}, {"page_content": "1\n21 , 2\n21 , 3\n21 , 4\n21 , 5\n21 , 6\n21 . Were this the case, then the true probability, Ptrue, for any given pair (a,b) in Sis\nPtrue(a,b) = a\n21 \u00b7 b\n21 = ab\n441 . (10.14)\nIf the die has these probabilities, then the probabilities that result for the outcomes are\nPW(2) = 1\n441 , PW(3) = 4\n441 , PW(4) = 10\n441 , PW(5) = 20\n441 ,\nPW(6) = 35\n441 , PW(7) = 56\n441 , PW(8) = 70\n441 , PW(9) = 76\n441 , (10.15)\nPW(10) = 73\n441 , PW(11) = 60\n441 , PW(12) = 36\n441 .", "metadata": {"page": 61}}, {"page_content": "441 , PW(9) = 76\n441 , (10.15)\nPW(10) = 73\n441 , PW(11) = 60\n441 , PW(12) = 36\n441 .\nAs indicated by my notation, I am using the preceding probabilities for PW.\nNow, given that we have P W as just described, here is what the Bayesian \ufb01nds for the probabilities of some of the\nelements in the sample space S:\nPBayes(1,1) = 1\n441\nPBayes(2,1) = PBayes(1,2) = 2\n441\nPBayes(3,1) = PBayes(2,2) = PBayes(1,3) = 1\n3\n10\n441\nPBayes(4,1) = PBayes(3,2) = PBayes(2,3) = PBayes(1,4) = 5\n441 (10.16)", "metadata": {"page": 61}}, {"page_content": "3\n10\n441\nPBayes(4,1) = PBayes(3,2) = PBayes(2,3) = PBayes(1,4) = 5\n441 (10.16)\nPBayes(5,1) = PBayes(4,2) = PBayes(3,3) = PBayes(2,4) = PBayes(1,5) = 7\n441\nPBayes(6,1) = PBayes(5,2) = PBayes(4,3) = PBayes(3,4) = PBayes(2,5) = PBayes(1,6) = 1\n3\n28\n441\nPBayes(6,2) = PBayes(5,3) = PBayes(4,4) = PBayes(3,5) = PBayes(2,6) = 14\n441\nand so on.\n10.8 Exercises:\n1. (a) Complete the table in (10.16) by computing the values of P Bayes on the remaining pairs in S.", "metadata": {"page": 61}}, {"page_content": "(b) According to P Bayes, is the event of the \ufb01rst roll comes up 1 independent from the event that second roll\ncomes up 6? Justify your answer.\n2. Compute the mean and standard deviation for the random variable a+ b\ufb01rst using Ptrue from (10.15) and then\nusing PBayes.\n3. Consider now the same sample space for rolling a die twice, but now suppose that the die is fair, and so each\nnumber has probability of turning up on any given roll.", "metadata": {"page": 61}}, {"page_content": "number has probability of turning up on any given roll.\n(a) Compute the mean and standard deviation of the random variable a+ b.\n(b) Compute the mean and standard deviation for the random variable ab.\n(c) Are the random variables a+ band abindependent? In this regard, remember that two random variables,\nf and g, are said to be independent when P(f = r and g = s) = P(f = r)P(g = s) for all pairs (r,s)\nwhere ris a possible value of f and sis a possible value of g. Justify your answer.", "metadata": {"page": 61}}, {"page_content": "where ris a possible value of f and sis a possible value of g. Justify your answer.\n58 Chapter 10. More about Bayesian statistics", "metadata": {"page": 61}}, {"page_content": "CHAPTER\nELEVEN\nCommon probability functions\nThere are certain probability functions that serve as models that are commonly used when trying to decide if a given\nphenomena is \u2018unexpected\u2019 or not. This chapter describes those that arise most often.\n11.1 What does \u2018random\u2019 really mean?\nConsider a bacterial cell that moves one cell length, either to the right or left, in each unit of time. If we start some", "metadata": {"page": 62}}, {"page_content": "large number of these cells at x= 0 and wait tunits of time, we can determine a function, pt(x), which is the fraction\nof bacteria that end up at position x \u2208{0,\u00b11,\u00b12,... }at time t. We can now ask: What do we expect the function\nx\u2192pt(x) to look like?\nSuppose that we think that the bacteria is moving \u2018randomly\u2019. Two questions then arise:\n\u2022How do we translate our intuitive notion of the English term \u2018random\u2019 into a predic-\ntion forpt(x)?", "metadata": {"page": 62}}, {"page_content": "tion forpt(x)?\n\u2022Granted we have a prediction, for each t and x, then how far must pt(x) be from\nits predicted value before we must accept the fact that the bacteria is not moving\naccording to our preconceived notion of random?\n(11.1)\nThese questions go straight to the heart of what is called the \u2018scienti\ufb01c method\u2019. We made a hypothesis: The bacterium\nmoves left or right at random\u2019. We want to \ufb01rst generate some testable predictions of the hypothesis (the \ufb01rst point", "metadata": {"page": 62}}, {"page_content": "in (11.1)), and then compare these predictions with experiment. The second point in (11.1) asks for criterion to use to\nevaluate whether the experiment con\ufb01rms or rejects our hypothesis.\nThe \ufb01rst question in (11.1) is the provenance of \u2018probability theory\u2019 and the second the provenance of \u2018statistics\u2019. This\nchapter addresses the \ufb01rst question in (11.1), while aspects of the second are addressed in some of the subsequent\nchapters.\n11.2 A mathematical translation of the term \u2018random\u2019", "metadata": {"page": 62}}, {"page_content": "chapters.\n11.2 A mathematical translation of the term \u2018random\u2019\nTo say that an element is chosen from a given set at \u2018random\u2019 is traditionally given the following mathematical de\ufb01ni-\ntion:\nProbabilities are de\ufb01ned using the probability function that assigns all elements the same\nprobability. If the set has Lelements, then the probability of any given element appearing\nis 1\nL.\n(11.2)", "metadata": {"page": 62}}, {"page_content": "is 1\nL.\n(11.2)\nThe probability function that assigns this constant value to all elements is called the uniform probability function.\n59", "metadata": {"page": 62}}, {"page_content": "Here is an archetypal example: A coin is \ufb02ipped N times. Our sample space is the setSthat consists of the2N possible\nsequences (\u00b11,\u00b11,..., \u00b11), where +1 is in the kth spot when the kth \ufb02ip landed heads up, while \u22121 sits in this slot\nwhen the kth \ufb02ip landed tails up. If I assume that the appearance of heads on any \ufb02ip has probability , and that the\nappearance of heads on any subset of \ufb02ips has no bearing on what happens in the other \ufb02ips, then I would predict", "metadata": {"page": 63}}, {"page_content": "that the frequencies of appearance of any given sequence in S is 2\u2212N. This is to say that I would use the uniform\nprobability function on Sto predict the frequency of appearance of any subset of its elements.\nNote that after setting N = t, this same sample space describes all of the possibilities for the moves of our bacterium\nfrom Section 11.1, above.\nHere is another example: You go to a casino to watch people playing the game of \u2018craps\u2019. Remember that this game is", "metadata": {"page": 63}}, {"page_content": "played by rolling two six-sided dice, and looking at the numbers that show on the top faces when the dice stop rolling.\nThe sample space for one play of the game is the set of36 elements where each is of the form (a,b) for a, bintegers\nfrom 1 through 6. If I believe that the dice are \u2018fair\u2019 and that the appearance of any given number on one die has no\nbearing on what appears on the other, then I would use the uniform probability function on Sto predict the frequency", "metadata": {"page": 63}}, {"page_content": "of appearance of any given outcome.\nI might watch N games of craps played. In this case, the sample space is the set of 36N possible sequence of the form\n((a1,b1),..., (aN,bN)) where each ak and each bk is a number from 1 through 6. If I believe that the appearance of\nany face on the dice is as likely as any other, and if I believe that the appearance of any sequence in any given subset of", "metadata": {"page": 63}}, {"page_content": "the N games has no affect on what happens in the other games, then I would make my predictions for the frequencies\nusing the uniform probability function on this 36N element sample space.\nWhat follows is yet one more example: Look at a single stranded DNA molecule that is composed of N bases strung\nend to end. As each cite on the DNA molecule can be occupied by one of 4 bases, the sample space that describes the", "metadata": {"page": 63}}, {"page_content": "various possibilities for this DNA molecule is the 4N element set whose typical member is a sequence (\u03b81,...,\u03b8 N)\nwhere each \u03b8k is either A, C, G or T. If I believe that each such letter is equally likely to appear, and that the appearance\nof a given letter in any one slot has no bearing on what happens in the other slots, then I would use the uniform\nprobability function to predict the frequencies of occurrences of the various letters in lengthN strand of DNA.", "metadata": {"page": 63}}, {"page_content": "You might think that the uniform probability distribution is frightfully dull \u2013 after all, how much can you say about a\nconstant?\n11.3 Some standard counting solutions\nThe uniform probability distribution becomes interesting when you consider the probabilities for certain subsets,\nor the probabilities for the values of certain random variables. To motivate our interest in the uniform probability", "metadata": {"page": 63}}, {"page_content": "distribution, consider \ufb01rst its appearance in the case of bacteria. Here, our model of random behavior takes S as just\ndescribed, the set of 2N sequences of the form (\u03b11,\u03b12,...,\u03b1 N), where each \u03b1k is 1 or \u22121. Now de\ufb01ne f so that\nf(\u03b11,\u00b7\u00b7\u00b7 ,\u03b1N) = \u03b11 + \u03b12 + \u00b7\u00b7\u00b7 + \u03b1N and ask for the probabilities of the possible values of f. With regards to\nbacteria, f tells us the position of the bacteria after N steps in the model where the bacteria moves right or left with", "metadata": {"page": 63}}, {"page_content": "equal probability at each step. The probabilities for the possible values of f provide the theoretical predictions for the\nmeasured function pt(x) in the case that t = N. Thus, uniform probability function on S predicts that pt=N(x) is\nequal to 2\u2212N times the number of sequences in Swhose entries add up to x.\nNote that pN(x) is not the uniform probability function on the set of possible values for f. To see that such is the", "metadata": {"page": 63}}, {"page_content": "case, note that the possible values for xare the integers from the set W = {\u2212N,\u2212N + 2,...,N \u22122,N}. This set\nhas N + 1 elements. Since only the sequence (\u22121,\u22121,..., \u22121) has entries that sum to \u2212N, so pN(\u2212N) = 2 \u2212N.\nMeanwhile, the only sequence from Swhose entries sum to N is (1,1,..., 1), and so pN(N) = 2\u2212N also. However\nthere are N elements in Swhose entries sum to \u2212N+2, these the elements that have a single entry equal to+1. Thus,\npN(\u2212N + 2) = N2\u2212N. A similar count \ufb01nds pN(N \u22122) = N2\u2212N.", "metadata": {"page": 63}}, {"page_content": "pN(\u2212N + 2) = N2\u2212N. A similar count \ufb01nds pN(N \u22122) = N2\u2212N.\nIn general, if a set Shas N elements and a subset K \u2282Shas kelements, then the uniform probability distribution on\nS gives probability k\nN to the set K. Even so, it may be some task to count the elements in any given set. Moreover,\nthe degree of dif\ufb01culty may depend on the manner in which the set is described. There are, however, some standard", "metadata": {"page": 63}}, {"page_content": "counting formulae available to facilitate things. For example, letS denote the sample space with 2N elements as\n60 Chapter 11. Common probability functions", "metadata": {"page": 63}}, {"page_content": "described above. For n \u2208{0, 1,...,N }, let Kn denote the subset of elements in S with noccurrences of +1, thus\nwith N \u2212n occurrences of \u22121. Note that Kn has an alternate description, this as the set of elements on which\nf(\u03b11,...,\u03b1 N) = \u03b11 + \u03b12 + \u00b7\u00b7\u00b7 + \u03b1N has value \u2212N + 2n. Thus, the number of elements in Kn times 2\u2212N gives\npt=N(x) in the case where x= \u2212N + 2n.\nIn any event, here is a basic fact:\nThe set Kn has N!\nn!(N \u2212n)! members. (11.3)", "metadata": {"page": 64}}, {"page_content": "In any event, here is a basic fact:\nThe set Kn has N!\nn!(N \u2212n)! members. (11.3)\nIn this regard, remember that k! is de\ufb01ned for any positive integer kas k(k\u22121)(k\u22122) \u00b7\u00b7\u00b71. Also, 0! is de\ufb01ned to by\nequal to 1. For those who don\u2019t like to take facts without proof, I explain in the last section below how to derive (11.3)\nand also the formulae that follow.\nBy the way, arises often enough in counting problems to warrant its own symbol, this\n(N\nn\n)\n. (11.4)", "metadata": {"page": 64}}, {"page_content": "(N\nn\n)\n. (11.4)\nHere is another standard counting formula: Let b \u22651 be given, and let S denote the set of bN elements of the form\n(\u03b21,...,\u03b2 N), where each \u03b2k is now {1,...,b }. For example, if b= 6, then Sis the sample space for the list of faces\nthat appear when a six-sided die is rolled N times. If b= 4, then Sis the sample space for the possible single strands\nof DNA with N bases.", "metadata": {"page": 64}}, {"page_content": "of DNA with N bases.\nIf N >b, then each element in Shas at least one \u03b2k that is the same as another. If N \u2264b, then there can be elements\nin Swhere no two \u03b2k are the same. Fix band let Eb denote the subset of those N-tuples (\u03b21,...,\u03b2 N) where no two\n\u03b2k are identical.\nThe set Eb has b!\n(b\u2212N)! members. (11.5)\nThe case n= N in (11.5) provides the following:\nThere are N! ways to order a set of N distinct elements. (11.6)", "metadata": {"page": 64}}, {"page_content": "There are N! ways to order a set of N distinct elements. (11.6)\nHere, a set of elements is \u2018ordered\u2019 simply by listing them one after the other. For example, the set that consists\nof 1 apple and 1 orange has two orderings, (apple, orange) and (orange, apple). The set that consist of the three\nelements {apple, orange, grape} has six orderings, (apple, orange, grape), (apple, grape, orange), (orange, grape,\napple), (orange, apple, grape), (grape, apple, orange), (grape, orange, apple).", "metadata": {"page": 64}}, {"page_content": "apple), (orange, apple, grape), (grape, apple, orange), (grape, orange, apple).\nHere is a direct argument for (11.6): To count the number of orderings, note that there are N possible choices for the\n\ufb01rst in line. With the \ufb01rst in line chosen, then N\u22121 elements can be second in line. With the \ufb01rst two in line chosen,\nthere are N \u22122 left that can be third in line. Continuing in this reasoning leads to (11.6).\n11.4 Some standard probability functions", "metadata": {"page": 64}}, {"page_content": "11.4 Some standard probability functions\nThe counting formulae just presented can be used to derive some probability functions that you will almost surely see\nagain and again in your scienti\ufb01c career.\nThe Equal Probability Binomial: Let Sdenote the sample space with the 2N elements of the form (\u03b11,...,\u03b1 N)\nwhere each \u03b1k can be 1 or \u22121. For any given integer n \u2208 {0,1,...,n }, let Kn denote the event that there are", "metadata": {"page": 64}}, {"page_content": "precisely noccurrences of +1 in the N-tuple (\u03b11,...,\u03b1 N). Then the uniform probability function on S assigns Kn\nthe probability\nP(n) = 2\u2212N N!\nn! (N \u2212n)! (11.7)\n11.4. Some standard probability functions 61", "metadata": {"page": 64}}, {"page_content": "The fact that Kn has probability P(n) follows from (11.3).\nThe assignment of P(n) to an integer nde\ufb01nes a probability function on the N + 1 element set {0,1,...,N }. This\nis a probability function that is not uniform. We will investigate some of its properties below.\nEquation (11.7) can be used to answer the \ufb01rst question in (11.1) with regards to our random bacteria in Section 11.1.\nTo see how this comes about, let S denotes the sample space with the 2N elements of the form (\u00b11,\u00b11,..., \u00b11).", "metadata": {"page": 65}}, {"page_content": "Let f again denote the random variable that assigns \u03b11 + \u00b7\u00b7\u00b7 + \u03b1N to any given (\u03b11,...,\u03b1 N). Then, the event\nf = \u2212N + 2nis exactly our set Kn. This understood, if x\u2208{\u2212N,\u2212N + 2,...,N \u22122,N}then\nP(the event that f = x) = 2\u2212N N!(N+x\n2\n)\n!\n(N\u2212x\n2\n)\n!. (11.8)\nIf we believe that the bacteria chooses left and right with equal probability, and that moves made in any subset of\ntheN steps have no bearing on those made in the remaining steps, then we should be comparing our experimentally", "metadata": {"page": 65}}, {"page_content": "determined pt(x) with the N = tversion of (11.8).\nThe Binomial Probability Function: The probability function P in (11.7) is an example of what is called the\nbinomial probability function on the set {0,...,N }. The \u2018generic\u2019 version of the binomial probability distribution\nrequires the choice of a number, q \u2208[0,1]. With qchosen, the probability q-version assigns the following probability\nto an integer n:\nPq(n) = qn(1 \u2212q)N\u2212n. (11.9)", "metadata": {"page": 65}}, {"page_content": "to an integer n:\nPq(n) = qn(1 \u2212q)N\u2212n. (11.9)\nThe probability function in (11.9) arises from the sample space S whose elements are the N-tuples with elements\n(\u00b11,..., \u00b11). In this regard, (11.9) arises in the case that the probability of seeing +1 in any given entry is q and\nthat of \u22121 in a given entry is 1 \u2212q. Here, this probability function assumes that the occurrences of \u00b11 in any subset", "metadata": {"page": 65}}, {"page_content": "of entries must have no bearing on the appearances of \u00b11 in the remaining entries. The probability function in (11.9)\ndescribes the probability in this case for the setKn \u2282Sof elements with noccurrences of +1 and N\u2212noccurrences\nof \u22121.\nOne can also view the probability function in (11.9) as follows: De\ufb01ne a random variable, g, on S, that assigns to any\ngiven element the number of appearances of +1. Give Sthe probability function just described. Then (11.9) gives the", "metadata": {"page": 65}}, {"page_content": "probability that g = n. To see why this probability is (11.9), note that the event that g = nis just our set Kn. With\nthis new probability, each element in Kn has probability qn(1 \u2212q)N\u2212n. As (11.3) gives the number of elements in\nKn, its probability is therefore given by (11.9).\nThe probability function in (11.9) is relevant to our bacterial walking scenario when we make the hypothesis that the", "metadata": {"page": 65}}, {"page_content": "bacteria moves to the right at any given step with probabilityq, thus to the left with probability 1 \u2212q. I\u2019ll elaborate on\nthis in a subsequent chapter.\nHere is another example: Suppose you role a six sided die N times. I now ask \ufb01x an integer n from the set\n{0,1,...,N }and ask for the probability that precisely n \u2264N of the rolls are such that the number 6 appears. If\nI assume that the die is fair, and that the numbers that appear on any subset of rolls have no bearing on the numbers", "metadata": {"page": 65}}, {"page_content": "that appear in the remaining rolls, then the probability ofnoccurrences of 6 in N rolls of the die is given by theq= 1\n6\nversion of (11.9). Here is why: I make an N element sequence (\u03b11,...,\u03b1 N) with each \u03b1k = 1 or \u22121 by setting\n\u03b1k = +1 when 6 appears on the kth roll of the die, and setting \u03b1k = \u22121 when 1, 2, 3, 4, or 5 appears on the kth roll\nof the die. The possible sequence of this form make a 2N element set that I call S. As the probability is 1\n6 for any", "metadata": {"page": 65}}, {"page_content": "6 for any\ngiven \u03b1k to equal to +1, so a given sequence fromSwith precisely noccurrences of +1 has probability (1\n6 )n(5\n6 )N\u2212n.\nMeanwhile, the number of elements inSwith noccurrences of +1 is given by (11.3). This understood, the probability\nof an element in Shaving noccurrences of +1 is equal to the product of (11.3) with (1\n6 )n(5\n6 )N\u2212n, and this is just the\nq= 1\n6 version of (11.9).", "metadata": {"page": 65}}, {"page_content": "6 )n(5\n6 )N\u2212n, and this is just the\nq= 1\n6 version of (11.9).\nTo give an example, consider rolling a standard, six-sided die. If the die is rolled once, the sample space consists of the\nnumbers {1,2,3,4,5,6}. If the die is \u2018fair\u2019, then I would want to use the probability function that assigns the version\nof (11.9).\nHere is a \ufb01nal example: Consider a single stranded DNA molecule made of N bases. Suppose that the probability", "metadata": {"page": 65}}, {"page_content": "that the base C appears in any given position is given by some q \u2208[0,1]. If each base is equally likely, then q = 1\n4 to\n62 Chapter 11. Common probability functions", "metadata": {"page": 65}}, {"page_content": "each element. If the bases that appear in any given subset of the N stranded molecule have no bearing on those that\nappear in the remaining positions, then the probability that nof the N bases are C should be given by the expression\nin (11.9). Note that this is a de\ufb01nite prediction about the DNA molecules in a given cell, and can be tested in principle\nby determining the percent of DNA that has the base C (cytosine). If this percentage differs signi\ufb01cantly from 1\n4 , then", "metadata": {"page": 66}}, {"page_content": "4 , then\nat least one of our assumptions is incorrect: Either it is not true that the appearance of C in any given position has\nprobability1\n4 , or it is not true that the bases that appear in a given subset of the molecule have no bearing on those that\nappear in the remainder of the molecule.\nI talk later about the meaning of the term \u2018differs signi\ufb01cantly from\u2019.\nThe Poisson Probability Function: This is a probability function on the sample space, N = {0,1,... }, the non-", "metadata": {"page": 66}}, {"page_content": "negative integers. As you can see, this sample space has an in\ufb01nite number of elements. Even so, I de\ufb01ne a probability\nfunction on N to be a function, P, with P(n) \u2208(0,1) for each n, and such that\n\u2211\nn=0,1,...\nP(n) = P(0) + P(1) + P(2) + \u00b7\u00b7\u00b7 (11.10)\nis a convergent series with limit equal to 1.\nAs with the binomial probability function, there is a whole family of Poisson functions, one for each choice of a", "metadata": {"page": 66}}, {"page_content": "positive real number. Let\u03c4 >0 denote the given choice. The \u03c4 version of the Poisson function assigns to any given\nnon-negative integer the probability\nP\u03c4(n) = 1\nn!\u03c4ne\u2212\u03c4. (11.11)\nYou can see that \u2211\nn=0,1,...\nP\u03c4(n) = 1 if you know about power series expansions of the exponential function. In\nparticular, the function \u03c4 \u2192e\u03c4 has the power series expansion\ne\u03c4 = 1 + \u03c4 + 1\n2\u03c42 + 1\n6\u03c43 + \u00b7\u00b7\u00b7 + 1\nn!\u03c4n + \u00b7\u00b7\u00b7 . (11.12)\nGranted (11.12), then \u2211\nn=0,1,...\n1\nn! \u03c4ne\u2212\u03c4 =\n(\n\u2211\nn=0,1,...\n1\nn! \u03c4n\n)\ne\u2212\u03c4 = e\u03c4e\u2212\u03c4 = 1.", "metadata": {"page": 66}}, {"page_content": "Granted (11.12), then \u2211\nn=0,1,...\n1\nn! \u03c4ne\u2212\u03c4 =\n(\n\u2211\nn=0,1,...\n1\nn! \u03c4n\n)\ne\u2212\u03c4 = e\u03c4e\u2212\u03c4 = 1.\nThe Poisson probability enters when trying to decide if an observed cluster of occurrences of a particular phenomenon\nis or is not due to chance. For example, suppose that on average, some number,\u2206, of newborns in the United States\ncarry a certain birth defect. Suppose that some number, n, of such births are observed in 2006. Does this constitute an", "metadata": {"page": 66}}, {"page_content": "unexpected clustering that should be investigated? If the defects are unrelated and if the causative agent is similar in\nall cases over the years, then the probability ofnoccurrences in a given year should be very close to the value of the\n\u03c4 = \u2206 version of the Poisson function P\u03c4(n).\nHere is another example from epidemiology: Suppose that the university health service sees an average of 10 cases", "metadata": {"page": 66}}, {"page_content": "of pneumonia each winter. Granted that the conditions that prevail this coming winter are no different than those\nin the past, and granted that cases of pneumonia are unrelated, what is the probability of there being20 cases of\npneumonia this winter? If these assumptions are valid, then the \u03c4 = 10 Poisson function should be used to compute\nthis probability. In particular, P10(20) \u22480.0058.", "metadata": {"page": 66}}, {"page_content": "this probability. In particular, P10(20) \u22480.0058.\nWhat follows is a \ufb01nal example, this related to discerning whether patterns are \u2018random\u2019 or not. Consider, for example,\nthe sightings of \u2018sea monsters\u2019. In particular, you want to know if more sea monster sightings have occurred in the\nBermuda Triangle then can be accounted for by chance. One way to proceed is to catalogue all such sightings to", "metadata": {"page": 66}}, {"page_content": "compute the average number of sightings per day per ship. Let us denote this average by\u03b4. Now, estimate the number\nof \u2018ship days\u2019 that are accounted for by ships while in the Bermuda Triangle. Let N denote this last number. If\nsightings of sea monsters are unrelated and if any two ships on any two days in any two parts of the ocean are equally\nlikely to sight a sea monster, then the probability of n sightings in the Bermuda triangle is given by the\u03c4 = N\u03b4 version\nof (11.11).", "metadata": {"page": 66}}, {"page_content": "of (11.11).\nI give some further examples of how the Poisson function is used in a separate chapter.\n11.4. Some standard probability functions 63", "metadata": {"page": 66}}, {"page_content": "By the way, the Poisson function is an N \u2192\u221e limit of the binomial function. To be more precise, the \u03c4 version of\nthe Poisson probability P\u03c4(n) is the N \u2192\u221e limit of the versions of (11.9) with qset equal to 1 \u2212e\u2212\u03c4/N. This is to\nsay that\n1\nn!\u03c4ne\u2212\u03c4 = limN\u2192\u221e\nN!\nn! (N \u2212n)!\n(\n1 \u2212e\u2212\u03c4/N\n)n\ne(\u2212\u03c4/N)(N\u2212n). (11.13)\nThe proof that (11.13) holds takes us in directions that we don\u2019t have time for here. Let me just say that it uses the\napproximation to the factorial known as Stirling\u2019s formula:\nn! =\n\u221a", "metadata": {"page": 67}}, {"page_content": "approximation to the factorial known as Stirling\u2019s formula:\nn! =\n\u221a\n2\u03c0ne\u2212nnn(1 + error) (11.14)\nwhere the term designated as \u2018error\u2019 limits to zero as n \u2192\u221e. For example, the ratio with n! the numerator and\u221a\n2\u03c0ne\u2212nnn as the denominator is as follows for various choices of n:\nn n!\u221a\n2\u03c0ne\u2212nnn\n2 7.7\n5 1.016\n10 1.008\n20 1.004\n100 1.0008\nThus, the approximation in (11.14) serves for most uses.\n11.5 Means and standard deviations", "metadata": {"page": 67}}, {"page_content": "Thus, the approximation in (11.14) serves for most uses.\n11.5 Means and standard deviations\nLet me remind you that if P is a probability function on some subsetSof the set of integers, then its mean,\u00b5, is de\ufb01ned\nto be\n\u00b5=\n\u2211\nn\u2208S\nnP(n) (11.15)\nand the square of its standard deviation, \u03c3, is de\ufb01ned to be\n\u03c32 =\n\u2211\nn\u2208S\n(n\u2212\u00b5)2P(n). (11.16)\nHere, one should keep in mind that when Sis an in\ufb01nite number of elements, then \u00b5and \u03c3are de\ufb01ned only when the", "metadata": {"page": 67}}, {"page_content": "corresponding sums on the right sides of (11.15) and (11.16) are those of convergent series. The mean and standard\ndeviation characterize any given probability function to some extent. More to the point, both the mean and standard\ndeviation are often used in applications of probability and statistics.\nThe mean and standard deviation for the binomial probability on {0,1,...,N }are\n\u00b5= Nq and \u03c32 = Nq(1 \u2212q). (11.17)\nFor the Poisson probability function on {0,1,... }, they are", "metadata": {"page": 67}}, {"page_content": "\u00b5= Nq and \u03c32 = Nq(1 \u2212q). (11.17)\nFor the Poisson probability function on {0,1,... }, they are\n\u00b5= \u03c4 and \u03c32 = \u03c4. (11.18)\nI describe a slick method for computing the relevant sums below.\nTo get more of a feel for the binomial probability function, note \ufb01rst that the mean for theq = 1\n2 . version is N\n2 .\nThis conforms to the expectation that half of the entries in the \u2018average\u2019N-tuple (\u03b11,...,\u03b1 N) should be +1 and half", "metadata": {"page": 67}}, {"page_content": "should be \u22121. Meanwhile in the general version, the assertion that the mean is Nq suggests that the fraction qof the\nentries in the \u2018average\u2019N-tuple should be +1 and the fraction (1 \u2212q) should be \u22121.\nBy the way, one can ask for the value of nthat makes Pq(n) largest. To see what this is, note that\nPq(n+ 1)\nPq(n) = N \u2212n\nn+ 1\nq\n1 \u2212q. (11.19)\n64 Chapter 11. Common probability functions", "metadata": {"page": 67}}, {"page_content": "This ratio is less than 1 if and only if\nn<Nq \u2212(1 \u2212q). (11.20)\nSince (1 \u2212q) <1, this then means that the ratio in (11.19) peaks at a value of nthat is within \u00b11 of the mean.\nIn the case of the Poisson probability, the ratio P\u03c4(n+1)\nP\u03c4(n) is 1\nn+1 \u03c4. If \u03c4 < 1, then 0 has the greatest probability of\noccurring. If \u03c4 \u22651, then P\u03c4(n) is greatest when nis the closest integer to \u03c4 \u22121. Thus, the mean in this case is also\nvery nearly the integer with the greatest probability of occurring.", "metadata": {"page": 68}}, {"page_content": "very nearly the integer with the greatest probability of occurring.\n11.6 The Chebychev theorem\nAs I remarked in Chapter 5, the standard deviation indicates the extent to which the probabilities concentrate about\nthe mean. To see this, consider the following remarkable fact:\nThe Chebychev Theorem. Suppose that P is a given probability function on a subset of the integers,\n{..., \u22121,0,1,... }, one with a well de\ufb01ned mean \u00b5and standard deviation \u03c3. For any R \u22651, the probability as-", "metadata": {"page": 68}}, {"page_content": "signed to the set where |n\u2212\u00b5|>R\u03c3 is less than R\u22122.\nFor example, this says that the probability of being2\u03c3away from the mean is less than 1\n4 , and the probability of being\n3\u03c3away is less than 1\n9 .\nThis theorem justi\ufb01es the focus in the literature on the mean and standard deviation, since knowing these two numbers\ngives you rigorous bounds for probabilities without knowing anything else about the probability function!!", "metadata": {"page": 68}}, {"page_content": "If you remember only one thing from these lecture notes, remember the Chebychev Theorem.\nHere is the proof of the Chebychev theorem: LetSdenote the sample space under consideration, and letE \u2282Sdenote\nthe set where |n\u2212\u00b5|> R\u03c3. The probability of E is then \u2211\nn\u2208E\nP(n)). However, since |n\u2212\u00b5|> R\u03c3for n\u2208E, one\nhas\n1 \u2264|n\u2212\u00b5|2\nR2\u03c32 (11.21)\non E. Thus, \u2211\nn\u2208E\nP(n) \u2264\n\u2211\nn\u2208E\n|n\u2212\u00b5|2\nR2\u03c32 P(n). (11.22)", "metadata": {"page": 68}}, {"page_content": "has\n1 \u2264|n\u2212\u00b5|2\nR2\u03c32 (11.21)\non E. Thus, \u2211\nn\u2208E\nP(n) \u2264\n\u2211\nn\u2208E\n|n\u2212\u00b5|2\nR2\u03c32 P(n). (11.22)\nTo \ufb01nish the story, note that the right side of (11.22) is even larger when we allow the sum to include all points in S\ninstead of restricting only to points in E. Thus, we learn that\n\u2211\nn\u2208E\nP(n) \u2264\n\u2211\nn\n|n\u2212\u00b5|2\nR2\u03c32 P(n). (11.23)\nThe de\ufb01nition of \u03c32 from (11.16) can now be invoked to identify the sum on the right hand side of (11.23) with R\u22122.", "metadata": {"page": 68}}, {"page_content": "Here is an example of how one might apply the Chebychev Theorem: I watch a six-sided die rolled 100 times and see\nthe number 6 appear thirty times. I wonder how likely it is to see thirty or more appearances of the number 6 given\nthat the die is fair. Under the assumption that the die is fair and that the faces that appear in any given subset of the\n100rolls have no bearing on those that appear in the remaining rolls, I can compute this probability using the q = 1\n6", "metadata": {"page": 68}}, {"page_content": "6\nversion of the N = 100 binomial probability function by computing the sumP1/6(30) +P1/6(31) +\u00b7\u00b7\u00b7+ P1/6(100).\nAlternately, I can get an upper bound for this probability by using the Chebychev Theorem. In this regard, I note that\nthe mean of theq = 1\n6 and N = 100 binomial probability function is 100\n6 = 50\n3 = 16 2\n3 and the standard deviation\nis 5\n3\n\u221a\n5 \u22483.73. Now, 30 is 13 1\n3 from the mean, and this is \u22483.58 standard deviations. Thus, the probability of", "metadata": {"page": 68}}, {"page_content": "3 from the mean, and this is \u22483.58 standard deviations. Thus, the probability of\nseeing thirty or more appearances of the number 6 in 100 rolls is no greater than (3.58)\u22122 \u22480.08. As it turns out, the\nprobability of seeing 6 appear thirty or more times is a good deal smaller than this.\n11.6. The Chebychev theorem 65", "metadata": {"page": 68}}, {"page_content": "It is usually the case that the upper bound given by the Chebychev Theorem is much greater than the true probability.\nEven so, the Chebychev Theorem is one of the most important things to remember from this course since it allows\nyou to computesome upper bound with very little detailed knowledge of the probability function. Just two numbers,\nthe mean and standard deviation, give you an upper bound for probabilities.\n11.7 Characteristic functions", "metadata": {"page": 69}}, {"page_content": "11.7 Characteristic functions\nThe slick computation of the mean and standard deviation that I mentioned involves the introduction of the notion\nof the characteristic function. The latter is a bona \ufb01de function on the real numbers that is determined by a given\nprobability function on any sample space that is a subset of the non-negative integers, {0,1,... }. This characteristic", "metadata": {"page": 69}}, {"page_content": "function encodes all of the probability function. Note, however, that this function is useful for practical purposes only\nto the extent that it isn\u2019t some complicated mess.\nBefore turning to the de\ufb01nition in abstract, consider the special case where the subset is {0,1,2,...,N }and the\nprobability function is the qbinomial function from (11.9). In this case, the characteristic function is the function of x\ngiven by\nP(x) = (qx+ (1 \u2212q))N . (11.24)", "metadata": {"page": 69}}, {"page_content": "given by\nP(x) = (qx+ (1 \u2212q))N . (11.24)\nHere is the connection between P(x) and the binomial probability function: Expand the product so as to write P as\na polynomial of degree N in x. As I argue below, you will \ufb01nd that\nP(x) = Pq(0) + xPq(1) + x2Pq(2) + \u00b7\u00b7\u00b7 + xNPq(N). (11.25)\nThus, the coef\ufb01cient of xn of this polynomial representation of P gives us the probabilities that Pq assigns to the\ninteger n.", "metadata": {"page": 69}}, {"page_content": "integer n.\nTo see why (11.25) is the same as (11.24), consider multiplying out an N-fold product of the form:\n(a1x+ b1)(a2x+ b2) \u00b7\u00b7\u00b7(aNx+ bN) (11.26)\nA given term in the resulting sum can be labeled as(\u03b11,...,\u03b1 N) where \u03b1k = 1 if the kth factor in (11.26) contributed\nakx, while \u03b1k = \u22121 if the kth factor contributes bk. The power of xfor such a term is equal to the number of \u03b1k\nthat are +1. This is N!\nn! (N\u2212n)! , the number of elements in the set Kn that appears in (11.3). Thus, N!", "metadata": {"page": 69}}, {"page_content": "n! (N\u2212n)! , the number of elements in the set Kn that appears in (11.3). Thus, N!\nn! (N\u2212n)! terms\ncontribute to the coef\ufb01cient of xn in (11.26). In the case of (11.25), all versions of ak are equal to q, and all versions\nof bk are equal to (1 \u2212q), so each term that contributes to xn is qn(1 \u2212q)N\u2212nxn. As there are N!\nn! (N\u2212n)! of them, the\ncoef\ufb01cient of xn in (11.24) is Pq(n) as (11.24) claims.", "metadata": {"page": 69}}, {"page_content": "n! (N\u2212n)! of them, the\ncoef\ufb01cient of xn in (11.24) is Pq(n) as (11.24) claims.\nIn general the characteristic function for a probability function, P, on a subset of{0,1,... }is the polynomial or in\ufb01nite\npower series in xfor which the coef\ufb01cient of xn is P(n). This is to say that\nP(x) = P(0) + P(1)x+ P(2)x2 + \u00b7\u00b7\u00b7 =\n\u2211\nn=0,1,2,...\nP(n)xn. (11.27)\nThis way of coding the probability function P is practical only to the extent that the series in (11.27) sums to a relatively", "metadata": {"page": 69}}, {"page_content": "simple function. I gave the q-binomial example above. In the case of the uniform probability function on {0,...,N },\nthe sum in (11.27) is the function 1\nN+1\n1\u2212xN+1\n1\u2212x . I argue momentarily that the \u03c4 version of the Poisson probability\nfunction on {0,1,2,... }gives\nP(x) = e(x\u22121)\u03c4. (11.28)\nIn general, there are two salient features of a characteristic polynomial: First, the values of P, its derivative, and its\n66 Chapter 11. Common probability functions", "metadata": {"page": 69}}, {"page_content": "second derivative at x= 1 are:\n\u2022P(1) = 1\n\u2022\n( d\ndxP\n)\u23d0\u23d0\u23d0\n\u23d0\nx=1\n= \u00b5\n\u2022\n( d2\ndx2 P\n)\u23d0\n\u23d0\u23d0\u23d0\nx=1\n= \u03c32 + \u00b5(\u00b5\u22121)\n(11.29)\nSecond, the values of the value of P, its derivative and its higher order derivatives atx= 0 determine P since\n1\nn!\n( dn\ndxnP\n)\u23d0\n\u23d0\u23d0\u23d0\nx=0\n= P(n). (11.30)\nThis is how the function x \u2192P(x) encodes all of the information that can be obtained from the given probability\nfunction P.", "metadata": {"page": 70}}, {"page_content": "function P.\nTo explain how (11.29) follows from the de\ufb01nition in (11.27), note \ufb01rst thatP(1) = P(0)+P(1)+\u00b7\u00b7\u00b7, and this is equal\nto 1 since the sum of the probabilities must be 1. Meanwhile, the derivative of P at x= 1 is 1 \u00b7P(1) + 2\u00b7P(2) +\u00b7\u00b7\u00b7,\nand this is sum is the de\ufb01nition of the mean \u00b5. With the help of (11.16), a very similar argument establishes the third\npoint in (11.29).", "metadata": {"page": 70}}, {"page_content": "point in (11.29).\nI can use (11.29) to get my slick calculations of the mean and standard deviations for the binomial probability function.\nIn this case, P is given in (11.24) and so\nd\ndxP = Nq(qx+ (1 \u2212q))N\u22121 . (11.31)\nSet x= 1 here to \ufb01nd the mean equal to Nq as claimed. Meanwhile\nd2\ndx2 P = N(N \u22121)q2 (qx+ (1 \u2212q))N\u22121 , (11.32)\nSet x = 1 here \ufb01nds the right-hand side of (11.32) equal to N(N \u22121)q2. Granted this and the fact that \u00b5= qN, then\nthe third point in (11.29) \ufb01nd \u03c32 equal to", "metadata": {"page": 70}}, {"page_content": "the third point in (11.29) \ufb01nd \u03c32 equal to\nN(N \u22121)q2 \u2212N2q2 + Nq = Nq(1 \u2212q), (11.33)\nwhich is the asserted value.\nFor the Poisson probability function, the characteristic polynomial is the in\ufb01nite power series\nP\u03c4(0) + xP\u03c4(1) + x2P\u03c4(2) + \u00b7\u00b7\u00b7 =\n(\n1 + x\u03c4 + 1\n2x2\u03c42 + 1\n6x3\u03c43 + \u00b7\u00b7\u00b7 + 1\nn!xn\u03c4n + \u00b7\u00b7\u00b7\n)\ne\u2212\u03c4. (11.34)\nAs can be seen by replacing \u03c4 in (11.12) with x\u03c4, the sum on the right here is ex\u03c4 . Thus, P(x) = e(x\u22121)\u03c4 as claimed", "metadata": {"page": 70}}, {"page_content": "by (11.28). Note that the \ufb01rst and second derivatives of this function at x= 1 are both equal to \u03c4. With (11.29), this\nlast fact serves to justify the claim that the both the mean and standard deviation for the Poisson probability are equal\nto \u03c4.\nThe characteristic polynomial for a probability function is used to simplify seemingly hard computations in the manner\njust illustrated in the cases where the de\ufb01ning sum in (11.27) can be identi\ufb01ed with the power series expansion of a", "metadata": {"page": 70}}, {"page_content": "well known function. The characteristic function has no practical use when the power series expansion is not that of a\nsimple function.\n11.8 Loose ends about counting elements in various sets\nMy purpose in this last section is to explain where the formula in (11.3) and (11.5) come from. To start, consider (11.5).\nThere are bchoices for \u03b21. With \u03b21 chosen, there are b\u22121 choices for \u03b22, one less than that for \u03b21 since we are not\n11.8. Loose ends about counting elements in various sets 67", "metadata": {"page": 70}}, {"page_content": "allowed to have these two equal. Given choices for \u03b21 and \u03b22, there are b\u22122 choices for \u03b23. Continuing in this\nvein \ufb01nds b\u2212k choices available for \u03b2k+1 if (\u03b21,...,\u03b2 k) have been chosen. Thus, the total number of choices is\nb(b\u22121) \u00b7\u00b7\u00b7(b\u2212N + 1), and this is the claim in (11.5).\nTo see how (11.3) arises, let me introduce the following notation: Let mn(N) denote the number of elements in the", "metadata": {"page": 71}}, {"page_content": "(\u03b11,...,\u03b1 N) version of Kn. If we are counting elements in this set, then we can divide this version of Kn into two\nsubsets, one where \u03b11 = 1 and the other where \u03b11 = \u22121. The number of elements in the \ufb01rst is mn\u22121(N \u22121) since\nthe (N \u22121)-tuple (\u03b12,...,\u03b1 N) must have n\u22121 occurrences of +1. The number in the second is mn(N \u22121) since\nin this case, the (N \u22121)-tuple (\u03b12,...,\u03b1 N) must have all of the noccurrences of +1. Thus, we see that\nmn(N) = mn\u22121(N \u22121) + mn(N \u22121). (11.35)", "metadata": {"page": 71}}, {"page_content": "mn(N) = mn\u22121(N \u22121) + mn(N \u22121). (11.35)\nI will now make this last formula look like a matrix equation. For this purpose, \ufb01x some integer T \u22651 and make a\nT-component vector, \u20d7 m(N), whose coef\ufb01cients are the values of mn for the cases that 1 \u2264n \u2264T. This equation\nasserts that \u20d7 m(N) = A\u20d7 m(N\u22121) where Ais the matrix with Ak,k and Ak,k\u22121 both equal to 1 and all other entries\nequal to zero. Iterating the equation \u20d7 m(N) = A\u20d7 m(N\u22121) \ufb01nds\n\u20d7 m(N) = AN\u22121 \u20d7 m(1), (11.36)", "metadata": {"page": 71}}, {"page_content": "equal to zero. Iterating the equation \u20d7 m(N) = A\u20d7 m(N\u22121) \ufb01nds\n\u20d7 m(N) = AN\u22121 \u20d7 m(1), (11.36)\nwhere \u20d7 m(1)is the vector with top component 1 and all others equal to zero.\nNow, we don\u2019t have the machinery to realistically compute AN\u22121, so instead, lets just verify that the expression\nin (11.3) gives the solution to (11.35). In this regard, note that \u20d7 m(N) is completely determined from \u20d7 m(1)us-", "metadata": {"page": 71}}, {"page_content": "ing (11.36), and so if we believe that we have a set {\u20d7 m(1),\u20d7 m(2),...}of solutions, then we need only plug in our\ncandidate and see if (11.36) holds. This is to say that in order to verify that (11.3) is the correct, one need only check\nthat the formula in (11.36) holds. This amounts to verifying that\nN!\nn! (N \u2212n)! = (N \u22121)!\n(n\u22121)! (N \u2212n)! + (N \u22121)!\nn! (N \u2212n\u22121)!. (11.37)\nI leave this task to you.\n11.9 A Nobel Prize for the clever use of statistics", "metadata": {"page": 71}}, {"page_content": "n! (N \u2212n\u22121)!. (11.37)\nI leave this task to you.\n11.9 A Nobel Prize for the clever use of statistics\nThe 1969 Nobel Prize for Physiology and Medicine was given to Max Delbruck and Salvador Luria for work that had\na crucial statistical component. What follows is a rough description of their Nobel Prize winning work.\nPrior to Delbruck and Luria\u2019s Nobel Prize winning work1 in 1943, there were two popular explanations for the evo-", "metadata": {"page": 71}}, {"page_content": "lution of new traits in bacteria. Here is the \ufb01rst: Random mutations occur as reproduction proceeds in a population.\nTherefore, no population of size greater than 1 is completely homogeneous. As time goes on, environmental stresses\nfavor certain variations over others, and so certain pre-existing variants will come to dominate any given population.\nThis view of evolution is essentially that proposed by Darwin. There is, in contrast, the so called Lamarckian pro-", "metadata": {"page": 71}}, {"page_content": "posal: Environmental stresses causenew traits to arise amongst some individuals in any given population, and these\nare subsequently favored. What follows summarizes these two proposals:\nDarwin: Evolution occurs through the selection due to environmen-\ntal stresses of the most favorable traits from a suite of pre-\nexisting traits.\nLamarck: Evolution occurs when environmental stresses force new,\nmore favorable traits to arise.", "metadata": {"page": 71}}, {"page_content": "Lamarck: Evolution occurs when environmental stresses force new,\nmore favorable traits to arise.\nMax Delbruck and Salvador Luria shared the 1969 Nobel Prize in Physiology and Medicine for an experiment, done\nin 1943, that distinguished between these two proposals as explanations for the evolution of immunity in bacterium.\nThe results conclusively favored Darwin over Lamarck.\n1Luria, SE, Delbruck, M., \u201cMutations of Bacteria from Virus Sensitivity to Virus Resistance\u201d Genetics28 (1943) 491-511.", "metadata": {"page": 71}}, {"page_content": "68 Chapter 11. Common probability functions", "metadata": {"page": 71}}, {"page_content": "What follows is an idealized version of what Luria and Delbruck did. They were working with a species of bacteria\nthat was susceptible to a mostly lethal virus. (A virus that attacks bacteria is called a \u2018phage\u2019.) They started a large\nnumber of colonies, each with one of these bacteria, and fed these colonies well for some length of timeT. Here,\nit is convenient to measure time in units so that a change of one unit of time is the mean time between successive", "metadata": {"page": 72}}, {"page_content": "generations. With these units understood, each colony contained roughlyN= 2T bacteria. Each of their colonies was\nthen exposed to the virus. Shortly after exposure to the virus, the number of members in each colony were counted.\nI useKto denote the number of colonies. The count of the number of living bacteria in each colony after phage\ninfection gives a list of numbers that I write as(z1,z2,...,z K). This set of numbers constitutes the experimental data.", "metadata": {"page": 72}}, {"page_content": "With regards to this list, it is suf\ufb01cient for this simpli\ufb01ed version of the story to focus only on\n\u00b5exp = 1\nK\n\u2211\n1\u2264j\u2264K\nzj and \u03c32\nexp = 1\nK\n\u2211\n1\u2264j\u2264K\n(zj \u2212\u00b5exp)2. (11.38)\nLuria and Delbruck realized that\u00b5exp and \u03c32\nexp can distinguish between the Darwin and Lamarck proposals. To explain\ntheir thinking, consider \ufb01rst what one would expect were the Lamarck proposal true. Let p denote the probability", "metadata": {"page": 72}}, {"page_content": "that exposure to the virus causes a mutation in any given bacterium that allows the bacterium to survive the infection.\nThe probability of n surviving bacteria in a colony with N members should be given by the binomial probability\nfunction from (11.9) as de\ufb01ned using pand Nin lieu of qand N. Thus, this probability is N!\nn! (N\u2212n)! pn(1\u2212p)N\u2212n. In\nparticular, the mean number of survivors should bepNand the square of the standard deviation should bep(1 \u2212p)N.", "metadata": {"page": 72}}, {"page_content": "This is to say that the Lamarck proposal predicts that the experimental data\n\u00b5exp \u2248pN and \u03c32\nexp \u2248p(1 \u2212p)N. (11.39)\nNot knowing the value for p, one can none the less say that the Lamarck proposal predicts the following:\n\u03c32\nexp\n\u00b5exp\n\u22481 \u2212p. (11.40)\nNote in particular that this number is independent of Nand K; and in any event it is very close to 1 when pis small.\nSmall pis expected.", "metadata": {"page": 72}}, {"page_content": "Small pis expected.\nNow consider the Darwinian proposal: In this case, there is some small, but non-zero probability, I\u2019ll call it p, for\nthe founding bacterium of any given colony to have a mutation that renders it immune to the virus. If the founder\nof a colony has this mutation, then it is unaffected by the virus as are all of its descendents. So, a colony with an\nimmune founder should have population2T = Nafter viral attack. If a colony is founded by a bacterium without this", "metadata": {"page": 72}}, {"page_content": "mutation, then its population after the viral attack should be very small. Thus, in this ideal situation, the Darwinian\nproposal predicts that eachzj should be either nearly zero or nearly N.\nIf Kcolonies are started, then the probability that n of them are founded by an immune bacterium is given by the\nbinomial probability function from (11.9) as de\ufb01ned now using p and Kin lieu of qand N. Thus, this probability is\nK", "metadata": {"page": 72}}, {"page_content": "K\nn! (K\u2212n)! pn(1 \u2212p)K\u2212n. Note that the mean of this is pKand the square of the standard deviation is p(1 \u2212p)K. As a\nconsequence, the Darwin proposal predicts that \u00b5exp and \u03c32\nexp should be roughly\n\u2022\u00b5exp \u2248\n\u2211\n0\u2264n\u2264K\n(Probability of nimmune founders)(K\u22121nN)\n\u2022\u03c32\nexp \u2248\n\u2211\n0\u2264n\u2264K\n(Probability of nimmune founders)(K\u22121nN2) \u2212\u00b52\nexp.\n(11.41)\nThese sums give the predictions \u00b5exp \u2248pNand \u03c32\nexp \u2248p(1 \u2212p)N2. As a consequence, the Darwin proposal predicts\nthe ratio\n\u03c32\nexp\n\u00b5exp\n\u2248(1 \u2212p)N. (11.42)", "metadata": {"page": 72}}, {"page_content": "the ratio\n\u03c32\nexp\n\u00b5exp\n\u2248(1 \u2212p)N. (11.42)\nNote in particular that if p is small, then this number is roughly N= 2T while that in (11.40) is roughly 1.\nDelbruck and Luria saw statistics that conclusively favored the Darwin proposal over the Lamarck one.\n11.9. A Nobel Prize for the clever use of statistics 69", "metadata": {"page": 72}}, {"page_content": "11.10 Exercises:\n1. Let Adenote the 4 \u00d74 version of the matrix in (11.36). Thus,\nA=\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n1 0 0 0\n1 1 0 0\n0 1 1 0\n0 0 1 1\n\uf8f6\n\uf8f7\uf8f7\uf8f8.\n(a) Present the steps of the reduced row echelon reduction of Ato verify that it is invertible.\n(b) Find A\u22121 using Fact 2.3.5 of Bretscher\u2019s book Linear Algebra with Applications.\n2. Let \u03c4 denote a \ufb01xed number in (0,1). Now de\ufb01ne a probability function, P, on the set {0,1,2,... }by setting\nP(n) = (1 \u2212\u03c4)\u03c4n.", "metadata": {"page": 73}}, {"page_content": "P(n) = (1 \u2212\u03c4)\u03c4n.\n(a) Verify that P(0) + P(1) + \u00b7\u00b7\u00b7 = 1, and thus verify that P is a probability function.\n(If you haven\u2019t seen how to do this, setV(n) = 1+\u03c4+\u03c42+\u00b7\u00b7\u00b7+\u03c4n. Prove that \u03c4V (n) = V(n)\u22121+\u03c4n+1.\nThen rearrange things to writeV(n)(1\u2212\u03c4) = (1\u2212\u03c4n+1). Now solve for V(n) and consider what happens\nwhen n\u2192\u221e.)\n(b) Sum the series P (0) +xP(1) +x2P(2) +\u00b7\u00b7\u00b7 to verify that the characteristic function is theP(x) = 1\u2212\u03c4\n1\u2212x\u03c4.\n(c) Use the formula in (11.28) to compute the mean and standard deviation of P.", "metadata": {"page": 73}}, {"page_content": "1\u2212x\u03c4.\n(c) Use the formula in (11.28) to compute the mean and standard deviation of P.\n(d) In the case \u03c4 = 1\n2 , the mean is1 and the standard deviation is\n\u221a\n2. As 6 \u2265\u00b5+3\u03c3, the Chebychev Theorem\nasserts that the probability for the set {6,7,... }should be less than 1\n9 . Verify this prediction by summing\nP(6) + P(7) + \u00b7\u00b7\u00b7.\n(e) In the case \u03c4 = 2\n3 , the mean is 2 and \u03c3 =\n\u221a\n6. Verify the prediction of the Chebychev Theorem that\n{7,8,... }has probability less than 1", "metadata": {"page": 73}}, {"page_content": "\u221a\n6. Verify the prediction of the Chebychev Theorem that\n{7,8,... }has probability less than 1\n4 by computing the sum P(7) + P(8)) + \u00b7\u00b7\u00b7.\n3. This exercise \ufb01lls in some of the details in the veri\ufb01cation of (11.3).\n(a) Multiply both sides of (11.37) by (n\u22121)!(N \u2212n\u22121)! and divide both sides of the result by (N \u22121)!\nGive the resulting equation.\n(b) Use this last result to verify (11.37).\n4. Suppose that I have a bag with 6 red grapes and 5 green ones. I reach in with my eyes closed and pick a grape", "metadata": {"page": 73}}, {"page_content": "at random. After looking at its color, I replace the grape, shake up the bag and redo this experiment, 10 times\nin all. Let nbe an integer between 0 and 10. Assume that any given grape is chosen at random each time, and\nthat my choices in any given subset of the experiments have no bearing on those of the remaining experiments.\nWhat is the probability of choosing exactly ngreen grapes in the 10 experiments?\n5. The purpose of this exercise is to explore the q = 1", "metadata": {"page": 73}}, {"page_content": "5. The purpose of this exercise is to explore the q = 1\n2 version of the binomial probability function, thus the\nfunction P(n) that appears in (11.7).\n(a) As explained in the text, P(n) is largest when n= N\n2 in the case that N is even. Use Stirling\u2019s formula to\njustify the claim that P(N\n2 ) \u2248\n\u221a\n2\n\u03c0N when N is even and very large. Compare this last number with the\ntrue value given by (11.7) for N = 10 and N = 100.\n(b) Suppose that N is a positive, even integer. The mean for the q = 1", "metadata": {"page": 73}}, {"page_content": "(b) Suppose that N is a positive, even integer. The mean for the q = 1\n2 binomial probability function for the\nset {0,1,...,N }is \u00b5 = 1\n2 N and the standard deviation is \u03c3 = 1\n2\n\u221a\nN. Use the Chebychev theorem in\nthe cases N = 36 and 64 to \ufb01nd an upper bound for the probability of the set of n \u226410. Then, use a\ncalculator to compute this probability using the formula in (11.7).\n6. The purpose of this problem is to explore the Poisson probability function in the context of discerning patterns.", "metadata": {"page": 73}}, {"page_content": "Suppose that I am a police captain in Boston in charge of a 10 block by 10 block neighborhood. Thus a\nneighborhood of 100 squares, each 1 block on a side. I note that in over the years, my force was asked to\nrespond to an average of 500 calls from the neighborhood, thus an average of 5 per square block per year. The\n70 Chapter 11. Common probability functions", "metadata": {"page": 73}}, {"page_content": "police station is in the square at one corner of the neighborhood. There were no calls from this square last year.\nThe square furthest from the police station recorded 15 calls. Under the assumption that a call is as likely to\ncome from any one square as any other, and that whether or not a call has come in from a square has no bearing\non when the next call will come in from that square or any other square, give\n(a) the probability that there are no calls from the police station\u2019s square, and", "metadata": {"page": 74}}, {"page_content": "(a) the probability that there are no calls from the police station\u2019s square, and\n(b) the probability that there are 15 calls from the square furthest from the police station\u2019s square.\n7. This problem is a continuation of the preceding one. The purpose here is to let you see how an event with\nsmall probability to occur in any one place can have a reasonably large probability of appearing somewhere. In", "metadata": {"page": 74}}, {"page_content": "particular, the goal for this exercise is to compute the probability that there is at least one square with no calls.\nFor this purpose, letqdenote your answer to Problem 6a.\n(a) Explain why the probability that every square has at least one call is (1 \u2212q)100.\n(b) Explain why the probability that at least one square has no calls is 1 \u2212(1 \u2212q)100.\n8. This is a problem whose purpose is to get you to think about counting and probability. The point is that being", "metadata": {"page": 74}}, {"page_content": "able to make an accurate count of the various ways that a given outcome can arise is crucial to making a correct\ncalculation of probabilities.\nHere is the scenario: In baseball\u2019s World Series, the two league champions in a given year meet to see which\nteam is the best in baseball for that year. The teams play games sequentially until one or the other wins four\ngames. The team that wins four games is declared the World Champion. Thus, the World Series can involve", "metadata": {"page": 74}}, {"page_content": "anywhere from 4 to 7 games. Since 1923, there have been 16 years where the series ended after 4 games, 16\nyears when it ended after 5 games, 18 years when it ended after 6 games and 33 years when it ended after\n7 games. Seehttp://mlb.mlb.com/NASApp/mlb/mlb/history/postseason/mlb_ws.jsp?\nfeature=recaps_index.\nThus, the relative frequencies for 4, 5, 6 and 7 game series are:\n4 games: 16\n83 \u22480.19\n5 games: 16\n83 \u22480.19\n6 games: 18\n83 \u22480.22\n7 games: 33\n83 \u22480.40", "metadata": {"page": 74}}, {"page_content": "4 games: 16\n83 \u22480.19\n5 games: 16\n83 \u22480.19\n6 games: 18\n83 \u22480.22\n7 games: 33\n83 \u22480.40\nHere is a question: What is the expected frequency of 4, 5, 6 and 7 game series if we assume that the two teams\nhave a 50-50 chance of winning any given game, and if we assume that the event of winning any one game is\nindependent of the event of winning any other. To answer this question, let A and N denote the two teams.", "metadata": {"page": 74}}, {"page_content": "(a) What is the probability for A to win the \ufb01rst 4 games? Likewise, what is the probability for N?\n(b) Explain why the probability for the World series to last 4 games is 1\n8 = 0.125.\n(c) Explain why the probability for the World Series to last 5 games is equal to\n2\n(4\n3\n)(1\n2\n)4 (1\n2\n)\n= 1\n4 = 0.25.\n(d) Explain why the probability for the World Series to last 6 games is\n2\n(5\n3\n)(1\n2\n)5 (1\n2\n)\n= 5\n16 = 0.3125.\n(e) Explain why the probability for the World Series to last 7 games is\n2\n(6\n3\n)(1\n2", "metadata": {"page": 74}}, {"page_content": "16 = 0.3125.\n(e) Explain why the probability for the World Series to last 7 games is\n2\n(6\n3\n)(1\n2\n)6 (1\n2\n)\n= 5\n16 = 0.3125.\n11.10. Exercises 71", "metadata": {"page": 74}}, {"page_content": "CHAPTER\nTWELVE\nP-values\nMy purpose in this chapter is to describe a criterion for deciding when data is \u201csurprising\u201d. For example, suppose that\nyou \ufb02ip a coin 100 times and see 60 heads. Should you be surprised? Should you question your belief that the coin is\nfair? More generally, how many heads should appear before you question the coin\u2019s fairness? These are the sorts of\nquestions that I will address below.\n12.1 Point statistics", "metadata": {"page": 75}}, {"page_content": "questions that I will address below.\n12.1 Point statistics\nSuppose that you do an experiment N times and \ufb01nd that a certain event occurs mtimes out of the N experiments.\nCan one determine from this data a probability function for the event to occur? For example, I \ufb02ip a coin 100 times\nand see 60 heads. Can I determine a probability function for heads to appear?\nIf we assume that the N experiments are identical in set up, and that the appearance of the event in any one has", "metadata": {"page": 75}}, {"page_content": "no bearing on its appearance in any other, then we can propose the following hypothesis: The event occurs in any\ngiven experiment with probability q (to be determined) and so the probability that some n \u2264N events occurs in N\nexperiments is given by the q-version of the binomial function, thus\nPq(n) = N!\nn!(N \u2212n)!qn(1 \u2212q)N\u2212n. (12.1)\nThe question now is: What value should be used for q?", "metadata": {"page": 75}}, {"page_content": "Pq(n) = N!\nn!(N \u2212n)!qn(1 \u2212q)N\u2212n. (12.1)\nThe question now is: What value should be used for q?\nThe use of experimental data to estimate a single parameter \u2013 qin this case \u2013 is an example of what are called point\nstatistics. Now, it is important for you to realize that there are various ways to obtain a \u201creasonable\u201d value to use for\nq. Here are some:\n\u2022Since we actually found mevents in N trials, take the value of q that gives mfor the mean of the probability", "metadata": {"page": 75}}, {"page_content": "function in (12.1). With reference to (11.17) in the previous chapter, this choice for qis m\nN.\n\u2022Take q to so that n = mis the integer with the maximal probability. If you recall (11.19) from the previous\nchapter, this entails taking qso that both\nPq(m)\nPq(m+ 1) >1 and Pq(m\u22121)\nPq(m) <1. (12.2)\nThis then implies that m\nN+1 <q < m+1\nN+1 . Note that q= m\nN satis\ufb01es these conditions.\nWith regards to my example of \ufb02ipping a coin 100 times and seeing 60 heads appear, these arguments would lead me", "metadata": {"page": 75}}, {"page_content": "to postulate that the probability of seeing some number, n, of heads, is given by the q= 0.6 version of (12.1).\n72", "metadata": {"page": 75}}, {"page_content": "12.2 P-value and bad choices\nA different approach asks for the bad choices of q rather than the \u201cbest\u201d choice. The business of ruling out various\nchoices for q is more in the spirit of the scienti\ufb01c method. Moreover, giving the unlikely choices for q is usually\nmuch more useful to others than simply giving your favorite candidate. What follows explains how statisticians often\ndetermine the likelihood that a given choice for qis realistic.", "metadata": {"page": 76}}, {"page_content": "determine the likelihood that a given choice for qis realistic.\nFor this purpose, suppose that we have some value for q in mind. There is some general agreement that q is not a\nreasonable choice when the following occurs:\nThere is small probability as computed by the q-version of (12.1) of there being the observed moccur-\nrences of the event of interest.\nTo make the notion of small probability precise, statisticians have introduced the notion of the \u201cP -value\u201d of a mea-", "metadata": {"page": 76}}, {"page_content": "surement. This is de\ufb01ned with respect to some hypothetical probability function, such as our q-version of (12.1). In\nour case, the P-value of mis the probability for the subset of numbers n\u2208{0, 1,...,N }that are at least as far from\nthe mean, qN, as is m. For example, mhas P-value 1\n2 in the case that (12.1) assigns probability 1\n2 to the set of integers\nnthat obey |n\u2212Nq|\u2265| m\u2212Nq|. A P-value that is less than 0.05 is deemed signi\ufb01cant by statisticians. This is to", "metadata": {"page": 76}}, {"page_content": "say that if mhas such a P-value, then qis deemed unlikely to be incorrect.\nIn general, the de\ufb01nition of the P-value for any measurement is along the same lines:\nDe\ufb01nition: Suppose that a probability function on the set of possible measurements for some experiment is proposed.\nThe P-value of any given measurement is the probability for the subset of values that lie as far or farther from the", "metadata": {"page": 76}}, {"page_content": "mean of this probability function than the given measurement. The P-value is deemed signi\ufb01cant if it is smaller than\n0.05.\nNote here that this de\ufb01nition requires computing the probability of being both greater than the mean and less than then\nthe mean. There is also the notion of a one-sided P-value. This computes the probability of being on the same side of\nthe mean as the observed value, and at least as far from the mean as the observed value. Thus, if the observed value is", "metadata": {"page": 76}}, {"page_content": "greater than the mean, the 1-sided P-value computes the probability that a measurement will be as large or larger than\nthe observed value. If the observed value is less than the mean, then the 1-sided P-value computes the probability of\nbeing as small or smaller than the observed value. There are other versions ofP-value used besides that de\ufb01ned above\nand the 1-sided P-values. However, in these lecture notes, P-value means what is written in the preceding de\ufb01nition.", "metadata": {"page": 76}}, {"page_content": "But, keep in mind when reading the literature or other texts that there are alternate de\ufb01nitions.\nConsider now the example where I \ufb02ip a coin 100 times and \ufb01nd 60 heads. If I want to throw doubt on the hypothesis\nthat the coin is fair, I should compute the P-value of 60 using the q = 1\n2 version of the binomial probability function\nin (12.1). This means computing P1/2(60) + P1/2(61) + \u00b7\u00b7\u00b7 + P1/2(100) + P1/2(40) + P1/2(39) + \u00b7\u00b7\u00b7 + P1/2(1).", "metadata": {"page": 76}}, {"page_content": "The latter sum gives the probability of seeing a number of heads appear that is at least as far from the mean, 50, as is\n60. My calculator \ufb01nds that this sum is 0.057. Thus, the P-value of 60 is greater than 0.05 and so I hesitate to reject\nthe hypothesis that the coin is fair.\nAn upper bound for the P-value that uses only the mean and standard deviation of a probability function can be had", "metadata": {"page": 76}}, {"page_content": "using the Chebychev theorem in Chapter 11. As you should recall, this theorem asserts that the probability of \ufb01nding\na measurement with distance R\u03c3from the mean is less than R\u22122. Here, \u03c3denotes the standard deviation of the given\nprobability function. Granted this, a measurement that differs from the mean by 5\u03c3or more has probability less than\n0.04 and so has a signi\ufb01cant P-value. Such being the case, the 5\u03c3bound is often used in lieu of the 0.05 bound.", "metadata": {"page": 76}}, {"page_content": "To return to our binomial case, to say that mdiffers from the mean, Nq, by at least 5\u03c3, is to say that\n|m\u2212Nq|\u2265 5(Nq(1 \u2212q))1/2. (12.3)\nWe would consider qto be a \u201cbad\u201d choice in the case that (12.3) holds.\nIn the case of my coin \ufb02ip example, with N = 100 and q = 1\n2 , the standard deviation is 5. Thus, 60 heads is only 2\nstandard deviations from the mean, which is less than the 5\u03c3criterion used above.\nWhen using the Chebychev theorem to get an upper bound for a P-value, remember the following:", "metadata": {"page": 76}}, {"page_content": "When using the Chebychev theorem to get an upper bound for a P-value, remember the following:\n12.2. P-value and bad choices 73", "metadata": {"page": 76}}, {"page_content": "If an outcome differs from the mean by 5\u03c3or more, then it\u2019sP-value is necessarily smaller than 0.05.\nThe converse of this last statement is not true. An event can have P-value that is smaller than 0.05 yet differ from the\nmean by less than 5\u03c3. Take for example the case where I \ufb02ip a coin 100 times and now see 61 heads instead of 60.\nUnder the assumption that the coin is fair, the P-value of 61 heads is 0.0452. This is less than 0.05. However, 61 is\n2.2\u03c3from the mean.", "metadata": {"page": 77}}, {"page_content": "2.2\u03c3from the mean.\n12.3 A binomial example using DNA\nAs you may recall, a strand of a DNA molecule consists of a chain of smaller molecules tied end to end. Each small\nmolecule in the chain is one of four types, these labeled A, T, G and C. Suppose we see that G appears some ntimes\non some length N strand of DNA. Should this be considered unusual?\nTo make this question precise, we have to decide how to quantify the term unusual, and this means choosing a proba-", "metadata": {"page": 77}}, {"page_content": "bility function for the sample space whose elements consist of all lengthN strings of letters, where each letter is either\nA, C, G or T. For example, the assumption that the appearances of any given molecule on the DNA strand are occurring\nat random suggests that we take the probability of any given letter A, C, G or T appearing at any given position to\nbe1\n4 . Thus, the probability that G does not appear at any given location is 3\n4 , and so the probability that there are n", "metadata": {"page": 77}}, {"page_content": "4 , and so the probability that there are n\nappearances of G in a length N string (if our random model is correct) would be given by the q = 3\n4 version of the\nbinomial function in equation (12.1).\nThis information by itself is not too useful. A more useful way to measure whether nappearances of G is unusual is\nto ask for the probability in our q = 1\n4 binomial model for more (or less) appearances of G to occur. This is to say", "metadata": {"page": 77}}, {"page_content": "4 binomial model for more (or less) appearances of G to occur. This is to say\nthat if we think that there are too many G\u2019s for the appearance to be random then we should consider the probability\nas determined by our q = 1\n4 binomial function of there being at least this many G\u2019s appearing. Thus, we should be\ncomputing the P-value of the measured number, n. In the binomial case with q= 1\n4 , this means computing\n\u2211\nk\u2208B\nN!\nk!(N \u2212k)!\n(1\n4\n)k(3\n4\n)N\u2212k\n, (12.4)", "metadata": {"page": 77}}, {"page_content": "4 , this means computing\n\u2211\nk\u2208B\nN!\nk!(N \u2212k)!\n(1\n4\n)k(3\n4\n)N\u2212k\n, (12.4)\nwhere the sum is over all integers kfrom the set, B, of integers in {0,...,N }that obey |b\u2212N\n4 |\u2265|n \u2212N\n4 |.\nSuppose, for instance that we have a strand of N = 20 bases and see 10 appearances of G. Does this suggest that our\nhypothesis is incorrect about G\u2019s appearance being random in the sense just de\ufb01ned? To answer this question, I would", "metadata": {"page": 77}}, {"page_content": "compute theP-value of 10. This means computing the sum in (12.4) with N = 20 and Bthe subset in {0,..., 20}of\nintegers bwith either b= 0 or b\u226510. As it turns out, this sum is close to 0.017. The P-value is less than 0.05 and so\nwe are told to doubt the hypothesis that the G appears at random.\nAs the sum in (12.4) can be dif\ufb01cult to compute in any given case, one can often make due with the upper bound for the", "metadata": {"page": 77}}, {"page_content": "P-value that is obtained from the Chebychev theorem. In this regard, you should remember the Chebychev theorem\u2019s\nassertion that the probability of being Rstandard deviations from the mean is less than R\u22122. Thus, Chebychev says\nthat a measurement throws signi\ufb01cant doubt on a hypothesis when it is at5 or more standard deviations from the mean.\nIn the case under consideration in (12.4), the standard deviation, \u03c3, is\n\u221a\n3N\n4 . In this case, the Chebychev theorem says", "metadata": {"page": 77}}, {"page_content": "\u221a\n3N\n4 . In this case, the Chebychev theorem says\nthat the set of integers bthat obey |b\u2212N\n4 |> R\n\u221a\n3N\n4 has probability less than R\u22122. Taking R = 5, we see that a\nvalue for nhas P-value less that 0.05 if it obeys |n\u2212N\n4 |\u2265 5\n4\n\u221a\n3N. This result in the DNA example can be framed\nas follows: The measured fraction, n\nN, of occurrences of G has signi\ufb01cant P-value in our q= 1\n4 random model if\n\u23d0\u23d0\u23d0\u23d0\nn\nN \u22121\n4\n\u23d0\n\u23d0\u23d0\u23d0> 5\n4\n\u221a\n3\nN. (12.5)", "metadata": {"page": 77}}, {"page_content": "4 random model if\n\u23d0\u23d0\u23d0\u23d0\nn\nN \u22121\n4\n\u23d0\n\u23d0\u23d0\u23d0> 5\n4\n\u221a\n3\nN. (12.5)\nYou should note here that as N gets bigger, the right-hand side of this last inequality gets smaller. Thus, as N gets\nbigger, the experiment must \ufb01nd the ratio n\nN ever closer to 1\n4 so as to forestall the death of our hypothesis about the\nrandom occurrences of the constituent molecules on the DNA strand.\n74 Chapter 12. P-values", "metadata": {"page": 77}}, {"page_content": "As I wrote earlier, the Chebychev theorem gives only an upper bound for the P-value. Indeed, in the example where\nN = 20 and n= 10, the actual P-value was found to be 0.017. Even so, the inequality in (12.5) is not obeyed since\nthe left-hand side is 1\n4 and the right-hand side is roughly 0.48. Thus, the Chebychev upper bound for the P-value is\nlarger than 0.05 even though the true P-value is less than 0.05.\n12.4 An example using the Poisson function", "metadata": {"page": 78}}, {"page_content": "12.4 An example using the Poisson function\nAll versions of the Poisson probability function are de\ufb01ned on the set of non-negative integers, N = {0,1,2,... }.\nAs noted in the previous chapter, a particular version is determined by a choice of a positive number, \u03c4. The Poisson\nprobability for the given value of \u03c4 is:\nP\u03c4(n) = 1\nn!\u03c4ne\u2212\u03c4 (12.6)\nHere is a suggested way to think about P\u03c4:\nP\u03c4(n) gives the probability of seeing n occurrences of a particular event in any given unit", "metadata": {"page": 78}}, {"page_content": "P\u03c4(n) gives the probability of seeing n occurrences of a particular event in any given unit\ntime interval when the occurrences are unrelated and they average \u03c4 per unit time. (12.7)\nWhat follows is an example that doesn\u2019t come from biology but is none-the-less dear to my heart. I like to go star\ngazing, and over the years, I have noted an average of 1 meteor per night. Tonight I go out and see 5 meteors. Is", "metadata": {"page": 78}}, {"page_content": "this unexpected given the hypothesis that the appearances of any two meteors are unrelated? To test this hypothesis,\nI should compute the P-value of n = 5 using the \u03c4 = 1 version of (12.6). Since the mean of P\u03c4 is \u03c4, this involves\ncomputing \uf8eb\n\uf8ed\u2211\nm\u22655\n1\nm!\n\uf8f6\n\uf8f8e\u22121 = 1 \u2212\n(\n1 + 1 + 1\n2 + 1\n6 + 1\n24\n)\ne\u22121 (12.8)\nMy trusty computer can compute this, and I \ufb01nd thatP(5) \u22640.004. Thus, my hypothesis of the unrelated and random\noccurrence of meteors is unlikely to be true.", "metadata": {"page": 78}}, {"page_content": "occurrence of meteors is unlikely to be true.\nWhat follows is an example from biology, this very relevant to the theory behind the \u201cgenetic clocks\u201d that predict\nthe divergence of modern humans from an African ancestor some 100,000 years ago. I start the story with a brief\nsummary of the notion of a point mutation of a DNA molecule. This occurs as the molecule is copied for reproduction", "metadata": {"page": 78}}, {"page_content": "when a cell divides; it involves the change of one letter in one place on the DNA string. Such changes, cellular\ntypographical errors, occur with very low frequency under non-stressful conditions. Environmental stresses tend to\nincrease the frequency of such mutations. In any event, under normal circumstances, the average point mutation rate\nper site on a DNA strand, per generation has been determined via experiments. Let \u00b5denote the latter. The average", "metadata": {"page": 78}}, {"page_content": "number of point mutations per generation on a segment of DNA with N sites on it is thus \u00b5N. In T \u22651 generations,\nthe average number of mutations in this N-site strand is thus \u00b5NT.\nNow, make the following assumptions:\n\u2022The occurrence of any one mutation on the givenN-site strand has no bearing on the\noccurrence of another.\n\u2022Environmental stresses are no different now than in the past,\n\u2022The strand in question can be mutated at will with no effect on the organism\u2019s repro-\nductive success.\n(12.9)", "metadata": {"page": 78}}, {"page_content": "ductive success.\n(12.9)\nGranted the latter, the probability of seeing nmutations in T generations on this N-site strand of DNA is given by the\n\u03c4 = \u00b5NT version of the Poisson probability:\n1\nn!(\u00b5NT)ne\u00b5NT. (12.10)\n12.4. An example using the Poisson function 75", "metadata": {"page": 78}}, {"page_content": "The genetic clock idea exploits this formula in the following manner: Suppose that two closely related species diverged\nfrom a common ancestor some unknown number of generations in the past. This is the number we want to estimate.\nCall itR. Today, a comparison of theN site strand of DNA in the two organisms \ufb01nds that they differ by mutations at\nnsites. The observed mutations have arisen over the course of T = 2Rgenerations. That is, there are Rgenerations", "metadata": {"page": 79}}, {"page_content": "worth of mutations in the one species and Rin the other, so 2Rin all. We next say that Ris a reasonable guess if\nthe \u03c4 = \u00b5N(2R) version of the Poisson function gives n any P-value that is greater than 0.05. For this purpose,\nremember that the mean of the \u03c4 version of the Poisson probability function is \u03c4.\nWe might also just look for the values ofRthat make nwithin 5 standard deviations of the mean for the\u03c4 = \u00b5N(2R)", "metadata": {"page": 79}}, {"page_content": "version of the Poisson probability. Since the square of the standard deviation of the\u03c4version of the Poisson probability\nfunction is also \u03c4, this is equivalent to the demand that |2\u00b5NR \u2212n|\u2264 5\u221a\n2\u00b5NR. This last gives the bounds\n2n+ 25 \u22125\u221a2n+ 25\n4\u00b5N \u2264R\u22642n+ 25 + 5\u221a2n+ 25\n4\u00b5N . (12.11)\n12.5 Another Poisson example\nThere is a well known 1998 movie calledA Civil Actionthat tells a \ufb01ctionalized account of a true story. The movie stars", "metadata": {"page": 79}}, {"page_content": "John Travolta and Robert Duvall. Here is a quote about the movie and the case from www.civil-action.com:\nIn the early 1980s, a leukemia cluster was identi\ufb01ed in the Massachusetts town of Woburn. Three\ncompanies, including W. R. Grace & Co., were accused of contaminating drinking water and causing\nillnesses. There is no question that this tragedy had a profound impact on everyone it touched, particularly\nthe families of Woburn.", "metadata": {"page": 79}}, {"page_content": "the families of Woburn.\nJohn Travolta and Robert Duvall play the roles of the lawyers for the folks in Woburn who brought forth the civil suit\nagainst the companies.\nHere are the numbers involved: Over twenty years, there were 20 cases of childhood leukemia in Woburn. On\naverage, there are 3,500 children of the relevant ages in Woburn each year, so we are talking about 20 cases per\n3,500 \u00d720 = 70,000 person-years. Given these numbers, there are two questions to ask:", "metadata": {"page": 79}}, {"page_content": "3,500 \u00d720 = 70,000 person-years. Given these numbers, there are two questions to ask:\n(a) Is the number of cases, 20, so high as to render it very likely that the cases are not random occurrences, but do\nto some underlying cause?\n(b) If the answer to (a) is yes, then are the leukemias caused by pollution from the companies that are named in the\nsuit?\nWe can\u2019t say much about question (b), but statistics can help us with question (a). This is done by testing the sig-", "metadata": {"page": 79}}, {"page_content": "ni\ufb01cance of the hypothesis that this large number of cases is a random event. For this purpose, note that the sort of\nleukemia that is involved here occurs with an expected count of13 per 100,000 person-years which is 9.1 per 70,000\nyears. Thus, we need to ask whether 20 per 70,000 person-years is signi\ufb01cant. If the probabilities here are well\nmodeled by a Poisson probability then the probability of seeing n\u2208{0,1,... }cases per 100,000 person-years is\n1\nn!(9.1)ne\u22129.1. (12.12)", "metadata": {"page": 79}}, {"page_content": "1\nn!(9.1)ne\u22129.1. (12.12)\nIn particular, of interest here is the P-value of 20. This is the probability of being 10.9 or more from the mean. The\nmean of this Poisson function is9.1 and the standard deviation is\n\u221a\n9.1 \u22483. Thus, 20 is roughly 4 standard deviations,\nwhich isn\u2019t enough to use the Chebychev theorem. This being the case, I can still try to compute the P-value directly\nusing its de\ufb01nition as the probability of getting at least as far from the mean as is 20. This is to say that", "metadata": {"page": 79}}, {"page_content": "P-value(20) =\n\u2211\nn\u226520\n1\nn!(9.1)ne\u22129.1 \u22480.0012. (12.13)\nThis number is less than 0.05, so indicates a signi\ufb01cant P-value.\n76 Chapter 12. P-values", "metadata": {"page": 79}}, {"page_content": "Now, if I were an advocate for the companies that are involved in the suit, I would assert that thisP-value is irrelevant,\nand for the following reason: Looking at the web site www.citypopulation.de/USA-Massachusetts.\nhtml, I see that there are 44 towns and cities in Massachusetts with population 30,000 or greater. (Woburn has\na population of roughly 37,000). I would say that the relevant statistic is not the the leukemia P-value 0.0012 for", "metadata": {"page": 80}}, {"page_content": "Woburn, but the P-value for at least one of the 44 towns or cities to have its corresponding leukemia P-value either\n0.0012 or less.\nThe point here is that given any suf\ufb01ciently large list of towns, random chance will \ufb01nd one (or more) with enough\ncases of childhood leukemia to give it a leukemia P-value less than or equal to 0.0012. An unethical lawyer could\nscour the leukemia records of all towns in Massachusetts (or the US!) so as to \ufb01nd one with a very small leukemia", "metadata": {"page": 80}}, {"page_content": "P-value. That lawyer might then try to convince the folks in that town to pay the lawyer to sue those local companies\nthat could conceivably pollute the water supply.\nAnyway, to see how to compute the P-value for at least one of 44 towns or cities to have leukemia P-value 0.0012,\nnote that this number is identical to the probability of getting at least 1 tail when a coin is \ufb02ipped 44 times, and where", "metadata": {"page": 80}}, {"page_content": "the probability of tails is 0.0012. To compute this probability, remark to start that it is 1 minus the probability of\ngetting all heads. The probability of all heads is(0.9988)44 \u22480.9485. Thus, the probability of at least 1 tail is 0.0515.\nThis is barely bigger than our 0.05 cut-off for a signi\ufb01cant P-value. This being the case, you can imagine that the\nlawyers might have brought to court rival \u201cexpert statistician witnesses\u201d to argue for or against signi\ufb01cance. I don\u2019t", "metadata": {"page": 80}}, {"page_content": "know if they did or not. You can read more about the actual events at: http://lib.law.washington.edu/\nref/civilaction.htm.\n12.6 A silly example\nWhat follows is a challenge of sorts: Morning after morning, you have woken and seen that daylight invariably arrives.\nThus, morning after morning, you have experimental evidence that the sun exists. Based on this, your own experience,\ngive a lower bound for the probability that the sun will exist tomorrow.", "metadata": {"page": 80}}, {"page_content": "give a lower bound for the probability that the sun will exist tomorrow.\nHow to answer this question? Here is one way: Suppose that there is some probability, \u03c4 \u2208[0,1], for the sun to exist\non any given day. If you are 20 years old, then you have seen the sun has existed on roughly 20 \u00d7365 \u22487730 days\nin a row. Now, suppose that I have a coin with probability \u03c4 for heads and so (1 \u2212\u03c4) for tails. What is the smallest", "metadata": {"page": 80}}, {"page_content": "value for \u03c4 such that the event of \ufb02ipping the coin 7730 times and getting all heads has P-value at least? I will take\nthis smallest \u03c4 for a lower bound on the probability of sun existing tomorrow.\nTo calculate this P-value, I imagine \ufb02ipping this coin until the \ufb01rst tails appears. The sample space for this problem is\nS = {0,1,2,... }, this the set of non-negative integers. The relevant probability function on Sis given by\nP(n) = (1 \u2212\u03c4)\u03c4n. (12.14)", "metadata": {"page": 80}}, {"page_content": "P(n) = (1 \u2212\u03c4)\u03c4n. (12.14)\nDo you see why (12.14) gives the probability? Here is why: The probability for the \ufb01rst \ufb02ip to land heads is \u03c4, that\nfor the \ufb01rst and second is \u03c4\u00d7\u03c4 = \u03c42, that for the \ufb01rst three to land heads is \u03c4\u00d7\u03c4\u00d7\u03c4 = \u03c43, and so on; thus you \ufb01nd\nthat the probability for the \ufb01rst nto land heads is \u03c4n. Now if there are precisely nheads in a row and then tails, the\nprobability is \u03c4n \u00d7(probability of tails) = \u03c4n(1 \u2212\u03c4). If you did Problem 2a in the previous chapter, you will have", "metadata": {"page": 80}}, {"page_content": "veri\ufb01ed that these probabilities sum to 1 as they should.\nIf you did Problem 2d in the previous chapter, you will have found that the mean of this probability function is\n\u00b5=\n\u2211\nn=0,1,2,...\nn(1 \u2212\u03c4)\u03c4n = \u03c4\n1 \u2212\u03c4. (12.15)\nNote that the mean increases when \u03c4 increases and it decreases when \u03c4 decreases.\nTo see what sort of value for \u03c4 gives a small P-value for 7730, I start by considering value of \u03c4 where the mean, \u00b5, is", "metadata": {"page": 80}}, {"page_content": "less than half of 7730. Note that I can solve (12.15) for \u03c4 in terms of \u00b5to see that this means looking for \u03c4 between 0\nand 3865\n3866 . There are two reasons why I look at these values \ufb01rst:\n\u2022If I \ufb01nd \u03c4 in this set, I needn\u2019t look further since (12.15) shows that the mean, \u00b5, when viewed as a function of\n\u03c4, increases with increasing \u03c4.\n12.6. A silly example 77", "metadata": {"page": 80}}, {"page_content": "\u2022There are no non-negative integersnthat are both less than \u00b5and further from \u00b5than 7730. This means that the\nP-value for 7730 as de\ufb01ned by \u03c4 <3865\n3866 is computed by summing the probability,(1 \u2212\u03c4)\u03c4n, for those integers\nn\u22657730.\nThis is to say that the P-value of 7730 as de\ufb01ned using any given \u03c4 <3865\n3866 is\n\u2211\nn\u22657730\n(1 \u2212\u03c4)\u03c4n = \u03c47730 \u2211\nn\u22650\n(1 \u2212\u03c4)\u03c4n = \u03c47730. (12.16)\nSince I want to \ufb01nd the smallest \u03c4 where the P-value of 7730 is 0.05, I solve\n\u03c47730 = 0.05 (12.17)", "metadata": {"page": 81}}, {"page_content": "Since I want to \ufb01nd the smallest \u03c4 where the P-value of 7730 is 0.05, I solve\n\u03c47730 = 0.05 (12.17)\nwhich is to say \u03c4 = (0.05)1/7730 \u22480.9996. Note that this is less than 3865\n3866 \u22480.9997.\nThus, if you are 20 years old, you can say, based on your personal experience, that the probability of the sun existing\ntomorrow is likely to be greater than 0.9996.\n12.7 Exercises:\n1. De\ufb01ne a probability function, P, on {0,1,2,... }by setting P(n) = 1\n10 ( 9\n10 )n. What is the P-value of 5 using", "metadata": {"page": 81}}, {"page_content": "10 ( 9\n10 )n. What is the P-value of 5 using\nthis probability function?\n2. Suppose we lock a monkey in a room with a word processor, come back some hours later and see that the monkey\nhas typed N lower case characters. Suppose this string of N characters contains precisely 10 occurrences of the\nletter \u201ce\u201d. The monkey\u2019s word processor key board allows48 lower case characters including the space bar.", "metadata": {"page": 81}}, {"page_content": "(a) Assume that the monkey is typing at random, and give a formula for the probability that 10 occurrences of\nthe letter \u201ce\u201d appear in theN characters.\n(b) Use the Chebychev theorem to estimate how big to take N so that 10 appearances of the letter \u201ce\u201d has\nsigni\ufb01cant P-value.\nHere is a rather more dif\ufb01cult thought problem along the same line: Suppose that you pick up a copy of Tolstoy\u2019s\nnovel War and Peace. The paperback version translated by Ann Dunnigan has roughly 1,450 pages. Suppose", "metadata": {"page": 81}}, {"page_content": "that you \ufb01nd as you read that if you take away the spacing between letters, there is an occurrence of the string of\nletters \u201cprofessortaubesisajerk\u201d. Note that spaces can be added here to make this a bona \ufb01de English sentence.\nYou might ask whether such a string is likely to occur at random in a book of 1,450 pages, or whether Tolstoy\ndiscovered something a century or so ago that you are only now coming to realize is true. (Probabilistic analysis", "metadata": {"page": 81}}, {"page_content": "of the sort introduced in this problem debunks those who claim to \ufb01nd secret, coded prophesies in the Bible and\nother ancient texts.)\n3. The Poisson probability function is often used to distinguish \u201crandom\u201d from less than random patterns. This\nproblem concerns an example that involves spatial patterns. The issue concerns the distribution of appearances\nof the letter \u201ce\u201d in a piece of writing. To this end, obtain two full page length columns of text from a Boston\nGlobe newspaper.", "metadata": {"page": 81}}, {"page_content": "Globe newspaper.\n(a) Draw a histogram on a sheet of graph paper whose bins are labeled by the nonnegative integers, n =\n0,1,... . Make the height of the bin numbered by any given integer nequal to the number of lines in your\ntwo newspaper columns that have nappearances of the letter \u201ce\u201d.\n(b) Compute the total number appearances of the letter \u201ce\u201d, and divide the latter number by the total number of", "metadata": {"page": 81}}, {"page_content": "lines in your two newspaper columns. Call this number \u03c4. Plot on your graph paper (in a different color),\na second histogram where the height of the nth bin is the function P\u03c4(n) as depicted in (12.6).\n(c) Compute the standard deviation of your observed data as de\ufb01ned in (1.3) of Chapter 1 and give the ratio\nof this observed standard deviation to the standard deviation, \u221a\n\u03c4, for P\u03c4(n).\n78 Chapter 12. P-values", "metadata": {"page": 81}}, {"page_content": "(d) Count the number of lines of your two newspaper columns where the number of appearance of the letter\n\u201ce\u201d differs from \u03c4 by more than 5\u221a\u03c4. Is the number of such columns more or less than 5% of the total\nnumber of lines?\n4. This problem concerns a very standard epidemiology application of P-value. If you go to work for the Center\nfor Disease Control, then you may well see a lot of applications of the sort that are considered here. Creutzfeld-", "metadata": {"page": 82}}, {"page_content": "Jacob disease is a degenerative brain disease with symptoms very much like that of mad cow disease. It is not\nknown how the disease is propagated, and one conjecture is that arises spontaneously due to chance protein\nmisfolding. Suppose that this is true, and that the average number of cases seen in England per year is10.\nSuppose that there were 15 cases seen last year in England. Does appearance of so many cases in one year shed", "metadata": {"page": 82}}, {"page_content": "signi\ufb01cant doubt on the hypothesis that the disease arises spontaneously?\n(a) Use a calculator to compute the actual P-value of 15 under the assumption that the probability of seeing\nncases in one year in England is determined by the \u03c4 = 10 version of the Poisson probability function.\nIn this regard, it is almost surely easier to \ufb01rst compute the probability of {6,7,..., 14}and then subtract\nthe latter number from 1. However, if you do the computation in this way, you have to explain why the", "metadata": {"page": 82}}, {"page_content": "resulting number is the P-value of 15.\n(b) Compute the Chebychev theorem\u2019s upper bound for the P-value of 15.\n5. Suppose that cells from the skin are grown in low light conditions, and it is found that under these circum-\nstances, there is some probability, p, for any given cell to exhibit some chromosome abnormality. Granted this,\nsuppose that 100 skin cells are taken at random from an individual and 2 are found to have some chromosome", "metadata": {"page": 82}}, {"page_content": "abnormality. How small must pbe to deem this number signi\ufb01cant?\n6. This problem returns to the World Series data that is given in Problem 8 of Chapter 11. The question here is\nthis: Does the data make it unlikely that, on average, each team in the World Series has a 50% probability of\nwinning any given game. To answer this question,\n(a) Use the actual data to prove that the average number of games played in the World Series is 5.8.", "metadata": {"page": 82}}, {"page_content": "(b) Use the probabilities given in Problem 8(b)\u2013(e) of Chapter 11 to justify the following:\nIf each team has a 50-50 chance of winning any given game, then the mean of the number of\ngames played is 5.8125.\nThus, the average numbers of games are almost identical.\n(c) Here is another approach: If the probability of a 7-game series is 5\n16 , one can ask for the P-value of the\nfact that there were 33 of them in 83 years. Explain why this question can be answered by studying the", "metadata": {"page": 82}}, {"page_content": "binomial probability function on the sample space {0,..., 83}with q= 5\n16 . In particular, explain why the\nP-value of 33 is\n\u2211\nn=33,34,...,83\n(83\nn\n)( 5\n16\n)n(11\n16\n)83\u2212n\n+\n\u2211\nn=0,1,...,19\n(83\nn\n)( 5\n16\n)n(11\n16\n)83\u2212n\n.\nAs it turns out, this number is roughly 0.12 which is not signi\ufb01cant.\n(d) There are other statistics to try. For example, lets agree on the following terminology: A short series is\none that lasts 4 or 5 games, and a long series is one that lasts 6 or 7 games.", "metadata": {"page": 82}}, {"page_content": "one that lasts 4 or 5 games, and a long series is one that lasts 6 or 7 games.\n(i) Under the assumption that each team has a 50-50 chance of winning any given game, explain why the\nprobability of a short series is 3\n8 , and so that of a long series is 5\n8 .\n(ii) Explain why the expected number of short series over 83 years is 31.125.\n(iii) The real data \ufb01nds 32 short series. Do you think this has a signi\ufb01cant P-value? Explain why the", "metadata": {"page": 82}}, {"page_content": "probability of having n\u2208{0,1,..., 83}short series in 83 years is\n(83\nn\n)(3\n8\n)n(5\n8\n)83\u2212n\n.\n12.7. Exercises 79", "metadata": {"page": 82}}, {"page_content": "CHAPTER\nTHIRTEEN\nContinuous probability functions\nSo far, we have only considered probability functions on \ufb01nite sets and on the set of non-negative integers. The task\nfor in this chapter is to introduce probability functions on the whole real line, R, or on a subinterval in R such as the\ninterval where 0 \u2264x\u22641. Let me start with an example to motivate why such a de\ufb01nition is needed.\n13.1 An example", "metadata": {"page": 83}}, {"page_content": "13.1 An example\nSuppose that we have a parameter that we can vary in an experiment, say the concentration of sugar in an airtight,\nenclosed Petri dish with photosynthesizing bacteria. Varying the initial sugar concentration, we measure the amount\nof oxygen produced after one day. Letxdenote the sugar concentration andythe amount of oxygen produced. We run\nsome large number, N, of versions of this experiment with respective sugar concentrations x1,...,x N and measure", "metadata": {"page": 83}}, {"page_content": "corresponding oxygen concentrations, y1,...,y N.\nSuppose that we expect, on theoretical grounds, a relation of the form y = cx+ dto hold. In order to determine the\nconstants cand d, we \ufb01nd the least squares \ufb01t to the data {(xj,yj)}1\u2264j\u2264N.\nNow, the differences,\n\u22061 = y1 \u2212cx1 \u2212d, \u22062 = y2 \u2212cx2 \u2212d, etc (13.1)\nbetween the actual measurements and the least squares measurements should not be ignored. Indeed, these differences", "metadata": {"page": 83}}, {"page_content": "might well carry information. Of course, you might expect them to be spread \u201crandomly\u201d on either side of0, but then\nwhat does it mean for a suite of real numbers to be random? More generally, how can we decide if their distribution\non the real line carries information?\n13.2 Continuous probability functions\nAs the example just given illustrates, a notion of \u201crandom\u201d is needed for drawing numbers from some interval of", "metadata": {"page": 83}}, {"page_content": "real numbers. This means introducing probability functions for sample spaces that are not \ufb01nite or discrete sets.\nWhat follows is the de\ufb01nition when the sample space is a bounded or unbounded in interval,[a,b] \u2282R where \u2212\u221e\n\u2264a < b\u2264\u221e. A continuous probability function on such a sample space is any function, x\u2192p(x), that is de\ufb01ned\nfor those points xin the interval, and that obeys the following two conditions:\n\u2022p(x) \u22650 for all x\u2208[a,b].\n\u2022\n\u222b b\na\np(x) dx= 1.\n(13.2)\n80", "metadata": {"page": 83}}, {"page_content": "I\u2019ll say more about these points momentarily. Here is how you are supposed to use p(x) to \ufb01nd probabilities: If\nU \u2282[a,b] is any given subset, then \u222b\nx\u2208U\np(x) dx (13.3)\nis meant to give the probability of \ufb01nding the pointxin the subset U. Granted this use of p(x), then the \ufb01rst constraint\nin (13.2) forbids negative probabilities; meanwhile, the second guarantees that there is probability 1 of \ufb01nding x\nsomewhere in the given interval [a,b].", "metadata": {"page": 84}}, {"page_content": "somewhere in the given interval [a,b].\nA continuous probability function is often called a \u201cprobability distribution\u201d since it signi\ufb01es how probabilities are\ndistributed over the relevant portion of the real line. Note in this regard, that people often refer to the \u201ccumulative\ndistribution function\u201d. This function is the anti-derivative ofp(x). It is often denoted as P(x) and is de\ufb01ned by\nP(x) =\n\u222b x\na\np(s) ds. (13.4)", "metadata": {"page": 84}}, {"page_content": "P(x) =\n\u222b x\na\np(s) ds. (13.4)\nThus, P(a) is zero, P(b) is one, and P\u2032(x) = p(x). In this regard, P(x) is the probability that passigns to the interval\n[a,x]. It is the probability of \ufb01nding a point that is less than the given point x.\nThe functions p(x) = 1, p(x) = 2x, p(x) = 2(x \u2212x2) and p(x) = sin( \u03c0x) are all probability functions for the\ninterval [0,1].\n13.3 The mean and standard deviation", "metadata": {"page": 84}}, {"page_content": "interval [0,1].\n13.3 The mean and standard deviation\nA continuous probability function for an interval on the real line can have amean and a standard deviation. The mean,\n\u00b5, is\n\u00b5=\n\u222b b\na\nxp(x) dx. (13.5)\nThis is the \u201caverage\u201d value of xwhere p(x) determines the meaning of average. The standard deviation, \u03c3, has its\nsquare given by\n\u03c32 =\n\u222b b\na\n(x\u2212\u00b5)2p(x) dx. (13.6)\nNote that in the case that |a|or bis in\ufb01nite, one must worry a bit about whether the integrals actually converge. We", "metadata": {"page": 84}}, {"page_content": "won\u2019t be studying examples in this course where this is an issue.\nBy the way, novices in probability theory often forget to put the factor of p(x) into the integrands when they compute\nthe mean or standard deviation. Many of you will make this mistake at some point.\n13.4 The Chebychev theorem\nAs with the probability functions studied previously, there is a \ufb01xation on the mean and standard deviation that is\njusti\ufb01ed by a version of Chapter 11\u2019s Chebychev theorem:", "metadata": {"page": 84}}, {"page_content": "justi\ufb01ed by a version of Chapter 11\u2019s Chebychev theorem:\nTheorem 1. (Chebychev Theorem) Let x \u2192p(x) denote a probability function on the interval [a,b] where |a|or |b|\ncan be \ufb01nite or in\ufb01nite. Suppose now that R \u22651. Then, the probability as de\ufb01ned by p(x) for the points x with\n|x\u2212\u00b5|\u2265 R\u03c3is no greater than 1\nR2 .\nNote that this theorem holds for any p(x) as long as both \u00b5and \u03c3 are de\ufb01ned. Thus, the two numbers \u00b5and \u03c3 give", "metadata": {"page": 84}}, {"page_content": "you enough information to obtain upper bounds for probabilities without knowing anything more about p(x).\nThe Chebychev theorem justi\ufb01es the ubiquitous focus on means and standard deviations.\n13.3. The mean and standard deviation 81", "metadata": {"page": 84}}, {"page_content": "13.5 Examples of probability functions\nThree examples of continuous probability functions appear regularly in the scienti\ufb01c literature.\nThe uniform probabilities: The simplest of the three is the uniform probability function on some \ufb01nite interval.\nThus, aand bmust be \ufb01nite. In this case,\np(x) = 1\nb\u2212a. (13.7)\nThis probability function asserts that the probability of \ufb01nding xin an interval of length L<b \u2212ainside the interval\n[a,b] is equal to L\nb\u2212a; thus it is proportional to L.", "metadata": {"page": 85}}, {"page_content": "[a,b] is equal to L\nb\u2212a; thus it is proportional to L.\nHere is an example where this case can arise: Suppose we postulate that bacteria in a petri dish can not sense the\ndirection of the source of a particular substance. We might then imagine that the orientation of the axis of the bacteria\nwith respect to thexy-coordinate system in the plane of the petri dish should be \u201crandom\u201d. This is to say that the head", "metadata": {"page": 85}}, {"page_content": "end of a bacteria is pointed at some angle, \u03b8\u2208[0,2\u03c0], and we expect that the particular angle for any given bacteria is\n\u201crandom\u201d. Should we have a lot of bacteria in our dish, this hypothesis implies that we must \ufb01nd that the percent of\nthem with head pointed between angles0 \u2264\u03b1<\u03b2 \u22642\u03c0is equal to \u03b2\u2212\u03b1\n2\u03c0 .\nThe mean and standard deviation for the uniform probability function are\n\u00b5= 1\n2(b+ a) and \u03c3= b\u2212a\u221a\n12 . (13.8)", "metadata": {"page": 85}}, {"page_content": "\u00b5= 1\n2(b+ a) and \u03c3= b\u2212a\u221a\n12 . (13.8)\nIn this regard, note that the mean is the midpoint of the interval [a,b] (are you surprised?). For example, the uniform\nprobability distribution on the interval [0,1] has mean 1\n2 and standard deviation 1\u221a\n12 .\nThe Gaussian probabilities: These are probability functions on the whole of R. Any particular version is deter-\nmined with the speci\ufb01cation of two parameters, \u00b5and \u03c3. Here, \u00b5can be any real number, but\u03c3must be a positive real", "metadata": {"page": 85}}, {"page_content": "number. The (\u00b5,\u03c3) version the Gaussian probability function is\np(x) = 1\u221a\n2\u03c0 \u03c3e\u2212|x\u2212\u00b5|2/(2\u03c32). (13.9)\nIf you have a graphing calculator and graph this function for some numerical choices of \u00b5and \u03c3, you will see that\nthe graph is the famous \u201cbell-shaped\u201d curve, but centered at the point \u00b5and with the width of the bell given by \u03c3. In\nfact, \u00b5is the mean of this probability function and \u03c3is its standard deviation. Thus, small \u03c3signi\ufb01es that most of the", "metadata": {"page": 85}}, {"page_content": "probability is concentrated at points very close to \u00b5. Large \u03c3signi\ufb01es that the probability is spread out.\nThere is a theorem called the \u201cCentral Limit Theorem\u201d that explains why the Gaussian probability function appears as\noften as it does. This is a fantastically important theorem that is discussed momentarily.\nThe exponential probabilities: These are de\ufb01ned on the half line [0,\u221e). There are various versions and the", "metadata": {"page": 85}}, {"page_content": "speci\ufb01cation of any one version is determined by the choice of a positive real number, \u00b5. With \u00b5chosen,\np(t) = 1\n\u00b5e\u2212t/\u00b5. (13.10)\nThis one arises in the following context: Suppose that you are waiting for some particular \u201cthing\u201d to happen and you\nknow the following:\n\u2022On average, you will have to wait for \u00b5minutes.\n\u2022The conditional probability that the \u201cthing\u201d occurs at times greater than tgiven that\nit has not happened after some previous t\u2032depends only on the elapsed time, t\u2212t\u2032.\n(13.11)", "metadata": {"page": 85}}, {"page_content": "it has not happened after some previous t\u2032depends only on the elapsed time, t\u2212t\u2032.\n(13.11)\n82 Chapter 13. Continuous probability functions", "metadata": {"page": 85}}, {"page_content": "If 0 \u2264a<b \u2264\u221e, you can ask for the probability that the \u201cthing\u201d occurs when a\u2264t<b. This probability is given\nby integrating p(t) in (13.10) over the interval where a\u2264t<b. Thus, it is e\u2212a/\u00b5 \u2212e\u2212b/\u00b5.\nThe mean of the exponential is \u00b5and the standard deviation is also equal to \u00b5.\nTo illustrate when you might use this probability function, suppose that you are waiting for a bus, and you know that", "metadata": {"page": 86}}, {"page_content": "the mean waiting time is 10 minutes. You have been waiting for 6 minutes, so you know that no bus has appeared\nduring times t\u2032 \u22646. You want to know the probability that you will have to wait at least 6 more minutes for the\nbus. You would use the \u00b5 = 10 version of the exponential probability function to compute this probability if you\nmake the following assumption about buses: Let Adenote the event that the bus appears after time t. Supposing that", "metadata": {"page": 86}}, {"page_content": "t\u2032< t, letB denote the event that the bus appears after time t\u2032. Then the conditional probability of Agiven B, thus\nP (A|B), is depends only on the t\u2212t\u2032. Granted this assumption, then the probability for waiting at least 6 more\nminutes is P (A|B) as computed for the case that A = [12,\u221e) and B = [6,\u221e). Compute this number using the\nformula P (A|B) = P(A\u2229B)\nP(B) . For this purpose, note that A\u2229B = [12,\u221e), so P(A \u2229B) is obtained by integrating", "metadata": {"page": 86}}, {"page_content": "P(B) . For this purpose, note that A\u2229B = [12,\u221e), so P(A \u2229B) is obtained by integrating\nthe function e\u2212t/10 from 12 to \u221e. The result is e\u22126/5. Meanwhile, P(B) is obtained by integrating this same function\nfrom 6 to \u221e. The result is e\u22123/5. Thus, the probability of waiting at least 6 more minutes for a bus is e\u22123/5 \u22480.55.\n13.6 The Central Limit Theorem: Version 1\nThe Central Limit Theorem explains why the \u201cbell shaped\u201d curve arises in so many different contexts. Here is a typical", "metadata": {"page": 86}}, {"page_content": "situation: You do the same experiment some large number of times, each time measuring some given quantity. The\nresult is a suite ofN numbers, {x1,...,x N}. Suppose now that we look at the average of the N measurements,\nx= 1\nN (x1 + \u00b7\u00b7\u00b7 + xN) . (13.12)\nEven if the xks have only a \ufb01nite set of possible values, the set of possible values ofxbecomes ever larger asN \u2192\u221e.\nThe question one might ask is what is the probability of x having any given value? More to the point, what is the", "metadata": {"page": 86}}, {"page_content": "probability function for the possible values of x?\nThe Central Limit Theorem answers this question when the following assumption is valid: There is some probability\nfunction,p, on the set of possible values for any given xk, and it is the same for each k. The Central Limit Theorem\nalso assumes that the values that are measured in any subset of the N experiments have no bearing on the values that", "metadata": {"page": 86}}, {"page_content": "are measured for the remaining experiments. The Central Limit Theorem then asserts that the probability function\nfor the possible values of the average,\nx, depends only on the mean and standard deviation of the probability function\np. The detailed ups and downs of pare of no consequence, only its mean, \u00b5, and standard deviation, \u03c3. Here is the\ntheorem:\nCentral Limit Theorem. Under the assumptions just stated, the probability that the value of x is in some given", "metadata": {"page": 86}}, {"page_content": "interval [a,b] is well-approximated by\n\u222b b\na\n1\u221a\n2\u03c0 \u03c3N\ne\u2212|x\u2212\u00b5|2/(2\u03c32\nN) dx (13.13)\nwhere \u03c3N = 1\u221a\nN\u03c3. This is to say that for very large N the probability function for the possible values of xis very\nclose to the Gaussian probability function with mean \u00b5and with standard deviation \u03c3N = 1\u221a\nN\u03c3.\nHere is an example: Suppose a coin is \ufb02ipped some N times. For any given k \u2208{1,2,...,N }, let xk = 1 if the kth", "metadata": {"page": 86}}, {"page_content": "\ufb02ip is heads, and let xk = 0 if it is tails. Let xdenote the average of these values, this as de\ufb01ned via (13.12). Thus,\nxcan take any value in the set {0, 1\nN, 2\nN,\u00b7\u00b7\u00b7 ,1}. According to the Central Limit Theorem, the probabilities for the\nvalues of xare, for very large N, essentially determined by the Gaussian probability function\npN(x) = 1\u221a\n2\u03c02\n\u221a\nNe\u22122N|x\u22121\n2 |2\n. (13.14)\n13.6. The Central Limit Theorem: Version 1 83", "metadata": {"page": 86}}, {"page_content": "Here is a second example: Suppose that N numbers are randomly chosen between 0 and 100 with uniform probability\nin each case. This is to say that the probability function is that given in (13.7) using b= 100 and a= 0. Let xdenote\nthe average of these N numbers. Then for very large N, the probabilities for the values ofxare are very close to those\ndetermined by the Gaussian probability function\npN(x) = 1\u221a\n2\u03c0\n\u221a\n12N\n1000 e\u22123N|x\u221250|2/5000. (13.15)", "metadata": {"page": 87}}, {"page_content": "determined by the Gaussian probability function\npN(x) = 1\u221a\n2\u03c0\n\u221a\n12N\n1000 e\u22123N|x\u221250|2/5000. (13.15)\nBy the way, here is something to keep in mind about the Central Limit Theorem: As N gets larger, the mean for the\nGaussian in (13.13) is unchanged, it is the same as that for the original probability functionpthat gives the probabilities\nfor the possible values of any given measurement. However, the standard deviation shrinks to zero in the limit that", "metadata": {"page": 87}}, {"page_content": "N \u2192\u221e since it is obtained from the standard deviation, \u03c3, of p as 1\u221a\nN\u03c3. Thus, the odds of \ufb01nding the average,\nx, some \ufb01xed distance from the mean \u00b5decreases to zero in the limit that N \u2192\u221e. The Chebychev inequality also\npredicts this. Indeed, the Chebychev inequality in this context asserts the following:\nFix a real number, r, and let pN(r) denote the probability that the average, x, of N mea-\nsurements obeys |x\u2212\u00b5|>r. Then pN(r) \u2264 1\nN\n(\u03c3\nr\n)2\nwhen N is very large. (13.16)", "metadata": {"page": 87}}, {"page_content": "surements obeys |x\u2212\u00b5|>r. Then pN(r) \u2264 1\nN\n(\u03c3\nr\n)2\nwhen N is very large. (13.16)\nThe use of (13.13) to approximate pN(r) when N is large suggests that pN(r) is much smaller than the Chebychev\nupper bound from (13.16). Indeed, the sum of the integrals that appears in (13.13) in the case a= \u00b5+ r, b= \u221eand\nin the case a= \u2212\u221eand b= \u00b5\u2212rcan be proved no greater than\n\u221a\n2\n\u03c0\n1\u221a\nN\n\u03c3\nre\u2212Nr2/(2\u03c32). (13.17)", "metadata": {"page": 87}}, {"page_content": "in the case a= \u2212\u221eand b= \u00b5\u2212rcan be proved no greater than\n\u221a\n2\n\u03c0\n1\u221a\nN\n\u03c3\nre\u2212Nr2/(2\u03c32). (13.17)\nTo derive (13.17), I am using the following fact: Suppose that both \u03baand rare positive real numbers. Then\n\u2022\n\u222b \u221e\n\u00b5+r\n1\u221a\n2\u03c0 \u03bae\u2212(x\u2212\u00b5)2/(2\u03ba2) dx\u2264 1\u221a\n2\u03c0\n\u03ba\nre\u2212r2/(2\u03ba2).\n\u2022\n\u222b \u00b5\u2212r\n\u2212\u221e\n1\u221a\n2\u03c0 \u03bae\u2212(x\u2212\u00b5)2/(2\u03ba2) dx\u2264 1\u221a\n2\u03c0\n\u03ba\nre\u2212r2/(2\u03ba2).\n(13.18)\nHere, I use \u03baas a generic stand-in for \u201c\u03c3\u201d, since we will be applying this formula in instances where \u03ba= \u03c3(such as\nin (13.17)), and others where \u03ba= \u03c3/\n\u221a", "metadata": {"page": 87}}, {"page_content": "in (13.17)), and others where \u03ba= \u03c3/\n\u221a\nN (such as when using the Central Limit Theorem). You are walked through\nthe proof of the top inequality of (13.18) in one of the exercises at the end of this chapter. The bottom inequality is\nobtained from the top by changing variables of integration from xto 2\u00b5\u2212x.\n13.7 The Central Limit Theorem: Version 2\nThere is another, even more useful version of the Central Limit Theorem that has the version just given as a special", "metadata": {"page": 87}}, {"page_content": "case. This more general version asserts that when a measurement is affected by lots of small, random perturbations,\nthe probabilities for the values of the measurement are well approximated by those that are determined via a Gaussian\nprobability function.\nFor example, consider measuring a certain quantity some large number of times, thus generating a sequence of num-\nbers, {x1,...,x N}, where xk is the result of the kth measurement. Consider the function", "metadata": {"page": 87}}, {"page_content": "bers, {x1,...,x N}, where xk is the result of the kth measurement. Consider the function\nfN(x) = fraction of measurements jwith xj <x. (13.19)\nThe possible values of fN(x) are {0, 1\nN, 2\nN,..., 1}. We are interested in the large N version of this function. What\nalmost always happens is that asN gets large, the graph of fN(x) gets closer and closer to the graph of the cumulative\n84 Chapter 13. Continuous probability functions", "metadata": {"page": 87}}, {"page_content": "distribution function for a Gaussian probability. To be explicit, the following phenomena is often observed:\nAs N \u2192\u221e, the graph of x\u2192fN(x) approaches that of the function\nx\u2192\n\u222b x\n\u2212\u221e\n1\u221a\n2\u03c0 \u03c3e\u2212(s\u2212\u00b5)2/(2\u03c3) dsfor an appropriate choice of \u00b5and \u03c3. (13.20)\nNote that the constant \u00b5to use is the N \u2192\u221e limit of the average, x= 1\nN(x1 + \u00b7\u00b7\u00b7xN).\nAnother version of the Central Limit Theorem provides a mathematical explanation for why this phenomena arises.", "metadata": {"page": 88}}, {"page_content": "As the assumptions of the version that follows are rather technical and the proof is not something we will cover, I just\ngive you the following somewhat vague statement:\nCentral Limit Theorem II. Suppose that the variations of some measurement are due to a very large number of\nsmall perturbations. If the probability of the value of any given perturbation is a function with mean zero, and if the", "metadata": {"page": 88}}, {"page_content": "collection of these probability functions are not unreasonable, then the probability for any given measurement is well\napproximated by a Gaussian probability function.\nNote that this version does not assume that the probability function for any given source of perturbation is the same as\nthat of any of the others. The term \u201cnot unreasonable\u201d means that the sum of the squares of the standard deviations for", "metadata": {"page": 88}}, {"page_content": "the probability functions of the perturbations is \ufb01nite, as is the sum of the average values ofx4 for these probability\nfunctions.\n13.8 The three most important things to remember\nThe three most important things to remember here are the following:\n\u2022A Gaussian probability function offers a good approximation to reality when a mea-\nsurement is affected by a very large number of small perturbations.\n\u2022The probabilities for the values of the average of some N measurements are deter-", "metadata": {"page": 88}}, {"page_content": "\u2022The probabilities for the values of the average of some N measurements are deter-\nmined to a very good approximation when N is very large by a Gaussian probability\nfunction of the form 1\n\u221a\n2\u03c0 \u03c3\n\u221a\nNe\u2212N(x\u2212\u00b5)2/(2\u03c32), where \u00b5and \u03c3are independent of\nN.\n\u2022When using a Gaussian probability measure, remember the following very useful\ninequality:\n\u222b\u221e\n\u00b5+r\n1\u221a\n2\u03c0 \u03c3e\u2212(x\u2212\u00b5)2/(2\u03c32) dx\u2264 \u03c3\u221a\n2\u03c0 re\u2212r2/(2\u03c32).\n(13.21)", "metadata": {"page": 88}}, {"page_content": "inequality:\n\u222b\u221e\n\u00b5+r\n1\u221a\n2\u03c0 \u03c3e\u2212(x\u2212\u00b5)2/(2\u03c32) dx\u2264 \u03c3\u221a\n2\u03c0 re\u2212r2/(2\u03c32).\n(13.21)\nThe top two points tell you that Gaussian probability functions arise in most contexts, and the bottom point tells you\nhow to \ufb01nd upper bounds for probabilities as determined by a Gaussian probability function.\n13.9 A digression with some comments on Equation (13.1)\nWhat follows constitutes a digression to point out that the suite of numbers {\u2206j}that appear in (13.1) cannot be", "metadata": {"page": 88}}, {"page_content": "completely arbitrary by virtue of the fact that their sum is zero: \u22061 + \u22062 + \u00b7\u00b7\u00b7 + \u2206N = 0. This conclusion is a\nconsequence of the least squares de\ufb01nition of the constants cand d. To see how this comes about, remember that the\nleast squares is de\ufb01ned by \ufb01rst introducing the matrix, A, with 2 columns and N rows whose jth row is (xj,1). The\nconstants cand dare then (c\nd\n)\n=\n(\nATA\n)\u22121\nAT\u20d7 y, (13.22)", "metadata": {"page": 88}}, {"page_content": "constants cand dare then (c\nd\n)\n=\n(\nATA\n)\u22121\nAT\u20d7 y, (13.22)\nwhere \u20d7 y\u2208RN is the vector whose jth entry is yj. Granted (13.22), the vector \u20d7\u2206 \u2208Rn whose jth entry is \u2206j is\n\u20d7\u2206 = \u20d7 y\u2212A\n(\nATA\n)\u22121\nAT\u20d7 y. (13.23)\n13.8. The three most important things to remember 85", "metadata": {"page": 88}}, {"page_content": "Now, save (13.23) for the moment, and note that if\u20d7 v\u2208RN is any vector, then\nAT\u20d7 v=\n(x1v1 + \u00b7\u00b7\u00b7 + xNvN\nv1 + \u00b7\u00b7\u00b7 + vN\n)\n. (13.24)\nIn the case that \u20d7 v= \u20d7\u2206, the top and bottom components in (13.24) are\n\u22061x1 + \u00b7\u00b7\u00b7 + \u2206NxN and \u22061 + \u00b7\u00b7\u00b7 + \u2206N. (13.25)\nNow, let us return to (13.23) to see what AT\u20d7\u2206 turns out to be. For this purpose, multiply both sides by AT to \ufb01nd\nAT\u2206 = AT\u20d7 y\u2212ATA\n(\nATA\n)\u22121\nAT\u20d7 y. (13.26)\nNext, use the fact that ATA\n(\nATA\n)\u22121", "metadata": {"page": 89}}, {"page_content": "AT\u2206 = AT\u20d7 y\u2212ATA\n(\nATA\n)\u22121\nAT\u20d7 y. (13.26)\nNext, use the fact that ATA\n(\nATA\n)\u22121\nis the identity matrix to conclude that AT\u20d7\u2206 = 0. Thus, both sums in (13.25)\nare zero, the right-hand one in particular.\n13.10 Exercises:\n1. (a) Sketch a graph of the Gaussian probability function for the values \u00b5= 0 and \u03c3= 1, 2, 4, and 8.\n(b) Prove the inequality that is depicted in the top line of (13.18) by the following sequence of arguments:", "metadata": {"page": 89}}, {"page_content": "(i) Change variables of integration using the substitution y= x\u2212\u00b5so write the integral as\n\u222b \u221e\nr\n1\u221a\n2\u03c0 \u03bae\u2212y2/(2\u03ba2) dy\n(ii) Note that the integration range has y\u2265r, so the integral is no greater than\n\u222b \u221e\nr\n1\u221a\n2\u03c0 \u03ba\ny\nre\u2212y2/(2\u03ba2) dy\n(iii) Change variables in this integral by writing u = y2\n2\u03ba2 . Thus, du = y\n\u03ba2 dyand so as to write this last\nintegral as\n\u03ba\u221a\n2\u03c0 r\n\u222b \u221e\nr2/(2\u03ba2)\ne\u2212u du.\n(iv) Complete the job by evaluating the integral in (iii).", "metadata": {"page": 89}}, {"page_content": "integral as\n\u03ba\u221a\n2\u03c0 r\n\u222b \u221e\nr2/(2\u03ba2)\ne\u2212u du.\n(iv) Complete the job by evaluating the integral in (iii).\n2. This exercise works with the exponential probability function in (13.10).\n(a) Verify by integration that the mean and standard deviations for p(x) in (13.10) are both \u00b5.\n(b) Use (13.10) to compute the probability that the desired event does not occur prior to time t\u2032>0.\n(c) Suppose t\u2032, a and b are given with 0 < t\u2032 < a < b. Prove that the conditional probability that the", "metadata": {"page": 89}}, {"page_content": "time of the desired event is in the interval [a,b] given that the event time is not before t\u2032 is given by\ne\u2212(a\u2212t\u2032)/\u00b5 \u2212e\u2212(b\u2212t\u2032)/\u00b5.\n3. Consider the uniform probability function on the interval [0,\u03c0]. Compute the mean and standard deviation for\nthe random variable x \u2192sin(x). In this regard, remember that when x \u2192f(x) is a random variable, then its\nmean is \u00b5f \u2261\n\u222bb\na f(x)p(x) dx, and the square of its standard deviation is \u03c32 \u2261\n\u222bb\na(f(x) \u2212\u00b5f)2p(x) dx.", "metadata": {"page": 89}}, {"page_content": "\u222bb\na f(x)p(x) dx, and the square of its standard deviation is \u03c32 \u2261\n\u222bb\na(f(x) \u2212\u00b5f)2p(x) dx.\n4. This problem concerns the example in Section 13.1 above where we runN versions of an experiment with sugar\nconcentration xk in the kth experiment and measure the corresponding oxygen concentration yk. We then do a\nleast squares \ufb01t of the data to a line of the form y = ax+ b. Then \u2206j = yj \u2212axj \u2212btells us how far yj is", "metadata": {"page": 89}}, {"page_content": "from the value predicted by the equation y= ax+ b. We saw that the average of these\u2206j\u2019s is zero. That is, the\nsum \u2211\nj\u2206j is 0. If we assume that the probability for any \u2206j landing in any given interval U \u2282(\u2212\u221e,\u221e) is\ndetermined by a Gaussian probability function, thus equal to\n\u222b\nx\u2208U\n1\u221a\n2\u03c0 \u03c3e\u2212|x\u2212\u00b5|2/(2\u03c32) dx.\nFind formulae for \u00b5and \u03c3in terms of the quantities {\u2206j}1\u2264j\u2264N. Hint: See Chapter 1.\n86 Chapter 13. Continuous probability functions", "metadata": {"page": 89}}, {"page_content": "5. Assume that a given measurement can take any value between 0 and \u03c0.\n(a) Suppose that the probability function for its possible values is p(x) = \u03b1sin(x). Find \u03b1, the mean, the\nstandard deviation, and the probability for the measurement to lie between 0 and \u03c0\n4 . Finally, write down\nthe cumulative distribution function.\n(b) Suppose instead that another probability function is proposed, this one whose cumulative distribution", "metadata": {"page": 90}}, {"page_content": "function has the form\u03b2+\u03b3ex. Find the constants \u03b2, \u03b3and the probability function. What is the probability\nin this case for the measurement to lie between 0 and \u03c0\n4 .\n6. Of interest to mathematicians is whether the digits that appear in the decimal expansion for the number \u03c0occur\nat random. What follows are the \ufb01rst 101 digits in this expansion:\n\u03c0= 3.1415926535 8979323846 2643383279 5028841971 6939937510\n5820974944 5923078164 0628620899 8628034825 3421170679", "metadata": {"page": 90}}, {"page_content": "5820974944 5923078164 0628620899 8628034825 3421170679\nIf the digits occur at random, then each digit should have probability 1\n10 of occurring at any given decimal place\nand one might expect that the probability of a given digit appearing ntimes in the expression above is given by\nthe version of the binomial function in (11.9) of Chapter 11 with N = 101 and q = 1\n10 . Count the number of\nappearances of each digit from the set {0,..., 9}, then use Chapter 11\u2019s Chebychev Theorem to get an upper", "metadata": {"page": 90}}, {"page_content": "bound the P-value of its appearance.\n7. Throw away the 3 to the left of the decimal place in the decimal expansion of \u03c0given in the previous problem.\nGroup the 100 remaining digits into 10 groups of 10, by moving from left to right. Thus, the \ufb01rst group is\n{1415926535}and the tenth group is {3421170679}. Let \u00b5k for k \u2208{1,..., 10}denote the mean of the kth\ngroup.\n(a) Compute each \u00b5k.\n(b) Let S denote the set {0,..., 9}and let p denote the probability function on S that assigns 1\n10 to each", "metadata": {"page": 90}}, {"page_content": "10 to each\nelement. Compute the mean and standard deviation of the numbers in Sas determined by p.\n(c) Compute the mean and standard deviation of the set {\u00b51,...,\u00b5 10}\n(d) What does the Central Limit Theorem predict for the mean and the standard deviation of the set\n{\u00b51,...,\u00b5 10}if it is assumed that the probability of a digit appearing in any given decimal place is1\n10 and\nthat the digits that appear in any subset of the 100 decimal places have no bearing on those that appear in", "metadata": {"page": 90}}, {"page_content": "the remaining places?\n8. Let p(x) denote the version of the Gaussian probability function in (13.9) with \u00b5= 0 and \u03c3= 1.\n(a) Use the relevant version of (13.18) to obtain an upper bound for the probability that passigns to the set of\npoints on the line where |x|\u2265 5. These are the points that differ from the mean by 5 standard deviations.\n(b) Use the Chebychev Theorem to obtain an upper bound for the probability that passigns to the set of points\nwhere |x|\u2265 5.", "metadata": {"page": 90}}, {"page_content": "where |x|\u2265 5.\n(c) Use a calculator to compute the ratio \u03b1\n\u03b2 where \u03b1is your answer to 8(a) and \u03b2is your answer to 8(b). (You\nwill see that the Chebychev upper bound for the Gaussian probability function is much greater than the\nbound obtained using (13.18).)\n13.10. Exercises 87", "metadata": {"page": 90}}, {"page_content": "CHAPTER\nFOURTEEN\nHypothesis testing\nMy purpose in this chapter is to say something about the following situation: You repeat some experiment a large num-\nber, N, times and each time you record the value of a certain key measurement. Label these values as {z1,...,z N}.\nA good theoretical understanding of both the experimental protocol and the biology should provide you with a hypo-\nthetical probability function,x\u2192p(x), that gives the probability that a measurement has value in any given interval", "metadata": {"page": 91}}, {"page_content": "[a,b] \u2282(\u2212\u221e,\u221e). Here, a<b and a= \u2212\u221eand b= \u221eare allowed. For example, if you think that the variations in\nthe values of zj are due to various small, unrelated, random factors, then you might propose that p(x) is a Gaussian,\nthus a function that has the form\np(x) = 1\n\u221a\n2\u03c0 \u03c3e\u2212(x\u2212\u00b5)2/(2\u03c32). (14.1)\nfor some suitable choice of \u00b5and \u03c3.\nIn any event, let\u2019s suppose that you have some reason to believe that a particularp(x) should determine the probabilities", "metadata": {"page": 91}}, {"page_content": "for the value of any given measurement. Here is the issue on the table:\nIs it likely or not thatN experiments will obtain a sequence {z1,...,z N}if the probability\nof any one measurement is really determined by p(x)? (14.2)\nIf the experimental sequence, {z1,...,z N}is \u201cunlikely\u201d for your chosen version of p(x), this suggests that your\nunderstanding of the experiment is less than adequate.\nI describe momentarily two ways to answer the question that is posed in (14.2).\n14.1 An example", "metadata": {"page": 91}}, {"page_content": "I describe momentarily two ways to answer the question that is posed in (14.2).\n14.1 An example\nBy way of an example, I ask you to consider the values of sin(x) for xan integer. As the function x\u2192sin(x) has its\nvalues between \u22121 and 1, it is tempting to make the following hypothesis:\nThe values ofsin(x) on the set of integers are distributed in a random fashion in the interval\n[\u22121,1].", "metadata": {"page": 91}}, {"page_content": "[\u22121,1].\nAs a good scientist, I now proceed to test this hypothesis. To this end, I do100 experiments, where the kth experiment\namounts to computing sin(k). Thus, the outcome of the kth experiment is zk = sin( k). For example, here are\nz1,...,z 10 to three signi\ufb01cant \ufb01gures:\n{0.841,0.909,0.141,\u22120.757,\u22120.959,\u22120.279,0.657,0.989,0.412,\u22120.544}.\nThe question posed in (14.2) here asks the following: How likely is it that the 100 numbers {zk = sin(k)}1\u2264k\u2264100", "metadata": {"page": 91}}, {"page_content": "are distributed at random in the interval [\u22121,1] with \u201crandom\u201d de\ufb01ned using the uniform probability function that has\np(x) = 1\n2 for all x?\n88", "metadata": {"page": 91}}, {"page_content": "14.2 Testing the mean\nTo investigate the question that is posed in (14.2), let\u00b5denote the mean for p(x). I remind you that\n\u00b5=\n\u222b b\na\nxp(x) dx, (14.3)\nwhere \u2212\u221e\u2264 a<b \u2264\u221e delineate the portion of R where p(x) is de\ufb01ned. If the probabilities for the measurements\n(z1,...,z N) are determined by our chosen p(x), then you might expect their average,\nz= 1\nN(z1 + z2 + \u00b7\u00b7\u00b7 + zN) (14.4)\nto be reasonably close to \u00b5. The size of the difference between zand \u00b5shed light on the question posed in (14.2).", "metadata": {"page": 92}}, {"page_content": "To elaborate, I need to introduce the standard deviation, \u03c3, for the probability function p(x). I remind you that\n\u03c32 =\n\u222b b\na\n(x\u2212\u00b5)2 p(x) dx. (14.5)\nI will momentarily invoke the Central Limit Theorem. To this end, suppose that N is a positive integer. Let\n{x1,...,x N}denote the result of N measurements where no one measurement is in\ufb02uenced by any other, and where\nthe probability of each is actually determined by the proposed function p(x). Let\nx\u2261 1\nN\n\u2211\n1\u2264j\u2264N\nxj (14.6)", "metadata": {"page": 92}}, {"page_content": "x\u2261 1\nN\n\u2211\n1\u2264j\u2264N\nxj (14.6)\ndenote the average of the N measurements. The Central Limit Theorem asserts the following: If R \u22650, then the\nprobability that xobeys |x\u2212\u00b5|\u2265 R 1\u221a\nN\u03c3is approximately\n2 1\u221a\n2\u03c0\n\u222b \u221e\nR\ne\u2212s2/2 ds (14.7)\nwhen N is very large. Note that this last expression is less than\n2 1\u221a\n2\u03c0\n1\nR e\u2212R2/2, (14.8)\nas can be seen by applying the version of (13.18) with \u00b5= 0, \u03ba= 1 and r= R.", "metadata": {"page": 92}}, {"page_content": "2\u03c0\n1\nR e\u2212R2/2, (14.8)\nas can be seen by applying the version of (13.18) with \u00b5= 0, \u03ba= 1 and r= R.\nWith the proceeding understood, I will use the Central Limit Theorem to estimate the probability thatxis further from\nthe mean of p(x) than the number zthat is depicted in (14.4). This probability is theP-value of zunder the hypothesis\nthat p(x) describes the variations in the measurements of the N experiments. The Central Limit Theorem\u2019s estimate\nfor this probability is given by the R= 1\n\u03c3\n\u221a", "metadata": {"page": 92}}, {"page_content": "for this probability is given by the R= 1\n\u03c3\n\u221a\nN|\u00b5\u2212z|version of (14.7).\nIf the number obtained by the R = 1\n\u03c3\n\u221a\nN|\u00b5\u2212z|version of (14.7) is less than 1\n20 , and N is very large, then the\nP-value of our experimental mean zis probably \u201csigni\ufb01cant\u201d and suggests that our understanding of our experiment\nis inadequate.\nTo see how this works in an example, consider the values ofsin(x) for xan integer. The N = 100 version of zfor the", "metadata": {"page": 92}}, {"page_content": "case where zk = sin(k) is \u22120.0013. Since I propose to use the uniform probability function to determine probabilities\non [\u22121,1], I should take \u00b5 = 0 and \u03c3 = 2\u221a\n12 = 1\u221a\n3 for determining Rfor use in (14.7). Granted that N = 100, I\n\ufb01nd that R= 1\n\u03c3\n\u221a\nN|\u00b5\u2212z|\u2248 0.0225. As this is very much smaller than\n\u221a\n2, I can\u2019t use (14.8) as an upper bound for\nthe integral in (14.7). However, I have other tricks for estimating (14.7) and \ufb01nd it very close to 0.982. This is a good", "metadata": {"page": 92}}, {"page_content": "approximation to the probability that the average of 100 numbers drawn at random from [\u22121,1] is further from 0 than\n0.0013. In particular, this result doesn\u2019t besmirch my hypothesis about the random nature of the values of sin(x) on\nthe integers.\n14.2. Testing the mean 89", "metadata": {"page": 92}}, {"page_content": "14.3 Random variables\nThe subsequent discussion should be easier to follow after this primer about random variables. To set the stage,\nsuppose that you have a probability function, p(x), de\ufb01ned on a subset [a,b] of the real numbers. A random variable\nin this context is no more nor less than a function, x\u2192f(x), that is de\ufb01ned when xis between aand b. For example,\nf(x) = x, f(x) = x2, and f(x) = esin(x) \u22125x3 are examples of random variables.", "metadata": {"page": 93}}, {"page_content": "f(x) = x, f(x) = x2, and f(x) = esin(x) \u22125x3 are examples of random variables.\nA random variable can have a mean and also a standard deviation. If f(x) is the random variable, then its mean is\noften denoted by \u00b5f, and is de\ufb01ned by\n\u00b5f =\n\u222b b\na\nf(x)p(x) dx. (14.9)\nThis is the \u201caverage\u201d of f as weighted by the probability p(x). The standard deviation of f is denoted by \u03c3f, it is\nnon-negative and de\ufb01ned by setting\n\u03c32\nf =\n\u222b b\na\n(f(x) \u2212\u00b5f)2 p(x) dx. (14.10)", "metadata": {"page": 93}}, {"page_content": "non-negative and de\ufb01ned by setting\n\u03c32\nf =\n\u222b b\na\n(f(x) \u2212\u00b5f)2 p(x) dx. (14.10)\nThe standard deviation measures the variation of f from its mean, \u00b5f. I assume in what follows that the integrals\nin (14.9) and (14.10) are \ufb01nite in the case that a= \u2212\u221eor b= \u221e. In general, this need not be the case.\nBy way of example, when f(x) = x, then \u00b5f is the same as the mean, \u00b5, of pthat is de\ufb01ned in (14.3). Meanwhile, \u03c3f\nin the case that f(x) = xis the same as the standard deviation that is de\ufb01ned in (14.5).", "metadata": {"page": 93}}, {"page_content": "in the case that f(x) = xis the same as the standard deviation that is de\ufb01ned in (14.5).\nHere is another example: Take p(x) to be the Gaussian probability function for the whole real line. Suppose that kis\na positive integer. Then x\u2192xk is a random variable whose mean is zero when kis odd and equal to k!\nk\n2 !2k/2 when k\nis even. Meanwhile, the square of the standard deviation for this random variable is (2k)!\nk!2k when kis odd and equal to\n1\n2k\n(\n(2k)!\nk! \u2212\n(\nk!\nk\n2 !\n)2)\nwhen kis even.", "metadata": {"page": 93}}, {"page_content": "k!2k when kis odd and equal to\n1\n2k\n(\n(2k)!\nk! \u2212\n(\nk!\nk\n2 !\n)2)\nwhen kis even.\nThere is no need for you to remember the numbers from these examples, but it is important that you understand how\nthe mean and standard deviation for a random variable are de\ufb01ned.\nBy the way, here is some jargon that you might run into: Suppose that p(x) is a probability function on a part of the\nreal line, and suppose that \u00b5is its mean. Then the average,\n\u222bb\na(x\u2212\u00b5)kp(x) dx, for positive integer kis called the kth", "metadata": {"page": 93}}, {"page_content": "\u222bb\na(x\u2212\u00b5)kp(x) dx, for positive integer kis called the kth\norder moment of p(x).\n14.4 The Chebychev and Central Limit Theorems for random variables\nThe mean and standard deviation for any given random variable are important for their use in the Chebychev Theorem\nand the Central Limit Theorem. For example, the Chebychev Theorem in the context of a random variable says the\nfollowing:\nChebychev Theorem for Random Variables. Let x \u2192p(x) denote a probability function on a part of the real", "metadata": {"page": 93}}, {"page_content": "line, and let x \u2192f(x) denote a random variable on the domain for pwith mean \u00b5f and standard deviation \u03c3f as\ndetermined by p. Let R be a positive number. Then, 1\nR2 is greater than the probability that p assigns to the set of\npoints where |f(x) \u2212\u00b5f|\u2265 R\u03c3f.\nThe proof of this theorem is identical, but for a change of notation, to the proof given previously for the original\nversion of the Chebychev Theorem.", "metadata": {"page": 93}}, {"page_content": "version of the Chebychev Theorem.\nHere is the context for the random variable version of the Central Limit Theorem: You have a probability function,\nx \u2192p(x), that is de\ufb01ned on some or all of the real line. You also have a function, x \u2192f(x), that is de\ufb01ned where\npis. You now interpret pand f in the following way: The possible \u201cstates\u201d of a system that you are interested in are\nlabeled by the points in the domain of p, and the function pcharacterizes the probability for the system to be in the", "metadata": {"page": 93}}, {"page_content": "state labeled by x. Meanwhile, f(x) is the value of a particular measurement when the system is in the state labeled\nby x.\n90 Chapter 14. Hypothesis testing", "metadata": {"page": 93}}, {"page_content": "Granted these interpretations ofpand f, imagine doing some large number,N, of experiments on the system and make\nthe corresponding measurement each time. Generate in this wayNnumbers, {f1,...,f N}. Let f = 1\nN(f1 +\u00b7\u00b7\u00b7+fN)\ndenote the average of the N measurements. These measurements should be independent in the sense that the values\nthat are obtained in any subset of the N measurements have no bearing on the values of the remaining measurements.", "metadata": {"page": 94}}, {"page_content": "Under these circumstances, the Central Limit Theorem asserts the following:\nCentral Limit Theorem. When Nis large, the probability thatfhas values in any given subset of the real line is very\nclose to the probability that is assigned to the subset by the Gaussian function 1\u221a\n2\u03c0 \u03c3f\ne\u2212(x\u2212\u00b5f)2/(2\u03c3f). For example,\nif \u2212\u221e\u2264 s<r \u2264\u221e and the subset in question is the interval where s<x<r , then the probability that s< f <r\nis very close when N is large to \u222b r\ns\n1\u221a\n2\u03c0(\u03c3f/\n\u221a\nN)\ne\u2212(x\u2212\u00b5f)/(2N\u03c3f) dx.", "metadata": {"page": 94}}, {"page_content": "is very close when N is large to \u222b r\ns\n1\u221a\n2\u03c0(\u03c3f/\n\u221a\nN)\ne\u2212(x\u2212\u00b5f)/(2N\u03c3f) dx.\nFor example, ifR\u22650, then the probability that xobeys |x\u2212\u00b5f|\u2265 R \u03c3f\u221a\nN is approximately\n2 \u00b7 1\u221a\n2\u03c0\n\u222b \u221e\nR\ne\u2212s2/2 ds (14.11)\nwhen N is very large.\n14.5 Testing the variance\nReturn now to the question posed in (14.2). If our experimental mean is reasonably close to hypothesized mean,\u00b5, for\np(x), then our hypothesis that p(x) describes the variations in the measurements {z1,...,z N}can be tested further", "metadata": {"page": 94}}, {"page_content": "by seeing whether the variation of the zj about their mean is a likely or unlikely occurrence.\nHere is one way to do this: Let f(x) denote the function x\u2192(x\u2212\u00b5)2. I view f as a random variable on the domain\nfor p(x) that takes its values in [0,\u221e). Its mean, \u00b5f, as determined by p(x), is\n\u00b5f =\n\u222b b\na\n(x\u2212\u00b5)2p(x) dx. (14.12)\nHere, I write the interval where pis de\ufb01ned as [a,b] with \u2212\u221e\u2264 a < b\u2264\u221e. You might recognize \u00b5f as the square", "metadata": {"page": 94}}, {"page_content": "of the standard deviation of p(x). I am calling it by a different name because I am thinking of it as the mean of the\nrandom variable f(x) = (x\u2212\u00b5)2.\nThe random variable f(x) = (x \u2212\u00b5)2 can have a standard deviation as well as a mean. According to our de\ufb01nition\nin (14.10), this standard deviation has square\n\u03c32\nf =\n\u222b b\na\n(\n(x\u2212\u00b5)2 \u2212\u00b5f\n)2\np(x) dx=\n\u222b b\na\n(x\u2212\u00b5)4p(x) dx\u2212\u00b52\nf. (14.13)\nPut \u00b5f and \u03c3f away for a moment to bring in our experimental data. Our data gives us the N numbers {(z1 \u2212", "metadata": {"page": 94}}, {"page_content": "\u00b5)2,(z2 \u2212\u00b5)2,..., (zN \u2212\u00b5)2}. The plan is to use the random variable version of the Central Limit Theorem to\nestimate a P-value. This is the P-value for the average of these N numbers,\nVar \u2261 1\nN\n\u2211\n1\u2264j\u2264N\n(zj \u2212\u00b5)2, (14.14)\nunder the assumption that p(x) determines the spread in the numbers {zj}1\u2264j\u2264N. I call Var the \u201cexperimentally\ndetermined variance\u201d.\nAccording to the Central Limit Theorem, if the spread in the numbers {zj}1\u2264j\u2264N are really determined by p(x), then", "metadata": {"page": 94}}, {"page_content": "the P-value of our experimentally determined variance is well approximated by the integral in (14.11) for the case that\nR=\n\u221a\nN\n\u03c3f\n|Var\u2212\u00b5f|. (14.15)\n14.5. Testing the variance 91", "metadata": {"page": 94}}, {"page_content": "To summarize: Our hypothetical probability function p(x) gives the expected variance, this the number that we com-\npute in (14.12). The Central Limit Theorem then says that our experimentally determined variance, this the number\nfrom (14.14), should approach the predicted one in (14.12) asN \u2192\u221e. Moreover, the Central Limit Theorem gives an\napproximate probability, this the expression given using (14.15) in (14.11), for any measured variance to differ from", "metadata": {"page": 95}}, {"page_content": "the theoretical one by more than the experimentally determined variance. In particular, if the integral in (14.11) is less\nthan 1\n20 using Rfrom (14.15), then there is a signi\ufb01cant chance that our theoretically determined p(x) is not correct.\nI am hoping that the example using the values of sin(x) on the integers makes this less abstract. To return to this\nexample, recall that the probability function p(x) I use is the uniform probability function on the interval[\u22121,1]. This", "metadata": {"page": 95}}, {"page_content": "version of p(x) \ufb01nds \u00b5= 0 and \u00b5f = 1\n3 for the case f(x) = (x\u2212\u00b5)2 = x2. The relevant version version of (14.13)\nuses a = \u22121, b = 1 and p(x) = 1\n2 ; and the resulting integral in (14.13) \ufb01nds \u03c32\nf = 4\n45 \u22480.089. Meanwhile,\nthe N = 100 and xk = sin(k) version of (14.14) \ufb01nds Var \u22480.503. Granted this, then I \ufb01nd the right hand side\nof (14.15) equal to 5.69. Since this value for Ris larger than\n\u221a\n2, I can use (14.8) as an upper bound for the value of", "metadata": {"page": 95}}, {"page_content": "\u221a\n2, I can use (14.8) as an upper bound for the value of\nthe integral in (14.11). This upper bound is less than4.2\u00d710\u22127! This is an upper bound for theP-value of the number\nVar \u22480.503 under the hypothesis that the values of{sin(k)}1\u2264k\u2264100 are distributed uniformly in the interval[\u22121,1].\nThis tiny P-value puts a very dark cloud over my hypothesis that the values of sin(x) for xan integer are uniformly\ndistributed between \u22121 and 1.\n14.6 Did Gregor Mendel massage his data?", "metadata": {"page": 95}}, {"page_content": "distributed between \u22121 and 1.\n14.6 Did Gregor Mendel massage his data?\nTo reach conclusions about inheritance, Gregor Mendel ran a huge number of experiments on pea plants 1 In fact,\nhe raised over 20,000 plants from seed over an 8 year period. From this data, he was able to glean the \u201csimple\u201d\ninheritance of dominant-recessive genes. In modern language, here is what this means: Some traits are controlled by", "metadata": {"page": 95}}, {"page_content": "two different versions of a particular gene. The different versions are called \u201calleles\u201d. One allele gives the \u201cdominant\u201d\ntrait or phenotype, and one gives the \u201crecessive\u201d trait. An individual plant has2 copies of each gene, and shows the\nrecessive trait only when both the copies are the recessive allele. Each pollen grain receives only one gene, and each\novule receives only one. A seed is formed when a pollen grain merges with an ovule; thus each offspring has again", "metadata": {"page": 95}}, {"page_content": "two copies of the gene. Granted this fact, if it is assumed that each allele has probability to be passed to any given\npollen grain or ovule, one should expect about a3:1 ratio of dominant phenotypes to recessive phenotypes.\nThis explanation for the inheritance of traits from parent plant to seedling predicts also predicts the following: A\nindividual plant with the dominant phenotype should have 2 dominant alleles of the time; the other of the time should", "metadata": {"page": 95}}, {"page_content": "have one copy of each of the two alleles. A plant with the dominant trait is called a \u201chomozygous\u201d dominant when it\nhas two dominant alleles, and it is called a \u201cheterozygous\u201d dominant when it has one dominant allele and one recessive\nallele. For hundreds of dominant phenotype plants, Mendel was able to classify whether they were homozygous or\nheterozygous by noting whether the given plant\u2019s clonal offspring (the plant was self-pollinated) showed any recessive", "metadata": {"page": 95}}, {"page_content": "traits. From these experiments, the theory predicts a 2:1 ratio of heterozygous dominant to homozygous dominant\nplants. Below is a chart of data from Mendel\u2019s work with regards to this homozygous/heterozygous experiment.\nMendel was accused, long after he died, of having massaged his data. One of the complains was that Mendel\u2019s data\ndoes not have enough spread about the mean. R. A. Fisher2 analyzed this complaint along the following lines: Imagine", "metadata": {"page": 95}}, {"page_content": "\ufb02ipping a coin some number, say N times. Here, the coin is such that the probability of heads is 1\n3 . Use the q = 1\n3\nbinomial probability functions to see that the mean number of heads for N \ufb02ips is N\n3 and the standard deviation is\n1\n3\n\u221a\n2N. Now, consider Mendel\u2019s data as presented above. There are 8 separate experiments, where the values of N\ndiffer. In particular, the values of the mean and the standard deviation for the8 different values of N are: Here, I have", "metadata": {"page": 95}}, {"page_content": "rounded \u00b5and \u03c3off to the nearest integer. The column that is headed as \u201ch\u201d, is a copy of the column that is headed by\n\u201cHomozygous Dominant\u201d in the previous table. Thus, this column contains the actual data, the number of homozygous\ndominant offspring. The far right column takes the number of homozygous dominant offspring, subtracts the mean,\ndivides the result by\u03c3and, after taking absolute values, rounds to the nearest10th. For example, the number in the far", "metadata": {"page": 95}}, {"page_content": "right column for round/wrinkled seeds is 193\u2212188\n11 = 5\n11 \u22480.5. The number in the far right column for green/yellow\nseeds is 173\u2212166\n10.7 \u22480.7. Thus, the far right column gives the distance of the observed value from the expected value\n1Mendel, G., 1866 Versuche \u00a8uber P\ufb02anzen-Hybriden. Verh. Naturforsch. Ver. Br \u00a8unn 4: 3\u201347 (\ufb01rst English translation in 1901, J. R. Hortic.\nSoc. 26: 1\u201332; reprinted in Experiments in Plant Hybridization, Harvard University Press, Cambridge, MA, 1967).", "metadata": {"page": 95}}, {"page_content": "2Fisher, R. A., 1936, Has Mendel\u2019s work been rediscovered? Ann. Sci.1: 115\u2013137.\n92 Chapter 14. Hypothesis testing", "metadata": {"page": 95}}, {"page_content": "Homozygous Heterozygous\nDominant Dominant Ratio\nround/wrinkled seed 193 372 1.93\ngreen/yellow seed 166 353 2.13\ncolored/white \ufb02ower 36 64 1.78\ntall/dwarf plant 28 72 2.57\nconstricted pod or not 29 71 2.45\ngreen/yellow pod #1 40 60 1.50\nterminal \ufb02ower or not 33 67 2.03\nyellow/green pod #2 35 65 1.86\nTotal 560 1124 2.01\nTable 14.1: Tests of Heterozygosity of Dominant Trait Phenotypes\nN \u00b5 \u03c3 h |h\u2212\u00b5|/\u03c3\nround/wrinkled seed 565 188 11 193 0.5\ngreen/yellow seed 519 173 11 166 0.6", "metadata": {"page": 96}}, {"page_content": "N \u00b5 \u03c3 h |h\u2212\u00b5|/\u03c3\nround/wrinkled seed 565 188 11 193 0.5\ngreen/yellow seed 519 173 11 166 0.6\ncolored/white \ufb02ower 100 33 5 36 0.6\ntall/dwarf plant 100 33 5 28 1\nconstricted pod or not 100 33 5 29 0.8\ngreen/yellow pod #1 100 33 5 40 1.4\nterminal \ufb02ower or not 100 33 5 33 0\nyellow/green pod #2 100 33 5 35 0.4\nas measured in units where 1 means 1 standard deviation.\nThose who complained about the spread of Mendel\u2019s data presumably made a table just like the one just given, and", "metadata": {"page": 96}}, {"page_content": "then noticed the following interesting fact: In 7 of the 8 cases, the observed value for hhas distance less than or equal\nto 1 standard deviation from the mean. They must have thought that this clustering around the mean was not likely to\narise by chance.\nWhat follows describes a method to estimate a P-value for having 7 of 8 measurements land within one standard\ndeviation of the mean. To do this invoke the Central Limit theorem in each of the eight cases to say that the probability", "metadata": {"page": 96}}, {"page_content": "that h has a given value, say x, should be determined to great accuracy by the Gaussian probability function:\np(x) = 1\u221a\n2\u03c0 \u03c3e\u2212(x\u2212\u00b5)2/(2\u03c32). (14.16)\nGranted that the Central Limit theorem is applicable, then the probability ofhhaving value within1 standard deviation\nfrom the mean is\nq=\n\u222b \u00b5+\u03c3\n\u00b5\u2212\u03c3\n1\u221a\n2\u03c0 \u03c3e\u2212(x\u2212\u00b5)2/(2\u03c32) dx\u22480.68. (14.17)\nThis has the following consequence: The probability of getting n \u2208 {0,1,..., 8}out of 8 measurements to lie", "metadata": {"page": 96}}, {"page_content": "within 1 standard deviation is given by the N = 8 binomial probability function using q = 0.68. This is p(n) =\n8!\nn! (8\u2212n)! (0.68)n(0.32)8\u2212n. Note that the mean this binomial function is 8(0.68) = 5.44. Thus, the P-value for\ngetting 7 of 8 measurements within 1 standard deviation of the mean is\np(0) + p(1) + p(2) + p(3) + p(7) + p(8).\nAs it turns out this \u22480.58, thus greater than 0.05.", "metadata": {"page": 96}}, {"page_content": "p(0) + p(1) + p(2) + p(3) + p(7) + p(8).\nAs it turns out this \u22480.58, thus greater than 0.05.\nLooking back on this discussion, note that I invoked binomial probability functions in two different places. The \ufb01rst\nwas to derive the right most column in the table. This used aq = 1\n3 binomial with values of N given by the left-most\ncolumn of numbers in the table. The value of came from the hypothesis about the manner in which genes are passed", "metadata": {"page": 96}}, {"page_content": "from parent plant to seedling. The second application of the binomial probability function used theq = 0.68 version\nwith N = 8. I used this version to compute aP-value for having7 of 8 measurements land within 1 standard deviation\nof the mean. The value 0.68 for qwas justi\ufb01ed by an appeal to the Central Limit theorem.\n14.6. Did Gregor Mendel massage his data? 93", "metadata": {"page": 96}}, {"page_content": "14.7 Boston weather 2008\nThis last part of the chapter brie\ufb02y discusses a case where hypothesis testing might say something interesting. The\nfollowing chart from the Sunday, January 4 New York Times gives the daily temperature extremes in Boston for the\nyear 2008: The chart claims that the average temperature for the year was0.5 degrees above normal. The grayish\nFigure 14.1: Temperature Extremes in Boston (NY Times, Sunday, January 4, 2009)", "metadata": {"page": 97}}, {"page_content": "Figure 14.1: Temperature Extremes in Boston (NY Times, Sunday, January 4, 2009)\nswath in the background is meant to indicate the \u201cnormal range\u201d. (Nothing is said about what this means.)\nWe hear all the time about \u201cglobal warming\u201d. Does this chart give evidence for warming in Boston during the year\n2008? What follows describe what may be an overly simplistic way to tease some answer to this question from the", "metadata": {"page": 97}}, {"page_content": "chart. Let us assume that the center of the grey swath on each day represents the mean temperature on the given\nday as computed by averaging over some large number of years. This gives us an ordered set of 366 numbers,\n{T1,...,T 366}. Meanwhile, the chart gives us, for each day in 2008, the actual mean temperature on that day, this\nhalf of the sum of the indicated high and low temperature on that day. This supplies us with a second sequence of366", "metadata": {"page": 97}}, {"page_content": "numbers, {t1,...,t 366}. Subtracting the \ufb01rst sequence from the second gives us numbers (x1 = t1 \u2212T1,...,x 366 =\nt366 \u2212T366). Take this set of 366 numbers to be the experimentally measured data.\nThe chart tells us that the average, \u00b5real = 1\n366 (x1 + \u00b7\u00b7\u00b7 + x366), of the experimental data is equal to 0.5. We can\nthen ask if this average is consistent with the hypothesis that the sequence (x1,...,x 366) is distributed according to a", "metadata": {"page": 97}}, {"page_content": "Gaussian distribution with mean zero and some as yet unknown standard deviation. Unfortunately, the chart doesn\u2019t\nsay anything about the standard deviation. This understood, let us proceed with the assumption that the standard\ndeviation is half the vertical height of the grey swath, which is to say roughly6\u25e6degrees Fahrenheit. If we make this\nassumption, then we are comparing the number0.5 with what would be expected using the Central Limit theorem with", "metadata": {"page": 97}}, {"page_content": "input a Gaussian with mean zero and standard deviation 6\u25e6. The Central Limit theorem asserts that the P-value of the\nmeasured mean 0.5 as computed using N = 366 and this hypothetical Gaussian is \u22480.14, which is not signi\ufb01cant.\nWe can also ask whether the variance of the data set (x1,...,x 366) is consistent with the assumption that these\nnumbers are distributed according a Gaussian with mean \u00b5= 0 and standard deviation \u03c3= 6\u25e6. This is to say that we\nare comparing the experimentally variance \u03c32", "metadata": {"page": 97}}, {"page_content": "are comparing the experimentally variance \u03c32\nreal = 1\n366 (x2\n1 + \u00b7\u00b7\u00b7 + x2\n366) with the average of 366 distances from the\nmean as computed using the Central Limit theorem for the Gaussian with mean \u00b5= 0 and \u03c3= 6\u25e6.94 Chapter 14. Hypothesis testing", "metadata": {"page": 97}}, {"page_content": "Of course, we can go further. For example, when I look at the graph, I sense that there are more peaks that are very\nmuch above the grey swath than there are peaks that are very much below the grey swath. I can test whether this is\nconsistent with the Gaussian assumption by using the central limit theorem for the average ofN = 366 measurements\nof the random variable f(x) = 1\n2 (x+ |x|). (Note that f(x) = 0 when x\u22640 and f(x) = xwhen x> 0.)\n14.8 Exercises:", "metadata": {"page": 98}}, {"page_content": "2 (x+ |x|). (Note that f(x) = 0 when x\u22640 and f(x) = xwhen x> 0.)\n14.8 Exercises:\n1. Suppose that we expect that the x-coordinate of bacteria in our rectangular petri dish should be any value\nbetween \u22121 and 1 with equal probability in spite of our having coated the x= 1 wall of the dish with a speci\ufb01c\nchemical. We observe the positions of 900 bacteria in our dish and so obtain 900 values, {z1,...,z 900}, for the\nx-coordinates.\n(a) Suppose the average, z= 1\n900\n\u2211", "metadata": {"page": 98}}, {"page_content": "x-coordinates.\n(a) Suppose the average, z= 1\n900\n\u2211\n1\u2264k\u2264900 zk, is 0.1. Use the Central Limit Theorem to obtain a theoretical\nupper bound based on our model of a uniform probability function for the probability that an average of\n900x-coordinates differs from 0 by more than 0.1.\n(b) Suppose that the average of the squares, Var = \u2211\n1\u2264k\u2264900 z2\nk, equals 0.36. Use the Central Limit Theorem\nand (13.21) to obtain a theoretical upper bound based on our model of a uniform probability function for", "metadata": {"page": 98}}, {"page_content": "the probability that an average of the squares of 900 x-coordinates is greater than or equal to 0.36. (Note\nthat I am not asking that it differ by a certain amount from the square of the standard deviation for the\nuniform probability function. If you compute the latter, you will be wrong by a factor of 2.)\n2. Use Stirling\u2019s formula in Equation (11.14) to give approximate formulae for both(2k)!\nk!2k and 1\n2k\n(\n(2k)!\nk! \u2212\n(\nk!\nk\n2 !\n)2)\nwhen k is large.", "metadata": {"page": 98}}, {"page_content": "k!2k and 1\n2k\n(\n(2k)!\nk! \u2212\n(\nk!\nk\n2 !\n)2)\nwhen k is large.\n3. R. A. Fisher (see also reference 2 above) discussed a second criticism of Mendel\u2019s experimental data. This\ninvolved the manner in which a given dominant phenotype plant was classi\ufb01ed as being \u201chomozygous dominant\u201d\nor \u201cheterozygous dominant\u201d. According to Fisher, Mendel used the following method on any given plant: He\ngerminated10 seeds from the plant via self-pollination, and if all 10 of the resulting seedlings had the dominant", "metadata": {"page": 98}}, {"page_content": "phenotype, he then labeled the plant as \u201chomozygous dominant\u201d. If one or more of the 10 seedlings had the\nrecessive phenotype, he labeled the plant as \u201cheterozygous dominant\u201d. Fisher pointed out that if Mendel really\ndid the classi\ufb01cation in this manner, then he should have mislabeled some heterozygous dominant plants as\nhomozygous dominant. The following questions walk you through some of Fisher\u2019s arguments.", "metadata": {"page": 98}}, {"page_content": "homozygous dominant. The following questions walk you through some of Fisher\u2019s arguments.\n(a) What is the probability for a heterozygous dominant plant to produce a seedling with the dominant pheno-\ntype?\n(b) What binomial probability function should you use to compute the probability that a heterozygous plant\nproduces 10 consecutive dominant seedlings.\n(c) Use the binomial probability function for (b) to compute the probability of any given heterozygous domi-", "metadata": {"page": 98}}, {"page_content": "nant plant to produce 10 consecutive dominant seedlings.\n(d) If any given plant has probability to be homozygous dominant and thus to be heterozygous dominant, what\nis the probability that Mendel would label any given plant as \u201chomozygous dominant\u201d? (To answer this,\nyou can use conditional probabilities. To this end, suppose that you have a sample space of N plants.\nUse A to denote the subset of plants that are homozygous dominant, B to denote the subset that are", "metadata": {"page": 98}}, {"page_content": "Use A to denote the subset of plants that are homozygous dominant, B to denote the subset that are\nheterozygous dominant, and Cto denote the subset that Mendel designates are homozygous dominant.)\n(e) Redo the second table in this chapter based on your answer to (c) of this chapter.\n4. The 2006 election for the United States Senator in Virginia had the following outcome (according tocnn.com):\nWebb: 1,172,671 votes\nAllen: 1,165,440 votes\n14.8. Exercises 95", "metadata": {"page": 98}}, {"page_content": "The total votes cast for the two candidates was2,238,111, and the difference in the vote totals was7,231. This\nwas considered by the press to be an extremely tight election. Was it unreasonably close? Suppose that an\nelection to choose one of two candidates is held with 2,238,111 voters. Suppose, in addition, that each voter\ncasts his or her vote at random. If you answer correctly the questions (b) and (c) below, you will obtain an", "metadata": {"page": 99}}, {"page_content": "estimate for the probability as determined by this \u201cvote at random\u201d model that the difference between the two\ncandidates is less than or equal to 7,231.\n(a) Set N = 2 ,238,111. Let S denote the sample space whose elements are sequences of the form\n{z1,...,z N}where each zk is either 1 or \u22121. Use f to denote the random variable on S that is given\nby the formula f(z1,...,z N) = z1 + z2 + \u00b7\u00b7\u00b7 + zN. What is the mean and what is the standard deviation\nof f?", "metadata": {"page": 99}}, {"page_content": "of f?\n(b) Use the Central Limit Theorem to \ufb01nd a Gaussian probability function that can be used to estimate the\nprobability that 1\nNf has value in any given interval on the real line.\n(c) Use the Gaussian probability from (b) to compute the probability that |1\nNf|\u2264 7,231\nN . Note that this is\nthe probability (as computed by this Gaussian) for |f|to be less than or equal to 7,231. (You can use a\ncalculator if you like to compute the relevant integral.)\n96 Chapter 14. Hypothesis testing", "metadata": {"page": 99}}, {"page_content": "CHAPTER\nFIFTEEN\nDeterminants\nThis chapter is meant to provide some cultural background to the story told in the linear algebra text book about\ndeterminants. As the text explains, the determinant of a square matrix is non-zero if and only if the matrix is invertible.\nThe fact that the determinant signals invertibility is one of its principle uses. The other is the geometric fact observed", "metadata": {"page": 100}}, {"page_content": "by the text that the absolute value of the determinant of the matrix is the factor by which the linear transformation\nexpands or contractsn-dimensional volumes. This chapter considers an invertibility question of a biological sort, an\napplication to a protein folding problem.\nRecall that a protein molecule is a long chain of smaller molecules that are tied end to end. Each of these smaller", "metadata": {"page": 100}}, {"page_content": "molecules can be any of twenty so called amino acids. This long chain appears in a cell folded up on itself in a\ncomplicated fashion. In particular, its interactions with the other molecules in the cell are determined very much by\nthe particular pattern of folding because any given fold will hide some amino acids on its inside while exhibiting others\non the outside. This said, one would like to be able to predict the fold pattern from knowledge of the amino acid that", "metadata": {"page": 100}}, {"page_content": "occupies each site along the chain.\nIn all of this, keep in mind that the protein is constructed in the cell by a component known as a \u201cribosome\u201d, and\nthis construction puts the chain together starting from one end by sequentially adding amino acids. As the chain is\nconstructed, most of the growing chain sticks out of the ribosome. If not stabilized by interactions with surrounding", "metadata": {"page": 100}}, {"page_content": "molecules in the cell, a given link in the chain will bend this way or that as soon as it exits the ribosome, and so the\nprotein would curl and fold even as it is constructed.\nTo get some feeling for what is involved in predicting the behavior here, make the grossly simplifying assumption\nthat each of the amino acids in a protein molecule of length N can bend in one of 2 ways, but that the probability of", "metadata": {"page": 100}}, {"page_content": "bending, say in the + direction for the nth amino acid is in\ufb02uenced by the direction of bend of its nearest neighbors,\nthe amino acids in sites n\u22121 and n+ 1. One might expect something like this for short protein molecules in as much\nas the amino acids have electrical charges on them and so feel an electric force from their neighbors. As this force is\ngrows weaker with distance, their nearest neighbors will affect them the most. Of course, once the chain folds back", "metadata": {"page": 100}}, {"page_content": "on itself, a given amino acid might \ufb01nd itself very close to another that is actually some distance away as measured by\nwalking along the chain.\nIn any event, let us keep things very simple and suppose that the probability, pn(t), at time tof the nth amino acid\nbeing in the + fold position evolves as\npn(t+ 1) = an + An,npn(t) + An,n\u22121pn\u22121(t) + An,n+1pn+1(t). (15.1)\nHere, an is some \ufb01xed number between 0 and 1, and the numbers {an,An,n,An,n\u00b11}are constrained so that", "metadata": {"page": 100}}, {"page_content": "0 \u2264an + An,nx+ An,n\u22121 x\u2212+ An,n+1x+ \u22641 (15.2)\nfor any choice between 0 and 1 of values for x, x\u2212and x+ with x+ x\u2212+ x+ \u22641. This constraint is necessary to\nguarantee that pn(t+ 1) is between 0 and 1 if each of pn(t), pn+1(t) and pn\u22121(t) is between 0 and 1. In this regard,\nlet me remind you that pn(t+ 1) must be between 0 and 1 if it is the probability of something. My convention here\ntakes both A1,0 and AN,N+1 to be zero.", "metadata": {"page": 100}}, {"page_content": "takes both A1,0 and AN,N+1 to be zero.\nAs for the values of the other coef\ufb01cients, I will suppose that knowledge of the amino acid type that occupies site n\n97", "metadata": {"page": 100}}, {"page_content": "is enough to determine an and Ann, and that knowledge of the respective types that occupy sites n\u22121 and n+ 1 is\nenough to determine An,n\u22121 and An,n+1. In this regard, I assume access to a talented biochemist.\nGranted these formul\u00e6, the N-component vector \u20d7 p(t)whose nth component is pn(t) evolves according to the rule:\n\u20d7 p(t+ 1) = \u20d7 a+ A\u20d7 p(t) (15.3)\nwhere \u20d7 ais that N-component vector whose nth entry is an, and where Ais that N \u00d7N matrix whose only non-zero\nentries are {An,n,An,n\u00b11}1\u2264n\u2264N.", "metadata": {"page": 101}}, {"page_content": "entries are {An,n,An,n\u00b11}1\u2264n\u2264N.\nWe might now ask if there exists anequilibrium probability distribution, an N-component vector, \u20d7 p, with non-negative\nentries that obeys\n\u20d7 p= \u20d7 a+ A\u20d7 p (15.4)\nIf there is such a vector, then we might expect its entries to give us the probabilities for the bending directions of the\nvarious links in the chain for the protein. From this, one might hope to compute the most likely fold pattern for the\nprotein.\nTo analyze (15.4), let us rewrite it as the equation", "metadata": {"page": 101}}, {"page_content": "protein.\nTo analyze (15.4), let us rewrite it as the equation\n(I\u2212A)\u20d7 p= \u20d7 a, (15.5)\nwhere I here denotes the identity matrix; this the matrix with its only entries on the diagonal and with all of the latter\nequal to 1. We know now that there is some solution, \u20d7 p, whendet(I\u2212A) \u0338= 0. It is also unique in this case. Indeed,\nwere there two solutions, \u20d7 pand \u20d7 p\u2032, then\n(I\u2212A)(\u20d7 p\u2212\u20d7 p\u2032) = 0. (15.6)", "metadata": {"page": 101}}, {"page_content": "were there two solutions, \u20d7 pand \u20d7 p\u2032, then\n(I\u2212A)(\u20d7 p\u2212\u20d7 p\u2032) = 0. (15.6)\nThis implies (I\u2212A) has a kernel, which is forbidden when I\u2212Ais invertible. Thus, to understand this version of the\nprotein folding problem, we need to consider whether the matrix I\u2212Ais invertible. As remarked at the outset, this is\nthe case if and only if it has non-zero determinant.\nBy the way, we must also con\ufb01rm that the solution,\u20d7 p, to (15.5) has its entries between0 and 1 so as to use the entries\nas probabilities.", "metadata": {"page": 101}}, {"page_content": "as probabilities.\nIn any event, to give an explicit example, consider the3 \u00d73 case. In this case, the matrix I\u2212Ais\n\uf8eb\n\uf8ed\n1 \u2212A11 \u2212A12 0\n\u2212A21 1 \u2212A12 \u2212A23\n\u2212A31 \u2212A32 1 \u2212A12\n\uf8f6\n\uf8f8. (15.7)\nUsing the formulae from Chapter 6 of the linear algebra text book, its determinant is found to be\ndet(I\u2212A) = (1 \u2212A11)(1 \u2212A22)(1 \u2212A33) \u2212(1 \u2212A11)A23A32 \u2212(1 \u2212A33)A12A21. (15.8)\n98 Chapter 15. Determinants", "metadata": {"page": 101}}, {"page_content": "CHAPTER\nSIXTEEN\nEigenvalues in biology\nMy goal in this chapter is to illustrate how eigenvalues can appear in problems from biology.\n16.1 An example from genetics\nSuppose we have a \ufb01xed size N population of cells that reproduce by \ufb01ssion. The model here is that there are N cells\nthat divide at the start, thus producing 2N cells. After one unit of time, half of these die and half survive, so there are", "metadata": {"page": 102}}, {"page_content": "N cells to \ufb01ssion at the end of 1 unit of time. These N survivors divide to produce 2N cells and so start the next run\nof the cycle. In particular, there are always N surviving cells at the end of one unit of time and then 2N just at the\nstart of the next as each of these N cells splits in half.\nNow, suppose that at the beginning, t = 0, there is, for each n \u2208{0, 1,...,N }, a given probability which I\u2019ll call", "metadata": {"page": 102}}, {"page_content": "pn(0) for nof the N initial cells to carry a certain trait. Suppose that this trait (red color as opposed to blue) is neutral\nwith respect to the cell\u2019s survival. In other words, the probability is for any given red cell to survive to reproduce, and\nthe probability is for any given blue cell to survive to reproduce.\nHere is the key question: Given the initial probabilities, {p0(0),p1(0),...,p N(0)}, what are the corresponding prob-", "metadata": {"page": 102}}, {"page_content": "abilities after some tgenerations? Thus, what are the values of {p0(t),p1(t),...,p N(t)}where now pn(t) denotes\nthe probability that ncells of generation tcarry the red color?\nTo solve this, we can use our tried and true recourse to conditional probabilities by noting that\npn(t) = P (nred survivors|0 red parents) \u00b7p0(t\u22121)\n+ P (nred survivors|1 red parent) \u00b7p1(t\u22121) (16.1)\n+ P (nred survivors|2 red parents) \u00b7p2(t\u22121) + \u00b7\u00b7\u00b7", "metadata": {"page": 102}}, {"page_content": "+ P (nred survivors|1 red parent) \u00b7p1(t\u22121) (16.1)\n+ P (nred survivors|2 red parents) \u00b7p2(t\u22121) + \u00b7\u00b7\u00b7\nwhere P (nred survivors|mred parents) is the conditional probability that there are nred cells at the end of a cycle\ngiven that there weremsuch cells the end of the previous cycle. If the ambient environmental conditions don\u2019t change,\none would expect that these conditional probabilities are independent of time. As I explain below, they are, in fact,", "metadata": {"page": 102}}, {"page_content": "computable from what we are given about this problem. In any event, let me use the shorthand Ato denote the square\nmatrix of size N + 1 whose entry in row nand column mis P (nred survivors|mred parents). In this regard, note\nthat nand mrun from 0 to N, not the from 1 to N+ 1. Let \u20d7 p(t)denote the vector in RN+1 whose nth entry is pn(t).\nThen (16.1) reads\n\u20d7 p(t) =A\u20d7 p(t\u22121). (16.2)", "metadata": {"page": 102}}, {"page_content": "Then (16.1) reads\n\u20d7 p(t) =A\u20d7 p(t\u22121). (16.2)\nThis last equation would be easy to solve if we knew that \u20d7 p(0)was an eigenvector of the matrix A. That is, if it were\nthe case that\nA\u20d7 p(0) =\u03bb\u20d7 p(0) with \u03bbsome real number. (16.3)\nIndeed, were this the case, then the t = 1 version of (16.2) would read \u20d7 p(1) =\u03bb\u20d7 p(0). We could then use thet = 2\nversion of (16.2) to compute\u20d7 p(2)and we would \ufb01nd that \u20d7 p(2) =\u03bbA\u20d7 p(0) =\u03bb2\u20d7 p(0). Continuing in the vein, we would", "metadata": {"page": 102}}, {"page_content": "\ufb01nd that \u20d7 p(t) =\u03bbt\u20d7 p(0)and our problem would be solved.\n99", "metadata": {"page": 102}}, {"page_content": "Now, it is unlikely that\u20d7 p(0)is going to be an eigenvector. However, even if\u20d7 p(0)is a linear combination of eigenvectors,\n\u20d7 p(0) =c1\u20d7 e1 + c2\u20d7 e2 + \u00b7\u00b7\u00b7 , (16.4)\nwe could still solve for \u20d7 p(t). Indeed, let\u03bbk denote the eigenvalue for any given \u20d7 ek. Thus, A\u20d7 ek = \u03bbk\u20d7 ek. Granted we\nknow these eigenvalues and eigenvectors, then we can plug (16.4) into thet= 1 version of (16.2) to \ufb01nd\n\u20d7 p(1) =c1\u03bb1\u20d7 e1 + c2\u03bb2\u20d7 e2 + \u00b7\u00b7\u00b7 . (16.5)\nWe could then plug this into the t= 2 version of (16.2) to \ufb01nd that", "metadata": {"page": 103}}, {"page_content": "We could then plug this into the t= 2 version of (16.2) to \ufb01nd that\n\u20d7 p(2) =c1\u03bb2\n1\u20d7 e1 + c2\u03bb2\n2\u20d7 e2 + \u00b7\u00b7\u00b7 , (16.6)\nand so on. In general, this then gives\n\u20d7 p(t) =c1\u03bbt\n1\u20d7 e1 + c2\u03bbt\n2\u20d7 e2 + \u00b7\u00b7\u00b7 . (16.7)\n16.2 Transition/Markov matrices\nThe matrix Athat appears in (16.2) has entries that are conditional probabilities, and this has the following implica-\ntions:\n\u2022All entries are non-negative.\n\u2022A0,m + A1,m + \u00b7\u00b7\u00b7 + AN,m = 1 for all m\u2208{0,1,...,N }.\n(16.8)", "metadata": {"page": 103}}, {"page_content": "tions:\n\u2022All entries are non-negative.\n\u2022A0,m + A1,m + \u00b7\u00b7\u00b7 + AN,m = 1 for all m\u2208{0,1,...,N }.\n(16.8)\nThe last line above asserts that the probability is 1 of there being some number, either 0 or 1 or 2 or ... or N of red\ncells in the subsequent generation given mred cells in the initial generation.\nA square matrix with this property is called a transition matrix, or sometimes a Markov matrix. When Ais such a\nmatrix, the equation in (16.2) is called a Markov process.", "metadata": {"page": 103}}, {"page_content": "matrix, the equation in (16.2) is called a Markov process.\nAlthough we are interested in the eigenvalues of A, it is amusing to note that the transpose matrix, AT, has an\neigenvalue equal to 1 with the corresponding eigenvector being proportional to the vector, \u20d7 a, with each entry equal to\n1. Indeed, the entry in the mth row and nth column of AT is Anm, this the entry of Ain the nth row and mth column.\nThus, the mth entry of AT\u20d7 ais\n(AT\u20d7 a)m = A0,ma0 + A1,ma1 + \u00b7\u00b7\u00b7 + AN,maN. (16.9)", "metadata": {"page": 103}}, {"page_content": "Thus, the mth entry of AT\u20d7 ais\n(AT\u20d7 a)m = A0,ma0 + A1,ma1 + \u00b7\u00b7\u00b7 + AN,maN. (16.9)\nIf the lower line in (16.10) is used and if each ak is 1, then each entry of AT\u20d7 ais also 1. Thus, AT\u20d7 a= \u20d7 a.\nAs a \u201ccultural\u201d aside, what follows is the story on Anm in the example from Section 16.1. First, Anm = 0 if nis\nlarger than 2msince mparent cells can spawn at most 2msurvivors. For n\u2264m, consider that you have 2N cells of", "metadata": {"page": 103}}, {"page_content": "which 2mare red and you ask for the probability that a choice of N from the 2N cells results in nred ones. This is a\ncounting problem that is much like those discussed in Section 11.3 although more complicated. The answer here is:\nAnm = (N!)2\n(2N)!\n(2m)!\nn! (2m\u2212n)!\n(2N \u22122m)!\n(N \u2212m)! (N + n\u22122m)! when 0 \u2264n\u22642m. (16.10)\n16.3 Another protein folding example\nHere is another model for protein folding. As you may recall from previous chapters, a protein is made of segments", "metadata": {"page": 103}}, {"page_content": "tied end to end as a chain. Each segment is one of 20 amino acids. The protein is made by the cell in a large and\ncomplex molecule called a ribosome. The segments are attached one after the other in the ribosome and so the chain\ngrows, link by link. Only a few segments are in the ribosome, and the rest stick out as the protein grows. As soon\nas a joint between two segments is free of the ribosome, it can bend if it is not somehow stabilized by surrounding\n100 Chapter 16. Eigenvalues in biology", "metadata": {"page": 103}}, {"page_content": "molecules. Suppose that the bend at a joint can be in one of n directions as measured relative to the direction of\nthe previously made segment. A simplistic hypothesis has the direction of the bend of any given segment in\ufb02uenced\nmostly by the bend direction of the previously made segment.\nTo model this, let me introduce, for k,j \u2208{1,...,n }, the conditional probability, Ak,j that the a given segment has", "metadata": {"page": 104}}, {"page_content": "bend direction kwhen the previously made segment has bend direction j. Next, agree to label the segments along the\nprotein by the integers in the set {1,...,N }. If zdenotes such an integer, let pk(z) denote the probability that the zth\nsegment is bent in the kth direction relative to the angle of the z\u22121st segment. Then we have\npk(z) =\nn\u2211\nj=1\nAk,jpj(z\u22121). (16.11)\nI can now introduce the vector \u20d7 p(z) in Rn whose kth component is pk(z), and also the square matrix A with the", "metadata": {"page": 104}}, {"page_content": "components Ak,j. Then (16.11) is the equation\n\u20d7 p(z) = A\u20d7 p(z\u22121). (16.12)\nNote that as in the previous case, the matrixAis a Markov matrix. This is to say that eachAkj is non-negative because\nthey are conditional probabilities; and\nA1,j + A2,j + \u00b7\u00b7\u00b7 + An,j = 1 (16.13)\nbecause the segment must be at some angle or other.\nHere is an explicit example: Taken= 3 so that Ais a 3 \u00d73 matrix. In particular, take\nA=\n\uf8eb\n\uf8ec\uf8ed\n1\n2\n1\n4\n1\n4\n1\n4\n1\n2\n1\n4\n1\n4\n1\n4\n1\n2\n\uf8f6\n\uf8f7\uf8f8. (16.14)", "metadata": {"page": 104}}, {"page_content": "A=\n\uf8eb\n\uf8ec\uf8ed\n1\n2\n1\n4\n1\n4\n1\n4\n1\n2\n1\n4\n1\n4\n1\n4\n1\n2\n\uf8f6\n\uf8f7\uf8f8. (16.14)\nAs it turns out, A has the following eigenvectors:\n\u20d7 e1 = 1\u221a\n3\n\uf8eb\n\uf8ed\n1\n1\n1\n\uf8f6\n\uf8f8, \u20d7 e 2 = 1\n\u221a\n2\n\uf8eb\n\uf8ed\n\u22121\n1\n0\n\uf8f6\n\uf8f8 and \u20d7 e3 = 1\n\u221a\n6\n\uf8eb\n\uf8ed\n1\n1\n\u22122\n\uf8f6\n\uf8f8 (16.15)\nwith corresponding eigenvalues \u03bb1 = 1, \u03bb2 = 1\n4 and \u03bb3 = 1\n4 . Note that the vectors in (16.15) are mutually orthogonal\nand have norm 1, so they de\ufb01ne an orthonormal basis for R3. Thus, any given z version of \u20d7 p(z) can be written as a", "metadata": {"page": 104}}, {"page_content": "linear combination of the vectors in (16.15). Doing so, we write\n\u20d7 p(z) = c1(z)\u20d7 e1 + c2(z)\u20d7 e2 + c3(z)\u20d7 e3 (16.16)\nwhere each ck(z) is some real number. In this regard, there is one point to make straight away, which is that c1(z)\nmust equal 1\u221a\n3 when the entries of \u20d7 p(z) represent probabilities that sum to 1. To explain, keep in mind that the basis\nin (16.15) is an orthonormal basis, and this implies that \u20d7 e1 \u00b7\u20d7 p= c1(z). However, since each entry of \u20d7 e1 is equal to\n1\u221a", "metadata": {"page": 104}}, {"page_content": "1\u221a\n3 , this dot product is 1\u221a\n3 (p1(z) +p2(z) +p3(z)) and so equals 1\u221a\n3 when p1 + p2 + p3 = 1 as is the case when these\ncoef\ufb01cients are probabilities.\nIn any event, if you plug the expression in (16.16) into the left side of (16.12) and use the analogous z\u22121 version on\nthe right side, you will \ufb01nd that the resulting equation holds if and only if the coef\ufb01cients obey\nc1(z) = c1(z\u22121), c 2(z) = 1\n4c2(z\u22121) and c3(z) = 1\n4c3(z\u22121). (16.17)", "metadata": {"page": 104}}, {"page_content": "c1(z) = c1(z\u22121), c 2(z) = 1\n4c2(z\u22121) and c3(z) = 1\n4c3(z\u22121). (16.17)\nNote that the equality here betweenc1(z) and c1(z\u22121) is heartening in as much as both of them are supposed to equal\n1\u221a\n3 . Anyway, continue by iterating (16.17) by writing the z\u22121 versions of ck in terms of the z\u22122 versions, then the\nlatter in terms of the z\u22123 versions, and so on until you obtain\nc1(z) = c1(0), c 2(z) =\n(1\n4\n)z\nc2(0) and c3(z) =\n(1\n4\n)z\nc3(0). (16.18)\n16.3. Another protein folding example 101", "metadata": {"page": 104}}, {"page_content": "Thus, we see from (16.18) we have\n\u20d7 p(z) = 1\n3\n\uf8eb\n\uf8ec\uf8ed\n1\n1\n1\n\uf8f6\n\uf8f7\uf8f8+\n(1\n4\n)z\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u22121\u221a\n2 c2(0) + 1\u221a\n6 c3(0)\n1\u221a\n2 c2(0) + 1\u221a\n6 c3(0)\u221a\n2\n3 c3(0)\n\uf8f6\n\uf8f7\uf8f7\uf8f8. (16.19)\nBy the way, take note of how the probabilities for the three possible fold directions come closer and closer to being\nequal as zincreases even if the initial z= 0 probabilities were drastically skewed to favor one or the other of the three\ndirections. For example, suppose that\n\u20d7 p(0) =\n\uf8eb\n\uf8ed\n1\n0\n0\n\uf8f6\n\uf8f8 (16.20)\nAs a result, c1(0) = 1\n\u221a", "metadata": {"page": 105}}, {"page_content": "directions. For example, suppose that\n\u20d7 p(0) =\n\uf8eb\n\uf8ed\n1\n0\n0\n\uf8f6\n\uf8f8 (16.20)\nAs a result, c1(0) = 1\n\u221a\n3 , c2(0) = \u22121\u221a\n2 and c3(0) = 1\u221a\n6 . Thus, we have\n\u20d7 p(z) = 1\n3\n\uf8eb\n\uf8ec\uf8ed\n1\n1\n1\n\uf8f6\n\uf8f7\uf8f8+\n(1\n4\n)z\n\uf8eb\n\uf8ec\uf8ed\n2\n3\n\u22121\n3\n\u22121\n3\n\uf8f6\n\uf8f7\uf8f8. (16.21)\nThus, by z= 4, the three directions differ in probability by less than 1%.\n16.4 Exercises:\n1. Multiply the matrix Ain (16.14) against the vector \u20d7 p(z) in (16.19) and verify that the result is equal to \u20d7 p(z+ 1)\nas de\ufb01ned by replacing zby z+ 1 in (16.19).", "metadata": {"page": 105}}, {"page_content": "as de\ufb01ned by replacing zby z+ 1 in (16.19).\n2. Let Adenote the 2 \u00d72 matrix\n(1\n3\n2\n3\n2\n3\n1\n3\n)\n. The vectors \u20d7 e1 = 1\u221a\n2\n(1\n1\n)\nand \u20d7 e2 = 1\u221a\n2\n(\u22121\n1\n)\nare eigenvectors of A.\n(a) Compute the eigenvalues of \u20d7 e1 and \u20d7 e2.\n(b) Suppose z\u2208{0,1,... }and \u20d7 p(z+ 1) = A\u20d7 p(z) for all z. Find \u20d7 p(z) if \u20d7 p(0) =\n(1\n4\n3\n4\n)\n.\n(c) Find \u20d7 p(z) if \u20d7 p(0) =\n(0\n1\n)\n.\n(d) Find \u20d7 p(z) in the case that \u20d7 p(z+ 1) =\n(1\n\u22121\n)\n+ A\u20d7 p(z) for all zand \u20d7 p(0) =\n(0\n1\n)\n.", "metadata": {"page": 105}}, {"page_content": "1\n)\n.\n(d) Find \u20d7 p(z) in the case that \u20d7 p(z+ 1) =\n(1\n\u22121\n)\n+ A\u20d7 p(z) for all zand \u20d7 p(0) =\n(0\n1\n)\n.\n3. Suppose that you have a model to explain your data that predicts the probability of a certain measurement having\nany prescribed value. Suppose that this probability function has mean 1 and standard deviation 2.\n(a) Give an upper bound for the probability of a measurement being greater than 5.", "metadata": {"page": 105}}, {"page_content": "(a) Give an upper bound for the probability of a measurement being greater than 5.\n(b) Suppose that you average some very large number, N, of measurements that are taken in unrelated, but\nidentical versions of the same experimental set up. Write down a Gaussian probability function that you\ncan use to estimate the probability that the value of this average is greater than 5. In particular, give a\nnumerical estimate using this Gaussian function for N = 100.", "metadata": {"page": 105}}, {"page_content": "numerical estimate using this Gaussian function for N = 100.\n(c) Let us agree that you will throw out your proposed model if it predicts that the probability for \ufb01nding an\naverage value that is greater than your measured average for 100 measurements is less than 1\n20 . If your\nmeasured average is 1.6 for 100 experiments, should you junk your model?\n102 Chapter 16. Eigenvalues in biology", "metadata": {"page": 105}}, {"page_content": "4. Suppose that you repeat the same experiment 100 times and each time record the value of a certain key measure-\nment. Let {x1,...,x 100}denote the values of the measurement in N experiments. Suppose that\n100\u2211\nk=1\nxk = 0\nand 1\n99\n100\u2211\nk=1\nx2\nk = 1. Suppose, in addition, that \ufb01ve of the xk obey xk > 3. The purpose of this problem is to\nwalk you through a method for obtaining an upper bound for the likelihood of having \ufb01ve measurements that far", "metadata": {"page": 106}}, {"page_content": "from the mean. In this regard, suppose that you make the hypothesis that the spread of the measured numbers\n{xk}1\u2264k\u2264100 is determined by the Gaussian probability function with mean 0 and standard deviation equal to\n1.\n(a) Let p denote the probability using this Gaussian probability function for a measurement x with x \u22653.\nExplain why p\u2264e\u22129/2 \u22640.0015.\n(b) Use the binomial probability function to explain why the probability of \ufb01nding \ufb01ve or more values for k\nout of 100 that have xk >3 is equal to", "metadata": {"page": 106}}, {"page_content": "out of 100 that have xk >3 is equal to\n100\u2211\nk=5\n100!\nk! (100 \u2212k)!pk(1 \u2212p)100\u2212k.\n(c) Let k\u22655 and let z\u2192fk(z) = zk(1 \u2212z)100\u2212k. Show that f is an increasing function of zwhen z <1\n20 .\n(Hint: Take its derivative with respect to z.)\n(d) Since p\u22640.0015 \u22640.05 = 1\n20 , use the result from part (c) to conclude that the probability of \ufb01nding 5 or\nmore of the xk\u2019s out of100 with xk >3 has probability less than\n100\u2211\nk=5\n(0.0015)k(0.9985)100\u2212k.", "metadata": {"page": 106}}, {"page_content": "more of the xk\u2019s out of100 with xk >3 has probability less than\n100\u2211\nk=5\n(0.0015)k(0.9985)100\u2212k.\n(e) We saw in Equation (11.19) of Chapter 11 that the terms in the preceding sum get ever smaller as k\nincreases. Use a calculator to show that the k = 5 term and thus all higher k terms are smaller than\n5 \u00d710\u22127. Since there are less than 95 \u2264100 of these terms prove that the sum of these terms is no greater\nthan 0.00005.\n16.4. Exercises 103", "metadata": {"page": 106}}, {"page_content": "CHAPTER\nSEVENTEEN\nMore about Markov matrices\nThe notion of a Markov matrix was introduced in Chapter 15. By way of a reminder, this is an N \u00d7N matrix Athat\nobeys the following conditions:\n\u2022Ajk \u22650 for all jand k.\n\u2022A1k + A2k + \u00b7\u00b7\u00b7 + ANk = 1 for all k.\n(17.1)\nThese two conditions are suf\ufb01cient (and also necessary) for the interpretation of the components of Aas conditional\nprobabilities. To this end, imagine a system with N possible states, labeled by the integers starting at 1. Then Ajk can", "metadata": {"page": 107}}, {"page_content": "represent the conditional probability that the system is in state jat time tif it is in state kat time t\u22121. In particular,\none sees Markov matrices arising in a dynamical system where the probabilities for the various states of the system at\nany given time are represented by an N-component vector, \u20d7 p(t), that evolves in time according to the formula\n\u20d7 p(t) =A\u20d7 p(t\u22121). (17.2)\nHere is a 3 \u00d73 Markov matrix \uf8ee\n\uf8ef\uf8f0\n1\n4\n1\n3\n1\n6\n1\n2\n1\n3\n2\n3\n1\n4\n1\n3\n1\n6\n\uf8f9\n\uf8fa\uf8fb. (17.3)", "metadata": {"page": 107}}, {"page_content": "Here is a 3 \u00d73 Markov matrix \uf8ee\n\uf8ef\uf8f0\n1\n4\n1\n3\n1\n6\n1\n2\n1\n3\n2\n3\n1\n4\n1\n3\n1\n6\n\uf8f9\n\uf8fa\uf8fb. (17.3)\nAll entries are non-negative, and the entries in each column sum to 1. Note that there is no constraint for the sum of\nthe entries in any given row.\nA question that is often raised in the general context of (17.1) and (17.2) is whether the system has an equilibrium\nprobability function, thus some non-zero vector \u20d7 p\u2217, with non-negative entries that sum to 1, and that obeys A\u20d7 p\u2217= \u20d7 p\u2217.", "metadata": {"page": 107}}, {"page_content": "If so, there is the associated question of whether the t\u2192\u221e limit of \u20d7 p(t)must necessarily converge to the equilibrium\n\u20d7 p\u2217.\n104", "metadata": {"page": 107}}, {"page_content": "Here are some facts that allow us to answer these questions.\n\u2022Any Markov matrix always has an eigenvector with eigenvalue1.\n\u2022If each entry of a Markov matrix Ais strictly positive, then every non-zero vector\nin the kernel of A\u2212I has either all positive entries or all negative entries. Here, I\ndenotes the identity matrix.\n\u2022If each entry of a Markov matrix Ais strictly positive, there is a vector in the kernel\nof the matrix A\u2212I whose entries are positive and sum to 1.", "metadata": {"page": 108}}, {"page_content": "of the matrix A\u2212I whose entries are positive and sum to 1.\n\u2022If each entry of a Markov matrix Ais strictly positive, then the kernel of A\u2212I is\none dimensional. Furthermore, there is unique vector in the kernel of A\u2212I whose\nentries are positive and sum to 1.\n\u2022If each entry ofAis strictly positive, all other eigenvalues have absolute value strictly\nless than 1. Moreover, the entries of any eigenvector for an eigenvalue that is less\nthan 1 must sum to zero.\n(17.4)", "metadata": {"page": 108}}, {"page_content": "than 1 must sum to zero.\n(17.4)\nI elaborate on these points in Section 17.2, below. Accept them for the time being.\n17.1 Solving the equation\nLet me suppose that Ais a Markov matrix and that none of its entries are zero. This allows me to use all of the facts\nthat are stated in (17.4). Let me also suppose that Ahas a basis of eigenvectors. In this case, (17.2) can be solved\nusing the basis of eigenvectors. To make this explicit, I denote this basis of eigenvectors as {\u20d7 e1,...,\u20d7 en}and I use \u03bbk", "metadata": {"page": 108}}, {"page_content": "to denote the eigenvector of \u20d7 ek. Here, my convention has \u03bb1 = 1 and I take \u20d7 e1 to be the eigenvector with eigenvalue\n1 whose entries sum to 1. Note that I am using the \ufb01rst four facts in (17.4) to conclude that Amust have a unique\neigenvector with eigenvalue 1 and positive entries that sum to 1.\nNow, the solution to (17.2) depends on the starting vector, \u20d7 p(0). In the context where this is a vector of probabilities,", "metadata": {"page": 108}}, {"page_content": "it can not have a negative entry. Moreover, its entries must sum to1. As explained below, this requires that\n\u20d7 p(0) =\u20d7 e1 +\nn\u2211\nk=2\nck\u20d7 ek. (17.5)\nThe point is that the coef\ufb01cient in front of \u20d7 e1 is necessarily equal to 1. The coef\ufb01cient, ck, in front of any k \u22652\nversion of \u20d7 ek is not so constrained.\nHere is why (17.5) must hold: In general, I can write \u20d7 p(0) =c1\u20d7 e1 + \u2211n\nk=2 ck\u20d7 ek where c1,c2,...,c n are real numbers", "metadata": {"page": 108}}, {"page_content": "k=2 ck\u20d7 ek where c1,c2,...,c n are real numbers\nbecause I am assuming that Ais diagonalizable, and the eigenvectors of any diagonalizable matrix comprise a basis. It\nfollows from this representation of \u20d7 p(0)that the sum of its entries is obtained by adding the following numbers: First,\nc1 times the sum of the entries of \u20d7 e1, then c2 times the sum of the entries \u20d7 e2, then c3 times the sum of the entries of", "metadata": {"page": 108}}, {"page_content": "\u20d7 e3, and so on. However, the last point of (17.4) asserts that the sum of the entries of each k\u22652 version of \u20d7 ek is zero.\nThis means that the sum of the entries of \u20d7 p(0)is c1 times the sum of the entries of \u20d7 e1. Since the sum of the entries of\n\u20d7 e1 is 1 and since this is also the sum of the entries of \u20d7 p(0), soc1 must equal 1. This is what is asserted by (17.5).\nBy way of an example, consider the 2 \u00d72 case where\nA=\n[1\n4\n1\n2\n3\n4\n1\n2\n]\n. (17.6)\nThe eigenvalues in this case are 1 and \u22121", "metadata": {"page": 108}}, {"page_content": "A=\n[1\n4\n1\n2\n3\n4\n1\n2\n]\n. (17.6)\nThe eigenvalues in this case are 1 and \u22121\n4 and the associated eigenvectors \u20d7 e1 and \u20d7 e2 in this case can be taken to be\n\u20d7 e1 =\n[2\n5\n3\n5\n]\nand \u20d7 e2 =\n[1\n\u22121\n]\n. (17.7)\n17.1. Solving the equation 105", "metadata": {"page": 108}}, {"page_content": "For this 2 \u00d72 example, the most general form for \u20d7 p(0)that allows it to be a vector of probabilities is\n\u20d7 p(0) =\n[2\n5\n3\n5\n]\n+ c2\n[1\n\u22121\n]\n=\n[2\n5 + c2\n3\n5 \u2212c2\n]\n. (17.8)\nwhere c2 can be any number that obeys \u22122\n5 \u2264c2 \u22643\n5 .\nReturning to the general case, it then follows from (17.2) and (17.4) that\n\u20d7 p(t) =\u20d7 e1 +\nn\u2211\nk=2\nck\u03bbt\nk\u20d7 ek. (17.9)\nThus, as t\u2192\u221e, we see that in the case where each entry of Ais positive, the limit is\nlim\nt\u2192\u221e\n\u20d7 p(0) =\u20d7 e1. (17.10)", "metadata": {"page": 109}}, {"page_content": "lim\nt\u2192\u221e\n\u20d7 p(0) =\u20d7 e1. (17.10)\nNote in particular that \u20d7 p(t)at large tis very nearly \u20d7 e1. Thus, if you are interested only in the large tbehavior of \u20d7 p(t),\nyou need only \ufb01nd one eigenvector!\nIn our 2 \u00d72 example,\n\u20d7 p(t) =\n[2\n5 +\n(\n\u22121\n4\n)t\nc2\n3\n5 \u2212\n(\n\u22121\n4\n)t\nc2\n]\n. (17.11)\nIn the example provided by (17.3), the matrix has eigenvalues 1, 0 and \u22121\n4 . If I use \u20d7 e2 for the eigenvector with\neigenvalue 0 and \u20d7 e3 for the eigenvector with eigenvalue \u22121", "metadata": {"page": 109}}, {"page_content": "eigenvalue 0 and \u20d7 e3 for the eigenvector with eigenvalue \u22121\n4 , then the solution with \u20d7 p(0) =\u20d7 e1 + c2\u20d7 e2 + c3\u20d7 e3 is\n\u20d7 p(t) =\u20d7 e1 + c3\n(\n\u22121\n4\n)t\n\u20d7 e3 for t> 0. (17.12)\nAs I remarked above, I need only \ufb01nd \u20d7 e1 to discern the large tbehavior of \u20d7 p(t); and in the example using the matrix\nin (17.3),\n\u20d7 e1 =\n\uf8ee\n\uf8ef\uf8f0\n4\n15\n7\n15\n4\n15\n\uf8f9\n\uf8fa\uf8fb. (17.13)\n17.2 Proving things about Markov matrices", "metadata": {"page": 109}}, {"page_content": "in (17.3),\n\u20d7 e1 =\n\uf8ee\n\uf8ef\uf8f0\n4\n15\n7\n15\n4\n15\n\uf8f9\n\uf8fa\uf8fb. (17.13)\n17.2 Proving things about Markov matrices\nMy goal here is to convince you that a Markov matrix obeys the facts that are stated by the various points of (17.4).\nPoint 1: As noted, an equilibrium vector for Ais a vector, \u20d7 p, that obeysA\u20d7 p= \u20d7 p. Thus, it is an eigenvector ofA\nwith eigenvalue 1. Of course, we have also imposed other conditions, such as its entries must be non-negative and", "metadata": {"page": 109}}, {"page_content": "they should sum to 1. Even so, the \ufb01rst item to note is that Adoes indeed have an eigenvector with eigenvalue 1. To\nsee why, observe that the vector \u20d7 wwith entries all equal to 1 obeys\nAT \u20d7 w= \u20d7 w (17.14)\nby virtue of the second condition in (17.1). Indeed, if kis any given integer, then the kth entry of AT is A1k + A2k +\n\u00b7\u00b7\u00b7 + ANk and this sum is assumed to be equal to 1, which is the kth entry of \u20d7 w.", "metadata": {"page": 109}}, {"page_content": "\u00b7\u00b7\u00b7 + ANk and this sum is assumed to be equal to 1, which is the kth entry of \u20d7 w.\nThis point about AT is relevant since det(AT \u2212\u03bbI) = det(A\u2212\u03bbI) for any real number \u03bb. Because AT \u2212I is not\ninvertible, det(AT \u2212I) is zero. Thus, det(A\u2212I) is also zero and so A\u2212I is not invertible. Thus it has a positive\ndimensional kernel. Any non-zero vector in this kernel is an eigenvector for Awith eigenvalue 1.\n106 Chapter 17. More about Markov matrices", "metadata": {"page": 109}}, {"page_content": "Point 2: I assume for this point that all of A\u2019s entries are positive. Thus, all Ajk obey the condition Ajk > 0. I\nhave to demonstrate to you that there is no vector in the kernel of A\u2212I with whose entries are not all positive or all\nnegative. To do this, I will assume that I have a vector \u20d7 vthat violates this conclusion and demonstrate why this last\nassumption is untenable. To see how this is going to work, suppose \ufb01rst that only the \ufb01rst entry of\u20d7 vis zero or negative", "metadata": {"page": 110}}, {"page_content": "and the rest are non-negative with at least one positive. Since \u20d7 vis an eigenvector of Awith eigenvalue 1,\nv1 = A11v1 + A12v2 + \u00b7\u00b7\u00b7 + A1nvn. (17.15)\nAs a consequence, (17.15) implies that\n(1 \u2212A11)v1 = A12v2 + \u00b7\u00b7\u00b7 + A1nvn (17.16)\nNow, A11 is positive, but it is less than 1 since it plus A21 + A31 + \u00b7\u00b7\u00b7 + An1 give 1 and each of A21,...,A n1 is\npositive. This implies that the left-hand side of (17.16) is negative if it were the case that v1 < 0, and it is zero if v1", "metadata": {"page": 110}}, {"page_content": "were equal to zero. Meanwhile, the right-hand side to (17.16) is strictly positive. Indeed, at least one k >1 version\nof vk is positive and its attending A1k is positive, while no k >1 versions of vk are negative nor are any Ajk. Thus,\nwere it the case that v1 is not strictly positive, then (17.16) equates a negative or zero left-hand side with a positive\nright-hand side. Since this is nonsense, I see that I could never have an eigenvector of A with eigenvalue 1 that had", "metadata": {"page": 110}}, {"page_content": "one negative or zero entry with the rest non-negative with one or more positive.\nHere is the argument when two or more of thevks are negative or zero and the rest are greater than or equal to zero with\nat least one positive: Let\u2019s suppose for simplicity of notation thatv1 and v2 are either zero or negative, and that rest of\nthe vks are either zero or positive. Suppose also that at least one k \u22653 version of vk is positive. Along with (17.15),\nwe have", "metadata": {"page": 110}}, {"page_content": "we have\nv2 = A21v1 + A22v2 + A23v3 + \u00b7\u00b7\u00b7 + A2nvn. (17.17)\nNow add this equation to (17.15) to obtain\nv1 + v2 = (A11 + A21)v1 + (A12 + A22)v2 + (A13 + A23)v3 + \u00b7\u00b7\u00b7 + (A1n + A2n)vn. (17.18)\nAccording to (17.1), the sum A11 +A21 is positive and strictly less than1 since it plus the strictly positiveA31 +\u00b7\u00b7\u00b7+\nAn1 is equal to 1. Likewise, A21 + A22 is strictly less than 1. Thus, when I rearrange (17.18) as\n(1 \u2212A11 \u2212A21)v1 + (1 \u2212A21 \u2212A22)v2 = (A13 + A23)v3 + \u00b7\u00b7\u00b7 + (A1n + A2n)vn, (17.19)", "metadata": {"page": 110}}, {"page_content": "(1 \u2212A11 \u2212A21)v1 + (1 \u2212A21 \u2212A22)v2 = (A13 + A23)v3 + \u00b7\u00b7\u00b7 + (A1n + A2n)vn, (17.19)\nI again have an expression where the left-hand side is negative or zero and where the right hand side is greater than\nzero. Of course, such a thing can\u2019t arise, so I can conclude that the case with two negative versions of vk and one or\nmore positive versions can not arise.\nThe argument for the general case is very much like this last argument so I walk you through it in one of the exercises.", "metadata": {"page": 110}}, {"page_content": "Point 3: I know from Point 1 that there is at least one non-zero eigenvector of Awith eigenvalue 1. I also know,\nthis from Point 2, that either all of its entries are negative or else all are positive. If all are negative, I can multiply it\nby \u22121 so as to obtain a new eigenvector of Awith eigenvalue 1 that has all positive entries. Let rdenote the sum of\nthe entries of the latter vector. If I now multiply this vector by 1\nr, I get an eigenvector whose entries are all positive\nand sum to 1.", "metadata": {"page": 110}}, {"page_content": "r, I get an eigenvector whose entries are all positive\nand sum to 1.\nPoint 4: To prove this point, let me assume, contrary to the assertion, that there are two non-zero vectors in the\nkernel of A\u2212I and one is not a multiple of the other. Let me call them \u20d7 vand \u20d7 u. As just explained, I can arrange that\nboth have only positive entries and that their entries sum to 1, this by multiplying each by an appropriate real number.", "metadata": {"page": 110}}, {"page_content": "Now, if\u20d7 vis not equal to \u20d7 u, then some entry of one must differ from some entry of the other. For the sake of argument,\nsuppose that v1 <u1. Since the entries sum to 1, this then means that some other entry of \u20d7 vmust be greater than the\ncorresponding entry of \u20d7 u. For the sake of argument, suppose that v2 > u2. As a consequence the vector \u20d7 v\u2212\u20d7 uhas\nnegative \ufb01rst entry and positive second entry. It is also in the kernel of A\u2212I. But, these conclusions are untenable", "metadata": {"page": 110}}, {"page_content": "since I already know that every vector in the kernel of A\u2212I has either all positive entries or all negative ones. The\nonly escape from this logical nightmare is to conclude that \u20d7 vand \u20d7 uare equal.\nThis then demonstrates two things: First, there is a unique vector in the kernel ofA\u2212Iwhose entries are positive and\nsum to 1. Second, any one vector in kernel A\u2212I is a scalar multiple of any other and so this kernel has dimension 1.\n17.2. Proving things about Markov matrices 107", "metadata": {"page": 110}}, {"page_content": "Point 5 : Suppose here that \u03bbis an eigenvalue of Aand that \u03bb >1 or that \u03bb\u2264\u22121. I need to demonstrate that this\nassumption is untenable and I will do this by deriving some patent nonsense by taking it to be true. Let me start by\nsupposing only that \u03bbis some eigenvector of Awith out making the assumption about its size. Let \u20d7 vnow represent\nsome non-zero vector in the kernel of A\u2212\u03bbI. Thus, \u03bb\u20d7 v= A\u20d7 v. If I sum the entries on both sides of this equation, I\n\ufb01nd that", "metadata": {"page": 111}}, {"page_content": "\ufb01nd that\n\u03bb(v1 + \u00b7\u00b7\u00b7 + vn) = (A11 + \u00b7\u00b7\u00b7 + An1)v1 + (A12 + \u00b7\u00b7\u00b7 + An2)v2 + \u00b7\u00b7\u00b7 + (A1n + \u00b7\u00b7\u00b7 + Ann)vn. (17.20)\nAs a consequence of the second point in (17.1), this then says that\n\u03bb(v1 + \u00b7\u00b7\u00b7 + vn) = v1 + \u00b7\u00b7\u00b7 + vn. (17.21)\nThus, either \u03bb = 1 or else the entries of \u20d7 vsum to zero. This is what is asserted by the second sentence of the \ufb01nal\npoint in (17.4).\nNow suppose that \u03bb >1. In this case, the argument that I used in the discussion above for Point 2 can be reapplied", "metadata": {"page": 111}}, {"page_content": "with only minor modi\ufb01cations to produce the ridiculous conclusion that something negative is equal to something\npositive. To see how this works, remark that the conclusion that \u20d7 v\u2019s entries sum to zero implies that has at least one\nnegative entry and at least one positive one. For example, suppose that the \ufb01rst entry of \u20d7 vis negative and the rest are\neither zero or positive with at least one positive. Since \u20d7 vis an eigenvector with eigenvalue \u03bb, we have", "metadata": {"page": 111}}, {"page_content": "\u03bbv1 = A11v1 + A12v2 + \u00b7\u00b7\u00b7 + A1nvn, (17.22)\nand thus\n(\u03bb\u2212A11)v1 = A12v2 + \u00b7\u00b7\u00b7 + A1nvn. (17.23)\nNote that this last equation is the analog in the \u03bb >1 case of (17.15). Well since \u03bb >1 and A11 < 1, the left-hand\nside of (17.23) is negative. Meanwhile, the right-hand side is positive since each A1k that appears here is positive and\nsince at least one k\u22652 version of vk is positive.\nEquation (17.19) has its \u03bb> 1 analog too, this where the (1 \u2212A11 \u2212A21) is replaced by (\u03bb\u2212A11 \u2212A21) and where", "metadata": {"page": 111}}, {"page_content": "(1 \u2212A12 \u2212A22) is replaced by (\u03bb\u2212A12 \u2212A22). The general case where has some m<n negative entries and the\nrest zero or positive is ruled out by these same sorts of arguments.\nConsider now the case where \u03bb\u2264\u22121. I can rule this out using the trick of introducing the matrix A2 = A\u00b7A. This is\ndone in three steps.\nStep 1: If A obeys (17.1) then so does A2. If all entries of Aare positive, then this is also the case for A2. To see", "metadata": {"page": 111}}, {"page_content": "that all of this is true, note that the \ufb01rst point in (17.1) holds since each entry of A2 is a sum of products of the entries\nof Aand each of the latter is positive. As for the second point in (17.1), note that\nn\u2211\nm=1\n(A2)mk =\nn\u2211\nm=1\n\uf8eb\n\uf8ed \u2211\n1\u2264j\u2264n\nAmjAjk\n\uf8f6\n\uf8f8. (17.24)\nNow switch the orders of summing so as to make the right-hand side read\nn\u2211\nm=1\n(A2)mk =\nn\u2211\nj=1\n( n\u2211\nm=1\nAmjAjk\n)\n. (17.25)", "metadata": {"page": 111}}, {"page_content": "n\u2211\nm=1\n(A2)mk =\nn\u2211\nj=1\n( n\u2211\nm=1\nAmjAjk\n)\n. (17.25)\nThe sum inside the parentheses is 1 for each j because Aobeys the second point in (17.1). Thus, the right-hand side\nof (17.24) is equal to\nn\u2211\nj=1\nAjk, (17.26)\nand such a sum is equal to 1, again due to the second point in (17.1).\n108 Chapter 17. More about Markov matrices", "metadata": {"page": 111}}, {"page_content": "Step 2: Now, if \u20d7 vis an eigenvector of Awith eigenvalue \u03bb, then \u20d7 vis an eigenvector of A2 with eigenvalue \u03bb2. In\nthe case that \u03bb <\u22121, then \u03bb2 >1. Since A2 obeys (17.1) and all of its entries are positive, we know from what has\nbeen said so far that it does not have eigenvalues that are greater than 1. Thus, Ahas no eigenvalues that are less than\n\u22121.\nStep 3: To see that \u22121 is not an eigenvalue for A, remember that if \u20d7 vwere an eigenvector with this eigenvalue,", "metadata": {"page": 112}}, {"page_content": "then its entries would sum to zero. But \u20d7 vwould also be an eigenvector of A2 with eigenvalue 1 and we know that\nthe entries of any such eigenvector must either be all positive or all negative. Thus, Acan\u2019t have\u22121 as an eigenvalue\neither.\n17.3 Exercises:\n1. The purpose of this exercise is to walk you through the argument for the second point in (17.4). To start, assume\nthat obeys A\u20d7 v= \u20d7 vand that the \ufb01rst k <nentries of are are negative or zero and the rest either zero or positive", "metadata": {"page": 112}}, {"page_content": "with at least one positive.\n(a) Add the \ufb01rst k entries of the vector A\u20d7 vand write the resulting equation asserting that the latter sum is\nequal to that of the \ufb01rst kentries of \u20d7 v. In the case k= 2, this is (17.18).\n(b) Rewrite the equation that you got from (a) so that all terms that involve v1, v2, ..., and vk are on the\nleft-hand side and all terms that involve vk+1, ..., vn are on the right-hand side. In the case k = 2, this\nis (17.19).", "metadata": {"page": 112}}, {"page_content": "is (17.19).\n(c) Explain why the left-hand side of the equation that you get in (b) is negative or zero while the right-hand\nside is positive.\n(d) Explain why the results from (c) forces you to conclude that every eigenvector of Awith eigenvalue 1 has\nentries that are either all positive or all negative.\n2. (a) Consider the matrix A =\n\uf8ee\n\uf8ef\uf8f0\n2\n3 a b\na 2\n3 c\nb c 2\n3\n\uf8f9\n\uf8fa\uf8fb. Find all possible values for a, b and c that make this a Markov\nmatrix.", "metadata": {"page": 112}}, {"page_content": "3 a b\na 2\n3 c\nb c 2\n3\n\uf8f9\n\uf8fa\uf8fb. Find all possible values for a, b and c that make this a Markov\nmatrix.\n(b) Find the eigenvector for Awith eigenvalue 1 with positive entries that sum to 1.\n(c) As a check on you work in (a), prove that your values of a, b, care such that Aalso has eigenvalue 1\n2 . Find\ntwo linearly independent eigenvectors for this eigenvalue.\n3. This problem plays around with some of our probability functions.", "metadata": {"page": 112}}, {"page_content": "3. This problem plays around with some of our probability functions.\n(a) The exponential probability function is de\ufb01ned on the half line [0,\u221e). The version with mean \u00b5is the\nfunction x \u2192p(x) = e\u2212x/\u00b5. The standard deviation is also \u00b5. If R >0, what is the probability that\nx\u2265(R+ 1)\u00b5?\n(b) Let Q(R) denote the function of Ryou just derived in (a). We know a priori that Q(R) is no greater than\n1\nR2 and so R2Q(R) \u22641. What value of Rmaximizes the function R \u2192R2Q(R) and give the value of", "metadata": {"page": 112}}, {"page_content": "1\nR2 and so R2Q(R) \u22641. What value of Rmaximizes the function R \u2192R2Q(R) and give the value of\nR2Q(R) to two decimal places at this maximum. You can use a calculator for this last part.\n(c) Let p(x) denote the Gaussian function with mean zero and standard deviation \u03c3. Thus, p(x) =\n1\u221a\n2\u03c0 \u03c3e\u2212x2/(2\u03c32). We saw in (13.21) that the probability, P(R ), that x is greater than R\u03c3 is less than\n1\u221a\n2\u03c0 Re\u2212R2/2. We also know from the Chebychev Theorem that know that P(R) \u2264 1\nR2 . The ratio of these", "metadata": {"page": 112}}, {"page_content": "2\u03c0 Re\u2212R2/2. We also know from the Chebychev Theorem that know that P(R) \u2264 1\nR2 . The ratio of these\ntwo upper bounds is R. What value of Ris this ratio at its largest value? Use a calculator to write this\nlargest value.\n(d) Let L> 0 and let x\u2192p(x) denote the uniform probability function on the interval where \u2212L\u2264x\u2264L.\nThis probability has mean 0 and standard deviation L. Suppose that Ris larger than 1 but smaller than\u221a\n3. What is the probability that xhas distance RL\n2\n\u221a", "metadata": {"page": 112}}, {"page_content": "3. What is the probability that xhas distance RL\n2\n\u221a\n3 or more from the origin?\n17.3. Exercises 109", "metadata": {"page": 112}}, {"page_content": "(e) Let R \u2192Q(R) denote the function of Rthat gives the probability from (d) that xhas distance at least\nRL\n2\n\u221a\n3 from the origin. We know that R2Q(R) \u22641. What value of Rin the interval [1,\n\u221a\n3] maximizes this\nfunction and what is its value at its maximum?\n110 Chapter 17. More about Markov matrices", "metadata": {"page": 113}}, {"page_content": "CHAPTER\nEIGHTEEN\nMarkov matrices and complex eigenvalues\nThe previous chapter analyzed Markov matrices in some detail, but left open the question as to whether such a matrix\ncan have complex eigenvalues. My purpose here is to explain that such can be the case. I will then describe some of\ntheir properties.\n18.1 Complex eigenvalues\nAs it turns out, there are no 2 \u00d72 Markov matrices with complex eigenvalues. You can argue using the following\npoints:", "metadata": {"page": 114}}, {"page_content": "points:\n\u2022A matrix with real entries has an even number of distinct, complex eigenvalues since any given complex eigen-\nvalue must be accompanied by its complex conjugate.\n\u2022There are at most 2 eigenvalues for a 2 \u00d72 matrix: Either it has two real eigenvalues or one real eigenvalue with\nalgebraic multiplicity 2, or two complex eigenvalues, one the complex conjugate of the other.\n\u2022The number 1 is always an eigenvalue of a Markov matrix.", "metadata": {"page": 114}}, {"page_content": "\u2022The number 1 is always an eigenvalue of a Markov matrix.\nOn the other hand, here is a 3 \u00d73 Markov matrix with complex eigenvalues:\nA=\n\uf8ee\n\uf8ef\uf8f0\n1\n2 0 1\n2\n1\n2\n1\n2 0\n0 1\n2\n1\n2\n\uf8f9\n\uf8fa\uf8fb. (18.1)\nIf you compute the characteristic polynomial, P(\u03bb) = det(A\u2212\u03bbI), you will \ufb01nd that it is equal to\nP(\u03bb) = \u2212\n(\n\u03bb3 \u22123\n2 \u03bb2 + 3\n4 \u03bb+ 1\n4\n)\n. (18.2)\nOrdinarily, I would be at a loss to factor a generic cubic polynomial, but in this case, I know that1 is a root, so I know", "metadata": {"page": 114}}, {"page_content": "that \u03bb\u22121 divides P(\u03bb) to give a quadratic polynomial. I can do this division and I \ufb01nd that\nP(\u03bb) = \u2212(\u03bb\u22121)\n(\n\u03bb2 \u22121\n2 \u03bb+ 1\n4\n)\n. (18.3)\nThe roots of the quadratic polynomial \u03bb\u2192\u03bb2 \u22121\n2 \u03bb+ 1\n4 are roots of P. The roots of the quadratic polynomial can\nbe found (using the usual formula) to be\n1\n2 \u00b1\n\u221a\n1\n4 \u22121\n2 = 1\n4 \u00b1\n\u221a\n3\n4 i. (18.4)\n111", "metadata": {"page": 114}}, {"page_content": "You might complain that the matrix Ahere has some entries equal zero, and it would be more impressive to see an\nexample where all entries of Aare positive. If this is your attitude, then consider the Markov matrix\nA=\n\uf8ee\n\uf8ef\uf8f0\n1\n2\n1\n16\n7\n16\n7\n16\n1\n2\n1\n16\n1\n16\n7\n16\n1\n2\n\uf8f9\n\uf8fa\uf8fb (18.5)\nwhose characteristic polynomial is \u2212\n(\n\u03bb3 \u22123\n2 \u03bb2 + 171\n256 \u03bb\u2212 43\n256\n)\n. The roots of the latter are 1 and 1\n4 \u00b13\n\u221a\n3\n16 i.\n18.2 The size of the complex eigenvalues", "metadata": {"page": 115}}, {"page_content": "256\n)\n. The roots of the latter are 1 and 1\n4 \u00b13\n\u221a\n3\n16 i.\n18.2 The size of the complex eigenvalues\nI demonstrated in the previous chapter that a Markov matrix with no zero entries has a single real eigenvalue equal\nto 1 and that all of its remaining real eigenvalues have absolute value less than 1. An argument very much along the\nsame lines will demonstrate that the absolute value of any complex eigenvalue of such a matrix is less than1 also. For", "metadata": {"page": 115}}, {"page_content": "example, the absolute value of the complex eigenvalues for the matrix in (18.5) is\n\u221a\n31\n64 .\nTo see how this works in the general case, let\u2019s again use Ato denote our Markov matrix with all Ajk > 0. If \u03bbis\na complex eigenvalue for A, then it must be a complex eigenvalue for AT. Let \u20d7 vdenote a corresponding complex\neigenvector; thus AT\u20d7 v= \u03bb\u20d7 v. In terms of components, this says that\nA1kv1 + A2kv2 + \u00b7\u00b7\u00b7 + Ankvn = \u03bbvk (18.6)", "metadata": {"page": 115}}, {"page_content": "A1kv1 + A2kv2 + \u00b7\u00b7\u00b7 + Ankvn = \u03bbvk (18.6)\nfor any k\u2208{1,2,...,n }. Taking absolute values of both sides in (18.6) \ufb01nds the inequality\nA1k|v1|+ A2k|v2|+ \u00b7\u00b7\u00b7 + Ank|vn|\u2265| \u03bb||vk| (18.7)\nHere, I have used two facts about absolute values: First, the absolute value of\u03bbvk is the product of|\u03bb|and |vk|. Indeed,\nif aand bare any two complex numbers, then |ab|= |a||b| which you can see by writing both aand bin polar form.", "metadata": {"page": 115}}, {"page_content": "Thus, write a= rei\u03b8 and b= sei\u03d5 with sand rnon-negative. Then ab= rsei(\u03b8+\u03d5) and so the absolute value of abis\nrswhich is also |a||b|. Meanwhile, I used the fact that |a+ b|\u2264| a|+ |b|in an iterated fashion to obtain\n|A1kv1 + A2kv2 + \u00b7\u00b7\u00b7 + Ankvn|\u2264 A1k|v1|+ |A2kv2 + \u00b7\u00b7\u00b7 + Ankvn| (18.8)\n\u2264A1k|v1|+ A2k|v2|+ |A3kv3 + \u00b7\u00b7\u00b7 + Ankvn|\n..\n.\n\u2264A1k|v1|+ A2k|v2|+ \u00b7\u00b7\u00b7 + Ank|vn|.\nto deduce that the expression on the left side of (18.7) is no less than that on the right side. By the way, the fact that", "metadata": {"page": 115}}, {"page_content": "|a+ b|\u2264| a|+ |b|holds for complex numbers is another way to say that the sum of the lengths of any two sides to a\ntriangle is no less than the length of the third side.\nConsider the inequality depicted in (18.7) in the case that kis chosen so that\n|vk|\u2265| vj| for all j \u2208{1,...,n }. (18.9)\nThus, vk has the largest absolute value of any entry of\u20d7 v. In this case, each |vj|that appears on the left side of (18.7) is", "metadata": {"page": 115}}, {"page_content": "no larger than |vk|, so the left-hand side is even larger if each|vj|is replaced by |vk|. This done, then (18.7) \ufb01nds that\n(A1k + A2k + \u00b7\u00b7\u00b7 + Ank)|vk|\u2265| \u03bb||vk|, (18.10)\nSince A1k + A2k + \u00b7\u00b7\u00b7 + Ank = 1, this last expression \ufb01nds that |vk|\u2265|\u03bb|| vk|and so 1 \u2265|\u03bb|.\nNow, to see that |\u03bb|is actually less than 1, let us see what is required if every one of the inequalities that were used to", "metadata": {"page": 115}}, {"page_content": "go from (18.6) to (18.7) and from (18.7) to (18.10) are equalities. Indeed, if any one them is a strict inequality, then\n1 > |\u03bb|is the result. Let\u2019s work this task backwards: To go from (18.7) to (18.10) with equality requires that each112 Chapter 18. Markov matrices and complex eigenvalues", "metadata": {"page": 115}}, {"page_content": "vj have the same norm as vk. To go from (18.6) to (18.7), we used the triangle inequality roughly ntimes, this the\nassertion that |a+ b|\u2264|a| + |b|for any two complex numbers aand b. Now this is an equality if and only if a= rb\nwith r> 0; thus if and only if the triangle is degenerated to one where the a+ bedge contains both the aand bedges\nas segments.\nIn the cases at hand, this means that Ajkvj = rAkkvk for each j. Thus, not only does each vj have the same norm as", "metadata": {"page": 116}}, {"page_content": "vk, each is a multiple of vk with that multiple being a positive real number. This means that the multiple is 1. Thus,\nthe vector is a multiple of the vector whose entries are all equal to 1. As we saw in Handout 14, this last vector is an\neigenvector of A with eigenvalue 1 so if |\u03bb|= 1, then \u03bb= 1 and so isn\u2019t complex.\n18.3 Another Markov chain example\nThe term Markov chain refers to an unending sequence, {\u20d7 p(0),\u20d7 p(1),\u20d7 p(2),...}of vectors that are obtained from \u20d7 p(0)", "metadata": {"page": 116}}, {"page_content": "by successive applications of a Markov matrix A. Thus,\n\u20d7 p(t) =A\u20d7 p(t\u22121) and so \u20d7 p(t) =At\u20d7 p(0). (18.11)\nI gave an example from genetics of such a Markov chain in Chapter 15. What follows is a hypothetical example from\nbiochemistry.\nThere is a molecule that is much like DNA that plays a fundamental role in cell biology, this denoted by RNA.\nWhereas DNA is composed of two strands intertwined as a double helix, a typical RNA molecule has just one long", "metadata": {"page": 116}}, {"page_content": "strand, usually folded in a complicated fashion, that is composed of standard segments linked end to end. As with\nDNA, each segment can be one of four kinds, the ones that occur in RNA are denoted as G, C, A and U. There are\nmyriad cellular roles for RNA and the study of these is arguably one of the hottest items these days in cell biology.\nIn any event, imagine that as you are analyzing the constituent molecules in a cell, you come across a long strand of", "metadata": {"page": 116}}, {"page_content": "RNA and wonder if the sequence of segments, say AGACUA\u00b7\u00b7\u00b7, is \u201crandom\u201d or not.\nTo study this question, you should know that a typical RNA strand is constructed by sequentially adding segments\nfrom one end. Your talented biochemist friend has done some experiments and determined that in a test tube (in vitro,\nas they say), the probability of using one of A, G, C, or U for thetth segment depends on which of A, C, G or U has", "metadata": {"page": 116}}, {"page_content": "been used for the(t\u22121)st segment. This is to say that if we label A as1, G as 2, C as 3 and U as 4, then the probability,\npj(t) of seeing the segment of the kind labeled j \u2208{1,2,3,4}in the tth segment is given by\npj(t) = Aj1p1(t\u22121) + Aj2p2(t\u22121) + Aj3p3(t\u22121) + Aj4p4(t\u22121) (18.12)\nwhere Ajk denotes the conditional probability of a given segment being of the kind labeled byjif the previous segment", "metadata": {"page": 116}}, {"page_content": "is of the kind labeled by k. For example, if your biochemist friend \ufb01nds no bias toward one or the other base, then one\nwould expect that each Ajk has value 1\n4 . In any event, Ais a Markov matrix, and if we introduce \u20d7 p(t)\u2208R4 to denote\nthe vector whose kth entry is pk(t), then the equation in (18.12) has the form of (18.11).\nNow, those of you with some biochemistry experience might argue that to analyze the molecules that comprise a cell,", "metadata": {"page": 116}}, {"page_content": "it is rather dif\ufb01cult to extract them without breakage. Thus, if you \ufb01nd a strand of RNA, you may not be seeing the\nwhole strand from start to \ufb01nish and so the segment that you are labeling ast = 0 may not have been the starting\nsegment when the strand was made in the cell. Having said this, you would then question the utility of the \u2018solution\u2019,\n\u20d7 p(t) =At\u20d7 p(0)since there is no way to know \u20d7 p(0)if the strand has been broken. Moreover, there is no way to see if\nthe strand was broken.", "metadata": {"page": 116}}, {"page_content": "the strand was broken.\nAs it turns out, this objection is a red herring of sorts because one of the virtues of a Markov chain is that the form\nof \u20d7 p(t)is determined solely by \u20d7 p(t\u22121). This has the following pleasant consequence: Whether our starting segment\nis the original t = 0 segment, or some t = N >0 segment makes no difference if we are looking at the subsequent\nsegments. To see why, let us suppose that the strand was broken at segment N and that what we are calling strand t", "metadata": {"page": 116}}, {"page_content": "was originally strand t+ N. Not knowing the strand was broken, our equation reads \u20d7 p(t) =At\u20d7 p(0). Knowing the\nstrand was broken, we must relabel and equate our original \u20d7 p(t)with the vector \u20d7 p(t+ N) that is obtained from the\nstarting vector, \u20d7 p(0), of the unbroken strand by the equation\u20d7 p(t+ N) = At+N\u20d7 p(0).\n18.3. Another Markov chain example 113", "metadata": {"page": 116}}, {"page_content": "Even though our equation \u20d7 p(t) =At\u20d7 p(0)has the tth power of Awhile the equation \u20d7 p(t+ N) = At+N\u20d7 p(0)has the\n(t+N)th power, these two equations make the identical predictions. To see that such is the case, note that the equation\nfor \u20d7 p(t+ N) can just as well be written as \u20d7 p(t+ N) = At\u20d7 p(N) since \u20d7 p(N) = AN\u20d7 p(0).\n18.4 The behavior of a Markov chain as t\u2192\u221e\nSuppose that we have a Markov chain whose matrix Ahas all entries positive and has a basis of eigenvectors. In this", "metadata": {"page": 117}}, {"page_content": "regard, we can allow complex eigenvectors. Let us use\u20d7 e1 to denote the one eigenvector whose eigenvalue is1, and let\n{\u20d7 e2,...,\u20d7 en}denote the others. We can then write our starting \u20d7 p(0)as\n\u20d7 p(0) =\u20d7 e1 +\nn\u2211\nk=2\nck\u20d7 ek (18.13)\nwhere ck is real if \u20d7 ek has a real eigenvalue, but complex when \u20d7 ek has a complex eigenvalue. With regards to the latter\ncase, since our vector \u20d7 p(0)is real, the coef\ufb01cients ck and ck\u2032 must be complex conjugates of each other when the", "metadata": {"page": 117}}, {"page_content": "corresponding \u20d7 ek and \u20d7 ek\u2032 are complex conjugates also.\nI need to explain why \u20d7 e1 has the factor 1 in front. This requires a bit of a digression: As you may recall from the\nprevious chapter, the vector \u20d7 e1 can be assumed to have purely positive entries that sum to 1. I am assuming that such\nis the case. I also argued that the entries of any eigenvector with real eigenvalue less than 1 must sum to zero. This", "metadata": {"page": 117}}, {"page_content": "must also be the case for any eigenvector with complex eigenvalue. Indeed, to see why, suppose that\u20d7 ek has eigenvalue\n\u03bb \u0338= 1, either real or complex. Let \u20d7 vdenote the vector whose entries all equal 1. Thus, \u20d7 vis the eigenvector of AT\nwith eigenvalue 1. Note that the dot product of \u20d7 vwith any other vector is the sum of the other vector\u2019s entries. Keep\nthis last point in mind. Now, consider that the dot product of \u20d7 vwith A\u20d7 ek is, on the one hand, \u03bb\u20d7 v\u00b7\u20d7 ek, and on the other", "metadata": {"page": 117}}, {"page_content": "(AT\u20d7 v) \u00b7\u20d7 ek. As AT\u20d7 v= \u20d7 v, we see that \u03bb\u20d7 v\u00b7\u20d7 ek = \u20d7 v\u00b7\u20d7 ek and so if \u03bb\u0338= 1, then \u20d7 v\u00b7\u20d7 ek = 0 and so the sum of the entries of\nis zero.\nNow, to return to the factor of1 in front of \u20d7 e1, remember that \u20d7 p(0)is a vector whose components are probabilities, and\nso they must sum to 1. Since the components of the vectors \u20d7 e2,...,\u20d7 en sum to zero, this constraint on the sum requires\nthe factor 1 in front of \u20d7 e1 in (18.13).", "metadata": {"page": 117}}, {"page_content": "the factor 1 in front of \u20d7 e1 in (18.13).\nWith (18.13) in hand, it then follows that any given t> 0 version of \u20d7 p(t)is given by\n\u20d7 p(t) =\u20d7 e1 +\nn\u2211\nk=2\nck\u03bbt\nk\u20d7 ek. (18.14)\nSince |\u03bbt|= |\u03bb|t and each \u03bbthat appears in (18.14) has absolute value less than 1, we see that the large tversions of\n\u20d7 p(t)are very close to \u20d7 e1. This is to say that\nlim\nt\u2192\u221e\n\u20d7 p(t) =\u20d7 e1. (18.15)\nThis last fact demonstrates that as t increases along a Markov chain, there is less and less memory of the starting vector", "metadata": {"page": 117}}, {"page_content": "\u20d7 p(0). It is sort of like the aging process in humans: Ast\u2192\u221e, a Markov chain approaches a state of complete senility,\na state with no memory of the past.\n18.5 Exercises:\n1. Any 2 \u00d72 Markov matrix has the generic form\n[ a 1 \u2212b\n1 \u2212a b\n]\n, where a,b \u2208[0,1]. Compute the characteristic\npolynomial of such a matrix and \ufb01nd expressions for its roots in terms of aand b. In doing so, you will verify\nthat it has only real roots.", "metadata": {"page": 117}}, {"page_content": "that it has only real roots.\n2. Let Adenote the matrix in (18.1). Find the eigenvectors for Aand compute \u20d7 p(1),\u20d7 p(2)and lim\nt\u2192\u221e\n\u20d7 p(t)in the case\nthat \u20d7 p(t) =A\u20d7 p(t\u22121) and \u20d7 p(0) =\n\uf8ee\n\uf8f0\n1\n0\n0\n\uf8f9\n\uf8fb. Finally, write \u20d7 p(0)as a linear combination of the eigenvectors for A.\n114 Chapter 18. Markov matrices and complex eigenvalues", "metadata": {"page": 117}}, {"page_content": "3. Repeat Problem 2 using for Athe matrix in (18.5).\n18.5. Exercises 115", "metadata": {"page": 118}}, {"page_content": "CHAPTER\nNINETEEN\nSymmetric matrices and data sets\nSuppose that you take some large number of measurements of various facets of a system that you are studying. Lets say\nthat there are nfacets and you take N \u226bnmeasurements under different conditions; thus, you generate a collection\nof N vectors in Rn which you can label as {\u20d7 x1,\u20d7 x2,...,\u20d7 xN}.\nAn issue now is whether some of the n facets that you measure are dependent on the rest. For example, suppose", "metadata": {"page": 119}}, {"page_content": "that one facet is the temperature of the sample, and if all of the other facets of the system are completely determined\nby the temperature, then all of theseN vectors will lie on very near some curve in the n-dimensional space, a curve\nparameterized as T \u2192\u20d7 x(T) by the temperature. On the other hand, if no facets are determined by any collection of\nthe others, then the N vectors could be spread in a more or less random fashion through a region of Rn. The point", "metadata": {"page": 119}}, {"page_content": "here is that if you are interested in discovering relations between the various facets, then you would like to know if the\ndistribution of theNvectors is spread out or concentrated near some lower dimensional object \u2013 a curve or a surface or\nsome such. Any such concentration towards something less spread out indicates relations between the various facets.\nFor example, suppose n = 2. If you plot the endpoints of the vectors {\u20d7 xk}in R2 and see the result as very much", "metadata": {"page": 119}}, {"page_content": "lying near a particular curve (not necessarily a line), this says that the two facets are not able to vary in an independent\nfashion. Indeed, if y = f(x) is the equation for the curve, it says that the variation in xdetermines the variation in y.\nNow, for this n= 2 case, you can go and plot your N vectors and just look at the picture. However, for n> 2, this is\ngoing to be hard to do. How then can you discern relationships when n> 2?\n19.1 An example from biology", "metadata": {"page": 119}}, {"page_content": "19.1 An example from biology\nHere is a topical example: The technology is such that it is possible to monitor the levels of huge numbers of proteins\nin a cell as it goes about its business. These proteins are typically interacting with each other, and so it is of crucial\nimport to determine which are in\ufb02uencing which. Letndenote the number of proteins involved. Discerning the levels", "metadata": {"page": 119}}, {"page_content": "of these nproteins at some N times during the cell cycle with various environmental factors altered at various times\ngives some very large data set of the sort described above,N vectors in Rn. Of interest is how these vectors distribute\nthemselves \u2013 is it random through some region, or are the vectors concentrated near some intrinsically much thinner\nset such as a curve, surface or what ever inRn. If the latter is the case, then the structure of this thinner set, a curve, a", "metadata": {"page": 119}}, {"page_content": "surface or whatever, carries information about the complicated chemical interactions in a cell.\n19.2 A fundamental concern\nI don\u2019t think that there is a completely foolproof way to discern relationships between a bunch of vectors inRn. What\nfollows is an n = 2 example to keep in mind. As is typically the case, the vectors {\u20d7 xk}can not be too big; they\nare constrained to lie in some region of \ufb01xed size in Rn. Let me suppose in this example that no vector from this", "metadata": {"page": 119}}, {"page_content": "collection has norm greater than 1 and so all lie in the disk of radius 1 about the origin.\n116", "metadata": {"page": 119}}, {"page_content": "Now, consider the curve in this disk given in the parametric form by\nt\u2192(x= ctcos(t),y = ctsin(t)). (19.1)\nHere c >0 is a constant and 0 \u2264t \u22641\nc is the parameter for the curve. For any given c, this curve is a spiral. The\nradius, r(t), is equal to ctand the angle is tas measured from the positive x-axis in the anti-clockwise direction.\nAs cgets smaller, the spiral gets tighter and tighter; there are more and more turns before the curve hits the boundary", "metadata": {"page": 120}}, {"page_content": "of the disk. Indeed, when cis very large, the spiral hardly turns and stays very close to the x-axis. When c= 1\n2\u03c0, the\nspiral makes one complete turn before exiting the disk. When c= 1\n2\u03c0m and mis an integer, the spiral makes mturns\nbefore it exits.\nNow, here is the problem: Suppose that our vectors are near some very small c version of (19.1). Since this spiral\nmakes a lot of turns in the disk, all points in the disk are pretty close to the spiral and so even a random collection of", "metadata": {"page": 120}}, {"page_content": "points in the disk will \ufb01nd each point close to the spiral. In particular, our collection of vectors {\u20d7 xk}will be close to\nall small cversions of the spiral no matter what!!\nHere is another example: Suppose that the points {\u20d7 xk}are distributed randomly in a thin strip of width r \u226a1 along\nthe diagonal y = x. Thus, each \u20d7 xk has coordinates xand y that obey |x\u2212y|\u2264 r. So, inside this strip, the points", "metadata": {"page": 120}}, {"page_content": "are spread at random. Outside the strip, there are no points at all. If your experimental error is on the order of r,\nthen I would be happy to conclude that the points lie on the line y = x and thus the experiment indicates that the\ny-measurement is completely determined by the x-measurement. On the other hand, if the experimental error is much\nless than r, then I would not agree that the concentration near thex\u2212yline signi\ufb01es that yis determined by x. Maybe", "metadata": {"page": 120}}, {"page_content": "some part of y, but there is some part left over that is independent.\n19.3 A method\nSuppose we have data {\u20d7 xk}1\u2264k\u2264N, each a vector in Rn. Here is a method to analyze whether the data near any given\n\u20d7 xj is concentrated near some lower dimensional subspace of Rn.\nStep 1: You must choose a number, r, that is a reasonable amount greater than your experimental error. In\nthis regard, r should also be signi\ufb01cantly less than the maximum distance between any two points in {\u20d7 xk}. Thus,", "metadata": {"page": 120}}, {"page_content": "r \u226amaxj,k|\u20d7 xj \u2212\u20d7 xk|. This number r determines the scale on which you will be looking for the clustering of the\nvectors.\nStep 2: Let \u20d7 xj \u2208{\u20d7 xk}. To see if the data is clustering around a lower dimensional subspace near\u20d7 xj, take all points\nin {\u20d7 xk}that have distance ror less from \u20d7 xj. Let mdenote the number of such points. Allow me to relabel these points\nas {\u20d7 y1,...,\u20d7 ym}. These are the only points from the collection {\u20d7 xk}that will concern us while we look for clustering", "metadata": {"page": 120}}, {"page_content": "around the given point \u20d7 xj at the scale determined by r.\nStep 3: Let \u20d7 adenote the vector 1\nm(\u20d7 y1 + \u00b7\u00b7\u00b7 + \u20d7 ym). This vector should be viewed as the center of the collection\n{\u20d7 y1,...,\u20d7 ym}. For each index i= 1,...,m , set \u20d7 zi = \u20d7 yi \u2212\u20d7 a. This just shifts the origin in Rn.\nStep 4: View each i \u2208{1,...,m }version of \u20d7 zi as a matrix with 1 column and then introduce the transpose, \u20d7 zT\ni ,", "metadata": {"page": 120}}, {"page_content": "i ,\nwhich is a matrix with 1 row. Note that if ever is a vector in Rn viewed as a 1-column matrix, then \u20d7 zcan multiply \u20d7 zT\nto give the square matrix \u20d7 z\u20d7 zT. For example, if \u20d7 zhas top entry 1 and all others 0, then \u20d7 z\u20d7 zT has top left entry 1 and all\nothers zero. In general, the entry (\u20d7 z\u20d7 zT)ik is the product of the ith and kth entries of \u20d7 z.\nGranted the preceding, introduce the matrix\nA= 1\nm\n(\n\u20d7 z1\u20d7 zT\n1 + \u00b7\u00b7\u00b7 + \u20d7 zm\u20d7 zT\nm\n)\n. (19.2)", "metadata": {"page": 120}}, {"page_content": "Granted the preceding, introduce the matrix\nA= 1\nm\n(\n\u20d7 z1\u20d7 zT\n1 + \u00b7\u00b7\u00b7 + \u20d7 zm\u20d7 zT\nm\n)\n. (19.2)\nThis is a symmetric, n\u00d7nmatrix, so it has nreal eigenvalues which I will henceforth denote by {\u03bb1,...,\u03bb n}.\n19.3. A method 117", "metadata": {"page": 120}}, {"page_content": "Step 5: As I explain below, none of the eigenvalues has absolute value greater thanr2. I also explain below why A\nhas no negative eigenvalues. Granted this, then A\u2019s eigenvalues can be anywhere from0 to r2. Those eigenvalues that\nare much smaller than r2 correspond to \u2018pinched\u2019 directions. Ifn\u2212d\u2264nof the eigenvalues are much smaller than\nr2, this indicates that the distribution of the vectors from {\u20d7 xk}with distance ror less from \u20d7 xj is concentrated near a", "metadata": {"page": 121}}, {"page_content": "subspace of Rn whose dimension is d. To quantify this, I am going to say that the vectors near \u20d7 xj cluster around a\nsubspace whose dimension is no greater than dwhen Ahas n\u2212dor more eigenvalues that are smaller than 1\n2(n+2) r2.\nIn this regard, note that I am not going to expect much accuracy with this prediction unless mis large.\nI give some examples below to illustrate what is going on. At the end, I outline my reasons for choosing the factor\n1", "metadata": {"page": 121}}, {"page_content": "1\n2(n+2) to distinguish between small and reasonably sized eigenvalues.\n19.4 Some loose ends\nTo tie up some loose ends, let me show you why all of A\u2019s eigenvalues are in the interval between 0 and r2. To this\nend, suppose that \u20d7 vis some vector. To see what A\u20d7 vlooks like, the \ufb01rst thing to realize is that if \u20d7 zis any given vector,\nthen \u20d7 zT\u20d7 vis a 1 \u00d71 matrix, thus a number and that this number is the dot product between \u20d7 zand \u20d7 v. This implies that\nA\u20d7 v= 1\nm(\u20d7 z1 \u00b7\u20d7 v)\u20d7 z1 + \u00b7\u00b7\u00b7 + 1", "metadata": {"page": 121}}, {"page_content": "A\u20d7 v= 1\nm(\u20d7 z1 \u00b7\u20d7 v)\u20d7 z1 + \u00b7\u00b7\u00b7 + 1\nm(\u20d7 zm \u00b7\u20d7 v)\u20d7 zm. (19.3)\nTherefore, if I take the dot product of A\u20d7 vwith \u20d7 v, I \ufb01nd that\n\u20d7 v\u00b7(A\u20d7 v) = 1\nm(\u20d7 z1 \u00b7\u20d7 v)2 + \u00b7\u00b7\u00b7 + 1\nm(\u20d7 zm \u00b7\u20d7 v)2. (19.4)\nNote that this is a sum of non-negative terms, so \u20d7 v\u00b7(A\u20d7 v) \u22650 for any vector \u20d7 v.\nNow suppose that \u20d7 vis an eigenvector with eigenvalue \u03bb. Then A\u20d7 v= \u03bb\u20d7 vand so \u20d7 v\u00b7(A\u20d7 v) = \u03bb|\u20d7 v|2. As this is\nnon-negative, we see that \u03bb\u22650.\nTo see that \u03bb\u2264r2, use the fact that\n|\u20d7 z\u00b7\u20d7 v|\u2264| \u20d7 z||\u20d7 v| (19.5)", "metadata": {"page": 121}}, {"page_content": "non-negative, we see that \u03bb\u22650.\nTo see that \u03bb\u2264r2, use the fact that\n|\u20d7 z\u00b7\u20d7 v|\u2264| \u20d7 z||\u20d7 v| (19.5)\nfor any given vector on each term on the right hand side of (19.5) to conclude that\n\u20d7 v\u00b7(A\u20d7 v) \u2264 1\nm|\u20d7 z1|2 |\u20d7 v|2 + \u00b7\u00b7\u00b7 + 1\nm|\u20d7 zm|2 |\u20d7 v|2. (19.6)\nWhen I pull out the common factor 1\nm|\u20d7 v|2 on the right of this last inequality, I \ufb01nd that\n\u20d7 v\u00b7(A\u20d7 v) \u2264 1\nm\n(\n|\u20d7 z1|2 + \u00b7\u00b7\u00b7 + |\u20d7 zm|2)\n|\u20d7 v|2 (19.7)\nsince each \u20d7 zk has norm less than r, the right-hand side of (19.7) is no larger than r2|\u20d7 v|2. Thus,", "metadata": {"page": 121}}, {"page_content": "\u20d7 v\u00b7(A\u20d7 v) \u2264r2|\u20d7 v|2 (19.8)\nfor any vector \u20d7 v.\nNow, if \u20d7 vis an eigenvector with eigenvalue \u03bb, then the left-hand side of this last equation is \u03bb|\u20d7 v|2 and so \u03bb \u2264r2 as\nclaimed.\n19.5 Some examples\nWhat follows are a few examples to try to convince you that my attempt at estimating a dimension is not unreasonable..\n118 Chapter 19. Symmetric matrices and data sets", "metadata": {"page": 121}}, {"page_content": "Example 1: Suppose that all \u20d7 zk sit very close to the origin, for example, suppose that each \u20d7 zk has |\u20d7 zk|< \u03b5rwith\n\u03b52 \u2264 1\n2(n+2) . To see what this implies, return now to (19.7) to conclude that\n\u20d7 v\u00b7(A\u20d7 v) \u2264\u03b52r2|\u20d7 v|2. (19.9)\nThis then means A\u2019s eigenvalues are no larger than\u03b52r2 which is smaller than 1\n2(n+2) r2. Thus, we would predict the\nvectors from {\u20d7 xk}are clustering around a point near \u20d7 xj.", "metadata": {"page": 122}}, {"page_content": "2(n+2) r2. Thus, we would predict the\nvectors from {\u20d7 xk}are clustering around a point near \u20d7 xj.\nExample 2: Suppose that m is large and that the vectors {\u20d7 xk}all sit on a single line, and that they are evenly\ndistributed along the line. In particular, let \u20d7 zdenote the unit tangent vector to the line, and suppose that \u20d7 zk = (\u22121 +\n2k\nm+1 )r\u20d7 z. Thus, \u20d7 z1 = \u2212m\u22121\nm+1 r\u20d7 zand \u20d7 zm = m\u22121\nm+1 r\u20d7 z. This being the case\nA= r2\nm\nm\u2211\nk=1\n(\n\u22121 + 2k\nm+1\n)2\n\u20d7 z\u20d7 zT. (19.10)", "metadata": {"page": 122}}, {"page_content": "m+1 r\u20d7 z. This being the case\nA= r2\nm\nm\u2211\nk=1\n(\n\u22121 + 2k\nm+1\n)2\n\u20d7 z\u20d7 zT. (19.10)\nAs it turns out, this sum can be computed in closed form and the result is\nA= r2\n3\nm\u22121\nm+ 1\u20d7 z\u20d7 zT. (19.11)\nThe matrix Ahas the form c\u20d7 z\u20d7 zT where cis a constant. Now any matrix \u02c6A= c\u20d7 z\u20d7 zT has the following property: If \u20d7 vis\nany vector in Rn, then\n\u02c6A\u20d7 v= c\u20d7 z(\u20d7 z\u00b7\u20d7 v) . (19.12)\nThus, \u02c6A\u20d7 v= 0 if \u20d7 vis orthogonal to \u20d7 z, and \u02c6A\u20d7 z= c\u20d7 z. Hence \u02c6Ahas 0 and cas its eigenvalues, where 0 has multiplicity", "metadata": {"page": 122}}, {"page_content": "n\u22121 and chas multiplicity 1.\nIn our case, this means that there is one eigenvalue that is on the order of r2\n3 and the others are all zero. Thus, we\nwould say that the clustering here is towards a subspace of dimension 1, and this is precisely the case.\nExample 3: To generalize the preceding example, suppose that d\u22651, that V \u2282Rn is a d-dimensional subspace,\nand that the vectors {\u20d7 zk}1\u2264k\u2264m all lie in V.", "metadata": {"page": 122}}, {"page_content": "and that the vectors {\u20d7 zk}1\u2264k\u2264m all lie in V.\nIn this case, one can immediately deduce thatAwill have n\u2212dorthonormal eigenvectors that have zero as eigenvalue.\nIndeed, any vector in the orthogonal complement to V is in the kernel of A and so has zero eigenvalue. As this\northogonal subspace has dimension n\u2212d, so Ahas n\u2212dlinearly independent eigenvalues with eigenvalue0.\nThus, we see predict here that the clustering is towards a subspace whose dimension is no greater than d.", "metadata": {"page": 122}}, {"page_content": "19.6 Small versus reasonably sized eigenvalues\nI am going to give some indication here for my choice of the factor 1\n2(n\u2212d+2) to distinguish the small eigenvalue of\nA. Note here that you or others might want some other, smaller factor. For example, I am probably being conservative\nwith this factor.\nTo explain where this factor 1\n2(n+2) is coming from, I have to take you on a digression that starts here with the", "metadata": {"page": 122}}, {"page_content": "2(n+2) is coming from, I have to take you on a digression that starts here with the\nintroduction of the uniform probability function on the ball of radius r in Rd. This is to say that the probability of\nchoosing a point in any given subset of the ball is proportional to the d-dimensional volume of the subset. To each\nvector \u20d7 u, in this ball (thus, to each \u20d7 uwith |\u20d7 u|\u2264 r), we can assign the square matrix \u20d7 u\u20d7 uT. This can be viewed as a", "metadata": {"page": 122}}, {"page_content": "random variable that maps vectors in the ball to d\u00d7dmatrices. For example, in the case d= 2, one has\n\u20d7 u\u20d7 uT =\n[x2 xy\nxy y 2\n]\n. (19.13)\n19.6. Small versus reasonably sized eigenvalues 119", "metadata": {"page": 122}}, {"page_content": "For any d, the mean of the random variable \u20d7 u\u2192\u20d7 u\u20d7 uT is the matrix whose entry in the jth row and kth column is the\naverage of ujuk over the ball. These averages can be computed and one \ufb01nds that the mean of \u20d7 u\u20d7 uT is r2\nd+2 Id, where\nId is the d\u00d7didentity matrix.\nKeeping all of this in mind, go back to the formula for A in (19.2). Suppose that I consider a set of m vectors,\n{\u20d7 uk}1\u2264k\u2264m that all lie in a d-dimensional subspace. Suppose further that I sprinkle these vectors at random in the", "metadata": {"page": 123}}, {"page_content": "radius rball inside this subspace. I can then view\n\u02c6A= 1\nm\n(\n\u20d7 u1\u20d7 uT\n1 + \u00b7\u00b7\u00b7 + \u20d7 um\u20d7 uT\nm\n)\n(19.14)\nas the average of midentical, but unrelated random variables, this being mmaps of the form \u20d7 u\u2192\u20d7 uT. According to\nthe Central Limit Theorem, when mis large, the probability that \u02c6Adiffers from r2\nd+2 Id is very small.\nIf in my actual data, the vectors {\u20d7 zk}, are truly clustering around a d-dimensional subspace of Rn and are more or", "metadata": {"page": 123}}, {"page_content": "less randomly sprinkled in the radius rball of this set, then I should expect the following:\nWhen mis very large, the Central Limit Theorem tells me that the matrix Ain (19.2) is very close to the\nmatrix r2\nd+2 P, where P here denotes the orthogonal projection on to the subspace in question.\nThus, it should have deigenvalues that are very close to r2\nd+2 and n\u2212deigenvalues that are much smaller than this", "metadata": {"page": 123}}, {"page_content": "d+2 and n\u2212deigenvalues that are much smaller than this\nnumber. In particular, when mis large, then under the hypothesis that the vectors{\u20d7 zk}1\u2264k\u2264m are sprinkled at random\nin a d-dimensional subspace, the matrix Ain (19.2) will be very likely to have deigenvalues that are on the order of\nr2\nd+2 and n\u2212deigenvalues that are zero.\nThis application of the Central Limit Theorem explains my preference for the size distinction 1\n2(n+2) r2 between small", "metadata": {"page": 123}}, {"page_content": "2(n+2) r2 between small\neigenvalues of Aand eigenvalues that are of reasonable size. I think that this cut-off is rather conservative and one can\ntake a somewhat larger one.\n19.7 Exercises:\n1. The purpose of this exercise is to compute the average of the matrix in (19.13) over the disk of radius rin the\nxy-plane. This average is the matrix, U, whose entries are\nU11 = 1\n\u03c0r2\n\u222b\u222b\nx2 dxdy, U 22 = 1\n\u03c0r2\n\u222b\u222b\ny2 dxdy, and U12 = U21 = 1\n\u03c0r2\n\u222b\u222b\nxy dxdy.", "metadata": {"page": 123}}, {"page_content": "U11 = 1\n\u03c0r2\n\u222b\u222b\nx2 dxdy, U 22 = 1\n\u03c0r2\n\u222b\u222b\ny2 dxdy, and U12 = U21 = 1\n\u03c0r2\n\u222b\u222b\nxy dxdy.\nTo compute U, change to polar coordinates (\u03c1,\u03b8) where \u03c1\u22650 and \u03b8 \u2208[0,2\u03c0] using the formula x= \u03c1cos(\u03b8)\nand y= \u03c1sin(\u03b8). Show that\n(a) U11 = 1\n\u03c0r2\n\u222b2\u03c0\n0\n\u222br\n0 \u03c12 cos2(\u03b8)\u03c1d\u03c1d\u03b8 ,\n(b) U22 = 1\n\u03c0r2\n\u222b2\u03c0\n0\n\u222br\n0 \u03c12 sin2(\u03b8)\u03c1d\u03c1d\u03b8 , and\n(c) U12 = U21 = 1\n\u03c0r2\n\u222b2\u03c0\n0\n\u222br\n0 \u03c12 sin(\u03b8) cos(\u03b8)\u03c1d\u03c1d\u03b8 .\nNext, use the formula cos(2\u03b8) = 2 cos2(\u03b8) \u22121 = 1 \u22122 sin2(\u03b8) and sin(2\u03b8) = 2 sin(\u03b8) cos(\u03b8) to do the angle\nintegrals \ufb01rst and so \ufb01nd that", "metadata": {"page": 123}}, {"page_content": "integrals \ufb01rst and so \ufb01nd that\n(d) U11 = U22 = 1\nr2\n\u222br\n0 \u03c13 d\u03c1, and\n(e) U12 = U21 = 0.\nFinally, do the \u03c1-integral to \ufb01nd that U11 = U22 = 1\n4 r2 and U12 = U21 = 0. Note that this means that U is the\nd= 2 version of r2\nd+2 Id.\n120 Chapter 19. Symmetric matrices and data sets", "metadata": {"page": 123}}]